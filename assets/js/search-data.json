{"0": {
    "doc": "Articles",
    "title": "Articles",
    "content": " ",
    "url": "/learning-notes/articles/",
    "relUrl": "/articles/"
  },"1": {
    "doc": "Books",
    "title": "Books",
    "content": " ",
    "url": "/learning-notes/books/",
    "relUrl": "/books/"
  },"2": {
    "doc": "Learning Notes",
    "title": "Learning Notes",
    "content": "Here you can explore the notes I took on books I read, talks I watch, articles I study, and papers I love – recalling them right afterward by creating short summaries – helps a lot in my learning process. Here you’ll find some of those little pieces. | Articles | Books | Talks | Papers | . ",
    "url": "/learning-notes/",
    "relUrl": "/"
  },"3": {
    "doc": "Papers",
    "title": "Papers",
    "content": " ",
    "url": "/learning-notes/papers/",
    "relUrl": "/papers/"
  },"4": {
    "doc": "Talks",
    "title": "Talks",
    "content": " ",
    "url": "/learning-notes/talks/",
    "relUrl": "/talks/"
  },"5": {
    "doc": "Microservices by Martin Fowler",
    "title": "Microservices by Martin Fowler",
    "content": "# [Microservices by Martin Fowler](https://www.martinfowler.com/articles/microservices.html) Microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. Built around business capabilities. ## Monolith - A monolithic application built as a single unit. All your logic for handling a request runs in a single process. - A change made to a small part of the application, requires the entire monolith to be rebuilt and deployed. - It's often hard to keep a good modular structure. - Scaling requires scaling of the entire application rather than parts. ## Microservice As well as the fact that services are independently deployable and scalable, each service also provides a firm module boundary. ### Characteristics * **Componentization via Services.** Component is a unit of software that is independently replaceable and upgradeable. Libraries in-memory function calls, while services are out-of-process communicate via RPC or web requests. services are independently deployable. More explicit components. Services map to runtime processes, but that is only a first approximation. Downsides: Remote calls are more expensive than in-process calls. * **Organized around Business Capabilities.** Broad-stack implementation of software for that business area, including user-interface. Siloed funcional teams creating layered architectures (silos) vs cross-functional team and softwar organised vs capabilities (Conways law). Micro-services are cross-functioal. The necessarily more explicit separation makes it easier to keep the team boundaries clear. * **Products not Projects.** A team should own a product over its full lifetime, you build it you run it. The product mentality, ties in with the linkage to business capabilities. Rather than looking at the software as a set of functionality to be completed. * **Smart endpoints and dumb pipes.** Microservices aim to be as decoupled and as cohesive as possible. Like Unix pipes (HTTP request-response and RESTish protocols), or messaging over lighweight message bus (RabbitMQ). Issue: The biggest issue in changing a monolith into microservices lies in changing the communication pattern. * **Decentralised governance.** You can choose the apropriate technology for your service, no need to limit or standarise the technology across use cases. Prefer to use sharing battle-tested code as libraries. * **Decentralised data management.** Conceptual model of the world will differ between systems. The concept of a Bounded Context in DDD. With microservices there is a natural correlation between service and context boundaries. Decentralise data storage decisions. Let each service manage their own data. This may introduce eventual consistency. Managing inconsistencies will be a challenge. * **Infrastructure automation.** Once you have automated the deployment pipeline for monolith, doing the same for microservices won't be that hard. The operational landscape between the monolith and the microservices can be strikingly different. * **Design for failure.** Applications need to be designed so they can tolerate the failure of services. Clients have to react gracefully on service failure. Detect failures quickly and recover the service automatically if possible. Sophisticated monitoring and logging setups, up/down dashboards, relevant metrics. Details on circtuit breakers, throughput, latency… * **Evolutionary design.** Control changes in the application without slowing down change. Independent replacement and upgradeability. Evolve monoliths to miroservices, the core as the monolith and new services in microservices. Granular release planning. You only need to deploy services you modify, not all like in the monolith. Use versioning internally if necessary, but try to avoid it. --- ### Are microservices the future? Worth serious consideration for enterprise applications. The decay seen in monolith applications may be less likely with microservices architecture where boundaries are explicit and hard to patch around. In the other hand, getting boundaries right is difficult and refactoring them is important. Refactoring a distributed architecture is more difficult as interface changes need to be coordinated. Components may not compose cleanly, complexity from inside a component may move to the connections between components. There is also a factor of team skill. Probably you shouldn't start with a microservices architecture, instead begin with a monolith, keep it modular and split it into microservices once th emonolith becomes a problem. ",
    "url": "/learning-notes/articles/microservices/",
    "relUrl": "/articles/microservices/"
  },"6": {
    "doc": "99 Bottles of OOP",
    "title": "99 Bottles of OOP",
    "content": "# [99 Bottles of OOP](https://www.goodreads.com/book/show/31183020-99-bottles-of-oop) - [Introduction](#introduction) - [Rediscovering simplicity](#rediscovering-simplicity) - [Test driving shameless green](#test-driving-shameless-green) - [Unearthing concepts](#unearthing-concepts) - [Practising horizontal refactoring](#practising-horizontal-refactoring) - [Separating responsibilities](#separating-responsibilities) - [Achieving openness](#achieving-openness) - [Afterword](#afterword) ## Introduction The book is about writing cost-effective, maintainable, and pleasing code. Writing code is the _process_ of working your way to the next stable end point, not the end point itself. ## Rediscovering simplicity You should not _reach_ for abstractions, but instead you should resist them until they absolutely insist upon being created. ### Simplifying code The code should meet often contradictory goals. It must remain concrete enough to be understood while simultaneously being abstract enough to allow for change. Code at the concrete end might be expressed as a single long procedure full of `if` statements. Code at the abstract end might consist of many classes, each with one method containing a single line of code. The best solution for most problems lies not at the extreme of this continuum, but somewhere in the middle. #### Incomprehensibly concise ##### Names > **Terminology: Method versus Message** > A \"method\" is defined on an object, and contains behaviour. An object like `Bottles` can define a method named `song`. > > A \"message\" is sent by an object to invoke behaviour. In the previous example, `song` can send the `verses` message to the implicit receiver `self`. > > Methods are _defined_, and messages are _sent_. > > The confusion between these terms comes about because it is common for the receiver of a message to define a method whose name exactly corresponds to that message. The `song` method sends the `verses` _message_ to `self`, which results in an invocation of the `verses` method. Writing code is like writing a book; your efforts are for _other_ readers. Getting insight into potential expense of a bit of code: 1. How difficult was it to write? 2. How hard is to understand? 3. How expensive will it be to change? Code is easy to understand when it clearly reflects the problem it's solving, and thus openly exposes that problem's domain. #### Concretely abstract DRYing out code is not free. It adds a level of indirection, and layers of indirection make the _details_ of what's happening harder to understand. **DRY makes sense when it reduces the cost of change more than it increases the cost of understanding the code.** You should name methods not after what they do (or how they behave), but after what they mean, what they represent in the context of your domain. #### Shameless green (TDD) The failure here is not bad intention, it's insufficient patience. _Shameless Green_ is clearly the best solution, yet almost no one writes it. It feels embarrassingly easy, and is missing many qualities that you expect in good code. One of the biggest challenges of design is when to stop, and deciding well requires making judgments about code. ### Judging code #### Evaluating code based on opinion Definitions generally describe how code looks when it's done without providing any concrete guidance about how to get there. **Any pile of code can be made to _work_; good code not only works, but it also simple, understandable, expressive and changeable.** #### Evaluating code based on facts You can think of metrics as crowd-sourced opinions about quality of code. ##### Source lines of code (SLOC) Measuring programmer productivity by counting lines of code assumes that all programmers write equally efficient code. Despite the fact that novices write more code to produce less function, by this metric, they can seem more productive. SLOC numbers reflect code volume, and while it's useful for some purposes, knowing SLOC alone is not enough to predict code quality. ##### Cyclomatic complexity A method with many deeply nested conditionals would score very high, while a method with no conditionals at all would score 0. You can use it to compare code or limit overall complexity. You can also use it to determine if you've written enough tests, as it tells you the minimum number of tests needed to cover all of the logic in the code. ##### Assignments, branches and conditions (ABC) metric * _Assignments_ is a count of variable assignments. * _Branches_ counts not branches of an if statement but branches of control, meaning function calls or message sends. * _Conditions_ counts conditional logic. ABC scores are reflected as cognitive instead of physical size. It does measure complexity. Metrics are fallible but human opinion is no more precise. Checking metrics regularly will keep you humble _and_ improve your code. Metrics clearly don't tell the whole story. --- Infinitely experienced programmers do not write infinitely complex code; they write code that's blindingly simple. The challenge comes when a change request arrives. Code that's good enough when nothing ever changes may _not_ be good enough when things do. ## Test driving shameless green ### Writing the first test You can't figure out what's right until you write some tests. The purpose of some of your tests might very well be to prove that they represent bad ideas. While it _is_ important to consider the problem and to sketch out an overall plan before writing the first test, don't overthink it. Tests contain three parts: * **Setup** Create the specific environment required for the test. * **Do** Perform the action to be tested. * **Verify** Confirm the result is as expected. **As the tests get more specific, the code gets more generic.** ### Understanding transformations In the [\"Transformation Priority Premise\"](https://8thlight.com/blog/uncle-bob/2013/05/27/TheTransformationPriorityPremise.html), Martin defines _transformations_ as \"simple operations that change the behaviour of code\". Transformations are arranged in \"priority\" order, from simpler to more complex. > 1. `({}–>nil)` no code at all->code that employs nil > 2. `(nil->constant)` > 3. `(constant->constant+)` a simple constant to a more complex constant > 4. `(constant->scalar)` replacing a constant with a variable or an > argument > 5. `(statement->statements)` adding more unconditional statements. > 6. `(unconditional->if)` splitting the execution path > 7. `(scalar->array)` > 8. `(array->container)` > 9. `(statement->recursion)` > 10. `(if->while)` > 11. `(expression->function)` replacing an expression with a function or algorithm > 12. `(variable->assignment)` replacing the value of a variable. ### Tolerating duplication As tests get more specific, code should become more generic. Code becomes more generic by becoming more abstract. One way to make code more abstract is to DRY it out. **DRY is important but if applied to early, and with too much vigour, it can do more harm than good.** It's a good idea to ask the following questions when doing so: * _Does the change I'm contemplating make the code harder to understand?_ Be suspicious of any change that muddies the waters. * _What is the future cost of doing nothing now?_ Some changes cost the same regardless of whether you make them now or delay them until later. **If it doesn't increase your costs, delay making changes.** * _When will the future arrive, or how soon will I get more information?_ It's better to tolerate duplication than to anticipate the wrong abstraction. Writing Shameless Green means optimising for understandability, not changeability, and patiently tolerating duplication if doing so will help reveal the underlying abstraction. ### Exposing responsibilities **Duplication is useful when it supplies independent, specific examples of a general concept that you don't yet understand.** A specific method is responsible for understanding its input arguments, and for knowing how to use these arguments to produce the correct output. Responsibilities out of the scope of the method itself it should be delegated to other parts of the system. When the obvious implementation is evident, it makes sense to jump straight to it. If you are absolutely certain of the correct implementation, there is no need to wear a hair shirt and repetitively inch through a series of tiny steps. The small steps of TDD act to incrementally reveal the correct implementation. If your absolute certainty turns out to be wrong, **skipping these incremental steps means you miss the opportunity of being set right.** Developing the habit of writing just enough code to pass the tests forces you to write better tests. ### Choosing names Knowledge that one object has about another creates a dependency. Dependencies tie objects together, exacerbating the cost of change. What's better to call `song` method, or to invoke `verses(0, 99)`? ```ruby def song verses(0, 99) end def verses(starting, ending) #... end ``` The `song` method imposes a single dependency. The `verses` method request the entire song, however requires significantly more knowledge: * name of the method * that it has two arguments: the first argument is the start, the second argument is the end * the song starts on verse 99 * the song ends on verse 0 That's why `song` method is better from the client perspective. ### Writing cost-effective tests The first step in learning the art of testing is to understand how to write tests that confirm _what_ your code does without the knowledge on _how_ your code does it. ### Avoiding the echo-chamber Programmers who are hyper-alert to duplication, might be tempted to test `song` like this: ```ruby def test_the_whole_song bottles = Bottles.new assert_equal bottles.verses(99, 0), bottles.song end ``` This test has a major flaw that can cause it toggle from \"short and sweet\" to \"painful and costly\" in the blink of an eye. This flaw lies dormant until something changes, so the benefits of writing tests like this accrue to the writer today, while the costs are paid by an unfortunate maintainer in the future. If you change an implementation detail while retaining existing behaviour and are then confronted with a sea of red, you are right to be exasperated. **This is completely avoidable, and a sign that tests are too tightly coupled to code. Such tests impede change and increase costs.** There is a solution to this testing problem. The `song` test should know nothing about how the `Bottles` class produces the song. The clear and unambiguous expectation here is that song return the complete set of lyrics, and the best way and easies way to do it is to assert that it does: ```ruby def test_the_whole_song expected = Refactoring is the process of changing a software system in such a way that it does not alter the external behaviour of the code yet improves its internal structure – Martin Fowler You should never change tests during a refactoring. If your tests are flawed such that they interfere with refactoring, improve them first, and then refactor. ### Following the _Flocking Rules_ You can abstractions by iteratively applying a small set of simple rules (Flowing Rules): 1. Select the things that are most alike. 2. Find the smallest difference between them. 3. Make the simplest change that will remove the difference. Changes to the code can be subdivided into four distinct steps: 1. Parse the new 2. Parse and execute it 3. Parse, execute and use its result 4. Delete unused code ### Converging on abstractions #### Focusing on difference DRYing out sameness has some value, but DRYing out difference has more. If two concrete examples represent the same abstraction and they contain a difference, that difference must represent a smaller abstraction within the larger one. #### Simplifying hard problems It is common to find that hard problems are hard only because the easy ones have not yet been solved. Don't discount the value of solving easy problems. #### Making methodical transformations Making a slew of simultaneous changes is not refactoring, it's _rehacktoring_. #### Refactoring gradually Real refactoring is comfortingly predictable, and saves brainpower for more thought-provoking challenges. ## Practising horizontal refactoring ### Equivocating about names Names should neither be too general nor too specific. When the perfect name for a concept is elusive, there are three strategies for moving forward: * Dedicate five to ten minutes to ponder, and then use the best name that you can come up with. * Instantly choose a meaningless name like `foo`. Like there is no point wasting time thinking about it now, the name will be obvious later. * You can ask someone else for help. ### Deriving names from responsibilities While you are allowed to use common sense, it's usually best to stay horizontal and concentrate on the current goal. The effort you put into selecting good names right now pays off by making it easier to recognise perfect names later. ### Seeking stable landing points Code is read many more times than it is written, so anything that increases understandability lowers costs. Next, and just as important, consistent code enables _future_ refactorings. ### Obeying the Liskov Substitution Principle The idea of reducing the number of dependencies imposed upon message senders by requiring that receiver return trustworthy objects is a generalisation of the Liskov Substitution Principle. Liskov, in plain terms, requires that objects be what they promise they are. When using inheritance, you must be able to freely substitute an instance of a subclass for an instance of its superclass. Subclasses, by definition, are all their superclasses, _plus more_, so this substitution should always work. Liskov Substitution Principle also applies to duck types. When relying on duck types, every object that asserts that it plays the duck's role must completely implement the duck's API. Duck types should be substitutable for one another. Liskov violations force message senders to have knowledge of the various return types, and to either treat them differently or convert them into something consistent. ### Taking bigger steps If you take bigger steps and the tests begin to fail, there's something about the problem that you don't understand. If this happens, don't push forward and refactor under red. Undo, return to green, and make incremental changes until you regain clarity. ### Depending on abstractions Abstractions are beneficial in many ways. They consolidate code into a single place so that it can be changed with ease. They _name_ this consolidated code, allowing the name to be used as a shortcut for an idea, independent of its current implementation. These are valuable benefits, but abstractions also help in another, more subtle, way. In addition to the above, abstractions tell you where the code _relies_ upon an idea. But to get this last benefit, you must refer to an abstraction in every place where it applies. ## Separating responsibilities ### Selecting the target code smell The truth about refactoring is that it sometimes makes things worse, in which case your efforts serve gallantly to disprove an idea. #### Spotting common qualities **Superfluous differences raises the cost of reading code, and increases the difficulty of future refactorings.** Having multiple methods that take the same argument is a code smell. \"Same\" means _same concept_, not _identical name_. In an ideal world, each different concept would have its own unique, precise name, and there would be no ambiguity. #### Enumerating flocked method commonalities Conditionals could logically have used the less than greater than or not equal operations, and that would still have passed the tests. Programmers tend to blithely interchange these different comparison operators, confident that if the tests pass, the code is correct. **Testing for equality has several benefits over the alternatives. Most obviously, it narrows the range of things that meet the condition.** This reduces the difficulty of debugging errors. Testing of equality also makes the code more precise, and this precision enables future refactorings. As an OO practitioner, when you see a conditional, the hairs on your neck should stand up. It means that objects are missing, and suggests that subsequent refactorigns are needed to reveal them. This is not to say that you'll never have a conditional in an object-oriented application. **Collaborators must be brought together in useful combinations, and assembling these combinations requires knowing which objects are suitable.** Some object, somewhere, must choose which objects to create, and this often involves a conditional. There is a big difference between a conditional that selects the correct object and one that supplies behaviour. The first is acceptable and generally unavoidable. The second suggests that you are missing objects in your domain. ### Extracting classes _Primitive Obsession_ is when you use one of these data classes to represent a concept in your domain. Obsessing on a primitive results in code that passes built-in types around, and supplies behaviour for them. The cure of _Primitive Obsession_ is to create new class to use in place of the primitive. For this operation, the refactoring recipe is _Extract Class_. #### Modelling abstractions` **It's easy to imagine creating objects that stand in for things, but the power of OO is that it lets you model ideas, things that don't physically exist.** Modellable ideas often lie dormant in interactions between objects. Imagine an event management application, it might contain `Buyer` and `Ticket`, but also you place the logic for managing purchases, discounts or refunds into `Purchase`, `Refund` or `Discount` objects. Experienced OO programmers deftly create virtual worlds in which ideas are as real as physical things. #### Naming classes **The rule about naming can thus be amended: while you should continue to name methods after what they _mean_, classes can be named after what they _are_.** #### Extracting classes You should refrain from altering the code of these copied methods until the new class is fully wired into the old. #### Removing arguments Learning the art of transforming code one line at a time, while keeping the tests passing at every point, let's you undertake enormous refactoring piecemeal. #### Trusting the process Refactorings that lead to errors can shake your faith in the validity of the corresponding recipes. However, these recipes have proven themselves reliable for many people across many circumstances. **If you adhere to a recipe and tests start failing, it's likely that there's something about the problem that you don't yet understand.** ### Appreciating immutability The best things about immutable objects is that they are easy to understand and to reason about. These objects never start out one way and the secretly morph into something else. Because they are easy to reason about, immutable objects are also easy to test. Tests for immutable objects avoid extra setup, which makes the tests cheaper to write and easier to understand. Another key virtue of immutable objects is that they are thread safe. You can't break shared state if shared state doesn't change. ### Assuming _fast enough_ The benefits of immutability are so great that, _if it were free_, you'd choose it every time. Immutability's offsetting costs are twofold. First you must become reconciled of the idea, second achieving immutability requires the creation of more new objects. The best programming strategy is to write the simplest code possible and measure its performance once you're done. If the whole is not acceptably fast, profile the performance, and speed up the slowest parts. **Your goal is to optimise for easy of understanding while maintaining performance that's fast enough.** Don't sacrifice readability in advance of having solid performance data. ## Achieving openness ### Consolidating data clumps _Data Clump_ is officially about _data_, and is defined as the situation in which several (three or more) data fields routinely occur together. Having a clump of data usually means you are missing a concept. ### Making sense of conditionals **Instead of injecting an object and conditionally supplying it with behaviour, you should instead arrange code such that you can merely forward the message to the injected object.** Fowler offers several curative refactoring recipes. The two main contenders are _Replace Conditional with State/Strategy_ and _Replace Conditional with Polymorphism_. _Polymorphism_ recipe uses inheritance, and _State/Strategy_ recipe does not. Skilled programmers do what's right when they intuit the truth, but otherwise they engage in careful, precise, reproducible, and reversible coding experiments. **Practice builds intuition.** ### Replacing conditionals with polymorphism Polymorphism allows senders to depend on the _message_ while remaining ignorant of the _type_, or class, of the receiver. Senders don't care what receivers are; instead, they depend on what receiver do. #### Dismembering conditionals Each conditional supplies _specific_ behaviour in its `true` branch and _generalised_ behaviour in its `false`. Modern object-oriented programming is biased towards preferring composition over inheritance. However, this bias shouldn't be taken to mean that the use of inheritance is banned. #### Manufacturing objects The code that is said to \"manufacture\" an instance of the right kind of object is commonly referred as a _factory_. **The factory's purpose is to isolate the names of the concrete classes, and to hide the logic needed to choose the correct one.** Then you invoke the factory to get an object, you have no need to know the class of the returned object. By refusing to be aware of the classes of the objects with which you interact, you grant others the freedom to alter your code's behaviour without editing its source. **Someone could amend the factory to return newly introduced players, and your existing code would happily collaborate with these unanticipated objects.** #### Making peace with conditionals Factories don't know _what_ to do: instead, they know how to choose who does. They consolidate the choosing and separate the chose. You _can_ use polymorphism to create pluggable behaviour, and confine conditionals to factories whose job is to select the right object. #### Transitioning between types Correcting Liskov violations is important because object oriented programming, especially in dynamically-typed languages, relies on _explicit_ trust in the _implicit_ contracts between objects. Trustworthy objects are a joy to work with because they always behave as you expect. Untrustworthy objects that sometimes fail to respond to a message force you into paranoid programming style. Untrustworthy objects require senders of messages to know too much. When your application has code that needs knowledge of the internals of other objects in order to correctly interact them, changes to those other objects might break your code. **If you have to check the type of an object in order to know what message to send, you are forced into a conditional that lists every concrete class with which you're willing to collaborate. Doing this dooms you into changing the conditional when you add a new class.** ### Making the easy change > Make the change easy (warning: this may be hard), then make the easy change – Kent Beck Most of this book has been concerned with making the change easy. That hard work paid off, where you made the easy change. ### Prying open factory Creating factories that are open for extension. If there is a predictable pattern to create needed objects, then it might be possible to dynamically generate the correct object for each case. ```ruby class BottleNumber def self.for(number) begin const_get(\"BottleNumber#{number}\") rescue NameError BottleNumber end.new(number) end end ``` If you introduce a behaviour, there will be no need to change any existing code at all. Not even the factory, making the factory open too. There are some reasonable objections: 1. This version is harder to understand than the original 2. Specific objects are no longer referenced in the source code. It will be hard to find references to the classes whose names are dynamically constructed. 3. The code uses an exception for flow control. Controlling the flow of a program with exceptions is roundly condemned. 4. The factory ignores bottle number classes whose names do not follow the convention. ## Afterword Strive for simplicity. Don't abstract too soon. Focus on smells. Concentrate on difference. Take small steps. Follow the _Flocking Rules_. Refactor under green. Fix the easy problems first. Work horizontally. Seek stable landing points. Be disciplined. Don't chase the shiny thing. In addition, deal with new requirements by first refactoring existing code to be open to them, and then writing new code to meet them. Achieving openness is usually the more challenging task, but can be sought in absolute safety if you have tests that act as a wall at your back. Your job is not to be perfect, but to write a generous and sympathetic story. ",
    "url": "/learning-notes/books/99-bottles-of-oop/",
    "relUrl": "/books/99-bottles-of-oop/"
  },"7": {
    "doc": "A Guide to the Good Life: The Ancient Art of Stoic Joy",
    "title": "A Guide to the Good Life: The Ancient Art of Stoic Joy",
    "content": "# [A Guide to the Good Life: The Ancient Art of Stoic Joy](https://www.goodreads.com/book/show/5617966-a-guide-to-the-good-life) - [A plan for a living](#a-plan-for-a-living) - [The rise of Stoicism](#the-rise-of-stoicism) - [Philosophy takes an interest in life](#philosophy-takes-an-interest-in-life) - [The first stoics](#the-first-stoics) - [Roman Stoicism](#roman-stoicism) - [Stoic psychological techniques](#stoic-psychological-techniques) - [Negative visualisation. What's the worst that can happen?](#negative-visualisation-whats-the-worst-that-can-happen) - [The dichotomy of control. On becoming invincible](#the-dichotomy-of-control-on-becoming-invincible) - [Fatalism. Letting go of the past... and the present](#fatalism-letting-go-of-the-past-and-the-present) - [Self-denial. On dealing with the dark side of pleasure](#self-denial-on-dealing-with-the-dark-side-of-pleasure) - [Meditation. Watching ourselves practice Stoicism](#meditation-watching-ourselves-practice-stoicism) - [Stoic advice](#stoic-advice) - [Duty. On loving mankind](#duty-on-loving-mankind) - [Social relations. On dealing with other people](#social-relations-on-dealing-with-other-people) - [Insults. On putting up with put-downs](#insults-on-putting-up-with-put-downs) - [Grief. On vanquishing tears with reason](#grief-on-vanquishing-tears-with-reason) - [Anger. On overcoming anti-joy](#anger-on-overcoming-anti-joy) - [Personal values. On seeking fame](#personal-values-on-seeking-fame) - [Personal values. On luxurious living](#personal-values-on-luxurious-living) - [Exile. On surviving a change of place](#exile-on-surviving-a-change-of-place) - [Old age. On being banished to a nursing home](#old-age-on-being-banished-to-a-nursing-home) - [Dying. On a good end to a good life](#dying-on-a-good-end-to-a-good-life) - [On becoming stoic. Start now and prepare to be mocked](#on-becoming-stoic-start-now-and-prepare-to-be-mocked) - [Stoicism for modern lives](#stoicism-for-modern-lives) - [The decline of Stoicism](#the-decline-of-stoicism) - [Stoicism reconsidered](#stoicism-reconsidered) - [Practicing Stoicism](#practicing-stoicism) - [A Stoic reading program](#a-stoic-reading-program) ## A plan for a living Our culture provides an endless stream of distractions so people won't ever have time to think about their grand goal in living. **If you lack a grand goal in living, your lack a coherent philosophy of life.** Without one, there is a danger that you will mislive. Suppose you can identify your grand goal in living. If you lack an effective strategy for attaining your goal, it is unlikely that you will attain it. Thus, the second component of a philosophy of life is a strategy for attaining your grand goal in living. **The goal at the pinnacle of this hierarchy is the goal that we should be unwilling to sacrifice to attain other goals.** For almost everyone, the default philosophy of life is to spend one's day seeking an interesting mix of affluence, social status, and pleasure. What might be called _enlightened form of hedonism_. Stoicism and Zen have certain things in common. Both stress the importance of contemplating the transitory nature of the world around us and the importance of mastering desire, to the extent that it is possible to do so. The goal of the Stoics was not to banish _emotion_ from life but to banish _negative_ emotions. Stoics were courageous, temperate, reasonable, and self-disciplined. **They also thought important for us to fulfil our obligations and help our fellow humans.** We are unlikely to have a good and meaningful life unless we can overcome our insatiability. One wonderful way to tame our tendency to always want more is to persuade ourselves to want the things we already have. To wish, if only for a time, to be the person we are, living the life we happen to be living. The Stoic claim that many of the things we desire, most notably, fame and fortune, are not worth pursuing. That we should turn our attention to pursuit of **tranquility** and what the Stoics called _virtue_. **A state marked by the absence of negative emotions such as anger, grief, anxiety, and fear, and the presence of positive emotions, in particular joy.** We should distinguish between things we can control and things we can't, so that we will no longer worry about the things we can't control. Also to practice Stoic techniques to prevent people from upsetting us. Finally we will become a more thoughtful observer of our own life, trying to identify the sources of distress in our life and thinking about how to avoid that distress. The practice of Stoicism doesn't require us to set aside blocks of time. It does require some periods of reflection but they can be fitted into odd moments of the day like while being stuck in traffic, or lying in bed waiting for sleep to come. On assessing the \"costs\" of practicing Stoicism you should realise the costs associated with _not_ having a philosophy of life. The danger that you will spend your days pursuing valueless things and will therefore waste your life. ## The rise of Stoicism ### Philosophy takes an interest in life Before Socrates, philosophers where primarily interested in explaining the world around them and the phenomena of the world, doing science. Although Socrates studied science as a young man, he focused his attention on the human condition. In Greece and Rome, the rise of democracy meant that those who were able to persuade others were most likely to have successful careers in politics or law. Parents sought teachers who could develop their child's persuasive ability. Parents might have sought the services of sophists, whose goal was to teach pupils to win arguments. Alternatively, parents might have sought the services of a philosopher. Philosophers taught persuasive techniques, but unlike sophists, they eschewed appeals to emotion. Besides teaching their pupils how to persuade, they should teach them how to live well. Philosophers acted as live-in tutors. Parents who would not afford a private tutor would have sent their sons to a school of philosophy. **Most religions, after telling followers what they must do to be morally upstanding and get into heaven, leave to people to determine what things in life are and aren't worth pursuing.** **This is why the followers of the various religions, despite the differences in their beliefs, end up with the same impromptu philosophy of life, namely, a form of enlightened hedonism.** Similar jobs, similar career ambitions, same degree of whatever consumer products are currently in vogue. Early Stoics were interested not only in philosophy of life, but in physics and logic as well, they though these areas of study were inherently entwined. There were other schools like the Cereniacs, that thought the grand goal in living was the experience of pleasure and therefore advocated taking advantage of every opportunity to experience it. Or the Cynics, who advocated for an ascetic lifestyle: If you want good life, you must learn to want next to nothing. Stoics fell somewhere in between. **People should enjoy the good things life has to offer, including friendship and wealth, but ony if they did not cling to these good things.** ### The first stoics Zeno of Citium (333-261 BC) was the first Stoic. Zeno's father was a merchant of purple dye. As a result of shipwreck, Zeno found himself in Athens and took advantages of the philosophical resources of the city. Became the pupil of Crates the Cynic. The Cynics had little interest in philosophical theorising. They advocated for a rather extreme philosophical lifestyle. The ancient equivalent of what we today call the homeless. Zeno decided that he was more interested in theory than Crates was. Combining lifestyle with theory, the way Socrates had done. He went off to study with Stilpo, of the Megarian school. He also studied Polemo at the Academy and around 300 BC, he started his own school of philosophy. He had a mixed lifestyle advice of Crates with theoretical philosophy of Polemo. He incorporated the Megarian school's interest in logic and paradoxes. His followers were called Zenonians, but because he was in the habit of giving his lectures in the Stoa Pikile, the became know as the Stoics. One thing that made Stoicism attractive was its abandonment of Cynic asceticism: The Stoics favoured a lifestyle that, although simple, allowed creature comforts. Those who studied Stoicism started with logic, moved onto physics, and ended up with ethics. **Their interest in logic was a consequence of their belief that man's distinguishing feature is his rationality. Logic, after all, is the study of proper use of reasoning** Students would sharpen their skills of persuasion by learning logic. Students doubtless appreciated explanations of the would around them and that's why the studied physics too. Stoic ethics in contrast, is what is called eudaemonistic ethics. **It is concerned not with moral right and wrong but with having a \"good spirit\", that is, with living a good, happy life.** **Many readers will equate having a good life with making a good living, like having a high-paying job. Stoics tho, thought it possible for someone to have a bad life, despite making a very good living.** Stoics would seek \"virtue\" in order to have a good life. But in modern days, \"virtue\" invites misunderstanding. What Stoics referred with \"virtue\" meant their excellence as human beings. **To live as we were designed to live, in accordance with nature. Were were designed to be reasonable.** Since nature intended to us to be social creatures, we have duties to our fellow men. The primary ethical goal of the Greek Stoics was the attainment of virtue. The Roman Stoics retained this goal, advancing a second goal: the attainment of tranquility. **Stoic tranquility was a psychological state marked by the absence of negative emotions, such as grief, anger, and anxiety, and the presence of positive emotions, such as joy.** For Roman Stoics, the goals of attaining tranquility and attaining virtue were connected, as serenity is the result at which virtue aims. Attainment of tranquility will help us pursue virtue. Someone who is distracted by negative emotions such as anger or grief, might find it difficult to do what his reason tells him to do. The pursuit of virtue results in a degree of tranquility, which in turn makes it easier for us to pursue virtue. Roman Stoics has less confidence than the Greeks in the power of pure reason to motivate people. They concluded that by sugarcoating virtue with tranquility, they would make Stoic doctrines more attractive to ordinary Romans. There is also a possibility that by accentuating tranquility in their philosophy, might have been trying to attract students away from the Epicureans. People don't need to be convinced of the value of tranquility. ### Roman Stoicism Seneca, Musonius Rufus, Epictetus, and Marcus Aurelius were all Roman Stoics. Seneca was the best writer. Musonius offered advice on how practicing Stoics should eat, wear, behave toward their parents or even how to have sex. Epictetus specialty was analysis, and explained why practicing Stoicism can bring us tranquility. Marcus's _Meditations_ searches Stoic solutions to problems of daily life. Epictetus told his students that a Stoic school should be like a physician's consulting room and that patients should leave feeling bad rather than feeling good. He invited his audience to examine themselves. Marcus observed, in his _Meditations_, that \"the art of living is more like wrestling than dancing\". The unpopularity of Stoicism among modern individuals could be cause of these same individuals rarely see the need to adopt a philosophy of life. **They tend to spend their days working hard to be able to afford the latest consumer gadget, in the resolute belief that if only they buy enough stuff, they will have a life that is both meaningful and maximally fulfilling.** ## Stoic psychological techniques ### Negative visualisation. What's the worst that can happen? By asking us this question we may prevent things from happening. If we think about these things, we will lessen their impact on us when, despite our efforts, they happen. **Misfortune weighs most on those who \"expect nothing but good fortune\".** If we go around assuming that we will always be able to enjoy the things we value, we will likely find ourselves subject to considerable distress when the things we value are taken from us. **We humans are unhappy in large part because we are insatiable; after working hard to get what we want, we routinely lose interest in the object of our desire. This is called _hedonic adaptation_.** _Hedonic adaptation_ happens with consumer purchases or even our relationships. Once people fulfil a desire for something, they adapt to its presence in their life and as a result stop desiring it. One key to happiness, is to forestall the adaptation process: We need to take steps to prevent ourselves form taking for granted. We need a technique for creating in ourselves a desire for the things we already have, to learn how to want those things. Stoics recommend that we spend time imagining that we have lost the things we value, _negative visualisation_. **To periodically stop to reflect on the possibility that this enjoyment will come to an end. If nothing else, our own death will end it.** Besides contemplating the death of relatives, the Stoics think we should spend time contemplating the loss of friends, to death, perhaps, or to falling-out. Among the deaths, we should contemplate our own, to live each day as if it were the last. Periodically reflecting on the fact that we will not live forever. Instead of converting us into hedonists, will make us appreciate how wonderful it is that we are alive and have the opportunity to fill this day with activity. The Stoics goal is not to change our activities but to change our state of mind as we carry out those activities. **As we think about and plan for tomorrow, to remember to appreciate today.** Doing so can dramatically enhance our enjoyment of life. We should also contemplate the loss of our possessions. **Spend time thinking of all the things we have and reflecting on how much we would miss them if they were not ours.** Stoics are by no means in favour of keeping people in state of subjugation. They would work to improve their external circumstances, but suggest things they could do to alleviate their misery until those circumstances are improved. The regular practice of negative visualisation has the effect of becoming full-blown optimists. We often see an optimist as somebody that sees the glass half full rather than half empty. A Stoic would not only appreciate that the glass is half full, but also express delight on having a glass at all. Things would not have to stop there, a Stoic could even appreciate how great glass vessels are, they are cheap, durable, and impart no taste. What a miracle! Hedonic adaptation has the power to extinguish our enjoyment of the world. To take things for granted rather than delighting in them. There are people who seem proud of their inability to take delight in the world around them. They've got the idea that by refusing to take delight in the world, they are demonstrating their emotional maturity. **It's foolish to spend your life in a state of dissatisfaction when satisfaction lies within your grasp, if only you will change your mental outlook.** We can practice negative visualisation by paying attention to the bad things that happen to other people. We can imagine that the bad things that happen to us happened instead to others (_projective visualisation_). Imagine someone breaks one of our cups, instead of getting angry, imagine how would you react if the cup wasn't yours and you wanted to calm down the host. It is a mistake to think Stoics will spend _all_ their time contemplating catastrophes. It is instead something they do periodically: A few times each day or a few times each week a Stoic will pause enjoyment of life to think about how all this, all these things that enjoy, could be taken away. Furthermore, **there is a difference between _contemplating_ something bad happening and _worrying about_ it. Contemplation is an intellectual exercise that we can conduct without affecting our emotions.** Periodic episodes of grief are part of the human condition. At the same time as the practice of negative visualisation is helping us appreciate the world, it is preparing us for changes in that world. Enjoy what we have without clinging to it. By practicing negative visualisation, we cannot only increase our chances of experiencing joy but increase the chance that the joy we experience will be durable, that will survive changes in our circumstances. We need to keep firmly in mind that everything we value and the people we love will someday be lost to us. If we think about things that happen for the last time, knowing that things cannot be repeated, they will likely become extraordinary events. **By contemplating the impermanence of everything in the world, we are forced to recognise that every time we do something could be the last time we do it, and this recognition can invest the things we do with a significance and intensity that would otherwise be absent.** ### The dichotomy of control. On becoming invincible **Give up the rewards the external world has to offer in order to gain \"tranquility, freedom and calm\".** Almost every philosopher and religious thinker agree that if what you seek is contentment, **it is better an easier to change yourself and what you want that it is to change the world around you.** If you refuse to enter contests that you are capable of losing, you will never lose a contest. Wanting things that are not up to us will disrupt our tranquility. **If we don't get what we want, we will upset, and if we do get what we want, we will experience anxiety in the process of getting it.** Consider winning a tennis match. No matter how much you practice and how much hard you try, you might nevertheless lose a match. You have _some_ control, but not _complete_ control. Epictetus's dichotomy of control as a trichotomy: There are things over which we have complete control, things over which we have no control at all, and things over which we have some but no complete control. Epictetus claims that we have complete control over our opinions, impulses, desires, and aversions. However, he may be wrong, we may have no control over a spontaneous desire, but we have control on how do we decide to act. These things belong to things over which we have _some_ but not complete control. **We have complete control over the goals we set for ourselves.** Obviously we don't have complete control over whether we achieve any of them. We have complete control over our core values. We also have complete control over our opinions, as our opinions are \"up to us\". The reward for choosing our goals and values properly can be enormous. We have it in our power to assign value to things, we have it in our power to live a good life. We have complete control over our character. We are the only ones who can stop ourselves from attaining goodness and integrity. There is nothing stopping us cultivating sincerity, dignity, industriousness, and sobriety; nor is anything to stop us from takings steps to curb our arrogance, stop lusting after popularity, and to control our temper, to stop grumbling and to be considerate and frank. On the things we have no control at all, it would be foolish to spend time and energy concerning ourselves with such things. Along these lines, people might conclude that Stoics will be passive, withdrawn under-achievers; depressed individuals who might not even be able to rouse themselves from bed in the morning. This is a misinterpretation of Stoic principles, more to follow. The things over which we have complete control are the goals we set for ourselves. Be careful to set _internal_ rather than _external_ goals. **A goal for playing tennis won't be winning a match (external), but to play to the best of our ability in the match (internal)**. Since our goal wasn't winning the match, we will not have failed as long as we played our best. Playing to our best of our ability in a tennis match, and winning a tennis match are causally connected. The Stoics realised that our internal goals will affect our external performance. With the goal of winning, we arguably don't increase our chances of winning. #### Epictetus's advice We should concern ourselves with the things over we have **complete control**. For example, the goals we set for ourselves and the values we form. We should not concern ourselves with the things over which we have **no control at all**. For example, whether the sun will rise tomorrow We should concern ourselves with the things which we have **some but not complete control**, but we should be careful to internalise the goals we form with respect to them. For example, whether we win while playing tennis. --- It might be possible for someone, by spending enough time practicing goal internalisation, to develop the ability not to look beyond their internalised goals. Even if the internalisation process is a mind game, it is a useful mind game. Now that we understand the technique of internalising our goals, we can explain what seems a paradoxical behaviour for Stoics. Although Stoics value tranquility, they feel duty-bound to be active participants in the society in which they live. But such participation clearly puts their tranquility in jeopardy. Stoics goal was not to change the world, but to do their best to bring about certain changes. Even if their efforts proved to be ineffectual, they could nevertheless rest easy knowing that they had accomplished their goal: That had done what they could do. ### Fatalism. Letting go of the past... and the present One way to preserve our tranquility, is to take a fatalistic attitude toward the things that happen to us. According to Epictetus, **we should keep firmly in mind that we are merely actors in a play written by someone else, the Fates.** Regardless of the role we are assigned, we must play it to the best of our ability. **Rather than wanting events to conform to our desires, make our desires conform to events.** To do otherwise is to rebel against nature, and such rebellions are counter-productive, if what we seek is a good life. We must learn to adapt ourselves to the environment into which fate has placed us and do our best to love the people with whom fate has surrounded us. We must learn to welcome whatever falls to our lot and persuade ourselves that whatever happens to us is for the best. **The ancients were not fatalistic about the future. Stoics did not sit around apathetically, resigned to whatever the future held in store;** to the contrary, they spent their days working to affect the outcome of future events. We need to distinguish between fatalism with respect of the future and fatalism with respect to the past. Stoics advice us to be fatalistic with respect to the past, **to keep firmly in mind that the past cannot be changed**. Stoics also advocate fatalism with respect to the present. We cannot, through our actions, affect the present, if by _the present_ we mean _this very moment_. As soon as I act to affect what is happening right now, that moment in time will have slipped into the past and therefore cannot be affected. Stoics were advising us to be fatalistic, not with respect to the future but with respect to the past and present. Making us learn to be happy with whatever we've got. **We can either spend this moment wishing it could be different, or we can embrace this moment**. With the former, we will spend much of our life in a state of dissatisfaction; if we habitually do the latter, we will enjoy life. We cannot concern ourselves with things over which we have no control. We have no control over the past; nor we have any control over the present. Therefore, we are wasting our time if we worry about pas or present events. We can engage in negative visualisation to reverse it's effect: **Instead of thinking about how our situation could be worse, _we refuse to think about how it could be better_.** It will make our current situation, whatever it may be, more tolerable. People may worry that the practice of Stoicism will lead to complacency, to be terribly unambitious. But this was not the case for Seneca, Musonious Rufus, Epictetus or Marcus Aurelius that ended up ruling the Roman Empire. They would have been satisfied with next to nothing, but they strove for something nevertheless. Stoic philosophy, **while teaching us to be satisfied with whatever we've got, also counsels to seek certain things in life, like striving ot become better people (virtuous) or to do our social duty**. Stoics won't seek fame and fortune, they thought they had no real value and consequently thought it foolish to pursue them. It's interesting how Stoics like Seneca and Marcus were wealthy and famous, and Musonious and Epictetus renown because of their philosophic schools. People who thought not seeking success, nevertheless gained it. ### Self-denial. On dealing with the dark side of pleasure **Seneca recommends that besides _contemplating_ bad things happening, we should sometimes _live as if_ they had happened**. Like instead of merely thinking about what it would be like to lose our wealth, we should periodically \"practice poverty\". Stoics didn't inflict this discomforts to punish themselves; rather. they did to increase their enjoyment of life. They _welcomed_ a voluntarily degree of discomfort in their life. **This also helps us harden ourselves against misfortunes that might befall us in the future, like a vaccine.** A person who periodically experiences minor discomforts will grow confident that he can withstand major discomforts as well, so the prospect of experiencing such discomforts at some future will not, at present, be a source of anxiety for him. This technique helps us appreciate what we already have. Someone who tries to avoid all discomforts is less likely to be comfortable than someone who periodically embraces discomfort. Besides periodically engage ourselves in acts of voluntarily discomfort, **we should periodically forgo opportunities to experience pleasure.** Seneca warns us that intense pleasure, when captured by us, become our captors. Stoics reveal their Cynic bloodlines. Diogenes de Cynic argues that the most important battle any person has to fight is the battle against pleasure. There are some pleasures, from which we should always abstain. **In particular the ones that can capture us in a single encounter, like drugs.** Stoics do also recommend sometimes to abstain from other, relatively harmless pleasures like a couple of wine in order to learn self-control. **If we cannot resist pleasures, we will end up playing, Marcus says, the role of the slave.** Stoics see nothing wrong with enjoying the pleasures to be derived form friendship, family life, a meal, or even wealth, but they counsel us to be circumspect in our enjoyment of these things. There is a danger we cling to things we enjoy, we should follow Epictetus's advice and be on guard. Self-denial technique is doubtless the hardest to practice. It won't be fun saying no to things we enjoy. Stoics can transform themselves into individuals remarkable for their courage and self-control. It might not be obvious, but abstaining from pleasure can itself be pleasant, we can praise ourselves for doing so. ### Meditation. Watching ourselves practice Stoicism **Periodically meditate on the events of daily living, how we responded to these events, and how, in accordance with Stoic principles, we should have responded them.** Did someone disrupt my tranquility? Did we experience anger? Envy? Lust? Why did the day's event upset myself? Is there something I could have done to avoid getting upset? Epictetus suggests that as we go about our daily business, we should simultaneously play the roles of participant and spectator. Marcus advises us to examine each thing we do, determine our motives for doing it, and consider the value of whatever it was we were trying to accomplish. We should continually ask whether we are being governed by our reason or by something else. We should likewise be careful observers of the actions of other people. We can learn, after all, from their mistakes and their successes. Judge our progress as Stoics. We will notice that our relations with other people have changed. Our feelings aren't hurt when others tell us that we know nothing. **We will shrug off their insults and slights. We will also shrug of any praise they might direct our way.** Epictetus thinks the admiration of other people is negative barometer of our progress as Stoics. We will stop blaming, censuring, and praising others; stop boasting about ourselves and how much we know; we will blame ourselves, not external circumstances, when our desires are thwarted. We will find that we have fewer desires than we did before. Zeno suggested we will stop having dreams in which we take pleasure in disgraceful things. Our philosophy will consist on actions rather than words. What matters is our ability to live in accordance with Stoic principles. The most important sign is a change in our emotional life. We will have fewer negative emotions, less time wishing things could be different and more time enjoying things as they are. We will have a degree of tranquility that our life previously lacked. We will discover also little outburst of joy: **We will, out of the blue, feeling delighted to be the person we are, living the life we are living, in the universe we happen to inhabit.** Seneca takes his progress to be adequate as long as \"every day I reduce the number of my vices, and blame my mistakes\". ## Stoic advice ### Duty. On loving mankind Stoics counsel us not to seek fame and fortune, since doing so will likely disrupt our tranquility. We will find that other people are the source of some of the greatest delights life has to offer, including love and friendship. But we will also discover that they are the cause of the most of negative emotions we experience. Even wen other people don't do anything to us, they can disrupt our tranquility. Stoics thought that **man is by nature a social animal and therefore that we have a duty to form and maintain relationships with other people, despite the trouble they might cause us.** Our primary function is to be rational. To discover our secondary functions, we need to apply our reasoning ability. **We were designed to live among other people and interact with them in a manner that is mutually advantageous. \"Fellowship is the purpose behind our creation\".** A person who performs well the function of man will be both rational and social. Marcus goal was \"the service and harmony of all\", more precisely \"to be bound to do good to fellow-creatures and bear with them\". **We cannot simply avoid dealing with annoying people, even though doing so would make our own life easier. We should confront them and work for the common welfare.** What motivate most of us to do our duty is the fear that we will be punished if we don't. **What motivated Marcus tho, was not fear of punishment but the prospect of reward, something far better than thanks, admiration, or sympathy: experience tranquility and have all things to our liking, a good life** Throughout the millennia and across cultures, those who have thought carefully about desire have drawn the conclusion that spending our days working to get whatever it is we find ourselves wanting is unlikely to bring us either happiness or tranquility. ### Social relations. On dealing with other people Stoics are faced with a dilemma, associate with other people at the risk of having their tranquility disturbed by them; or avoid people failing to do their social duty to maintain relationships. There will be time when we must associate with annoying, misguided, or malicious people in order to work for common interests. We can, however, be selective about whom we befriend. **We should seek as friend, people who share our (proper Stoic) values and in particular, people who are doing a greater job than we are of living in accordance with these values.** Vices, Seneca warns, are contagious. **Avoid people who are simply whiny, people who find pleasure in every opportunity for complaint, it foes our tranquility.** We should be selective too about which social functions we attend, Epictetus advises us to avoid banquets given by nonphilosophers. **People tend to talk about things like gladiators, horse races, athletes, eating and drinking, and most of all, about other people. Epictetus advises us to be silent or to have few words, or to divert the talk to something appropriate.** When interacting with annoying people, we should keep in mind that there are people who also find _us_ to be annoying. Be more empathetic to individuals faults and therefore become more tolerant with them. **By letting ourselves become annoyed, we only make things worse.** Lessen the negative impact other people have on our life by controlling our thoughts about them. **Do not think about what other people are thinking except when we must do so in order to serve public interest.** People do not choose to have the faults they do, it is therefore inevitable that some people will be annoying. Marcus, advocates for _social fatalism_, that we should operate on the assumption that they are fated to behave in a certain way. It is pointless to wish they could be less annoying. \"This mortal life endures but a moment\" to be concerned with such things. Some of our most important relationships are with members of the opposite sex. Musonious says that will not have sex outside of marriage and within marriage will have it only for the purpose of having children; to have sex in other circumstances suggests lack of self-control. Marcus offers us a technique for discovering the true value of things: Analyse them into the elements that compose it. **We would be foolish to place a high value on sexual relations and more foolish still to disrupt our life in order to experience such relations.** Musonious claims that few people are happier than a person who has both a loving spouse and devoted children. ### Insults. On putting up with put-downs Insults not only about verbal abuse, but also \"insults by omission\" or physical insults like a slap. **People tend to be exquisitely sensitive to insults.** When insulting people typically become angry. There are strategies to prevent insults from angering us. **Pause, when insulted, to consider whether what the insulter said is true. If it is, there is little reason to be upset.** Pause to consider how well-informed the insulter is. He might just simply be reporting how things seem to him, if incorrect, we should calmly set him straight. Consider the source of an insult. If we respect the source, if we value his opinions, then his critical remarks shouldn't upset me. If we don't respect the source of an insult, rather than feeling hurt by his insults, I should feel relieved: If _he_ disapproves of what I am doing, then what I am doing is doubtless the right thing to do. The most appropriate comment would be \"I'm relieved you feel that way\". Those people who insult, have deeply flawed characters. Rather than deserving our anger, deserve our pity. Wile progressing in Stoicism we will become increasingly indifferent to other people's opinions of us. We will feel no sting when they insult us, like the barking of a dog. We ourselves are the source of any sting that accompanies the insult. **If we can convince ourselves that a person has done no harm by insulting us, his insults will carry no sting.** **Our values are things over which we have complete control. If something external harms us, is our own fault: We should have adopted different values.** **A wonderful way to respond to an insult is with humour.** Self-deprecating humour can be particularly effective. By laughing off an insult, we are implying that we don' take the insulter and his insults seriously. Insult the insulter without directly doing so is far more effective than a counter-insult would be. The problem with answering with humour is that requires both wit and presence of mind, so **a second way to respond to insults is with no response at all**. The advantage is that it requires no thought on our part. A non-response can be quite disconcerting to the insulter, we are robbing him of the pleasure of having upset us, and he is likely to be upset as a result. We simply don't have time for the childish behaviour of this person. There are times when it is appropriate for us to respond vigorously to an insult. Some insulters might be encouraged by our jokes or silence and might start with an endless stream of insults. In some cases, we want to admonish or punish the person who childishly insults us. If a student insults her teacher in front of the class, the teacher would be unwise to ignore the insult. You would punish the insulter not because he has wronged her, but to correct his improper behaviour. Political correctness movement has some untoward side effects. By protecting disadvantaged individuals from insults will tend to make them hypersensitive to insults. They will come to believe that they are powerless to deal with insults on their own. The best way to deal with insults directed at the disadvantaged, Epictetus (born lame and slave) would argue, is to teach members of disadvantaged groups techniques of insult self-defence. ### Grief. On vanquishing tears with reason The belief that Stoics never grieve, is mistaken. Grief emotions are an emotional reflex. But there are some strategies by which we can prevent ourselves from experiencing excessive grief. The primary grief-prevention strategy is to engage with negative visualisation. **By contemplating the deaths of those we love we will remove some of the shock we experience if they die. Rather than mourning the end of their lives, we should be thankful that we they lived at all.** This may be called **_retrospective_ negative visualisation**. We imagine never having had something that we have lost. **We replace our feelings of regret, with feelings of thanks.** Epictetus advises us to take care not to \"catch\" the grief of others. We should display sings of grief without allowing ourselves to experience grief. Grief is a negative emotion and therefore one that we should, to the extent possible, avoid experiencing. Our goal should be to help people overcome grief. \"Catching\" other's grief won't help anybody but will hurt us. ### Anger. On overcoming anti-joy If we let it, it can destroy our tranquility. Anger, says Seneca is \"brief insanity\". **We live in a world in which there is so much to be angry about, that unless we can learn to control our anger, we will perpetually angry. A waste of precious time.** Some people claim that anger can help us to be motivated. Seneca rejects this claim, after we turn it on we will be unable to turn it off. Whatever good it initially does, it will offset the harm it subsequently does. Shouldn't somebody that sees his father killed and mother raped feel angry? Not at all, but most probably he will probably do a better job of punishing and protecting if he can avoid getting angry. We are punishing people not as retribution, but for their own good, to deter them from doing again. There are individuals that are incapable of changing their behaviour, then it does not make sense to become actually angry. Although Seneca rejects the idea of allowing ourselves to become angry and disrupt our tranquility, he is open to the idea of pretending to be angry to motivate others (_feign_ anger). **To prevent anger we should fight our tendency to believe the worst about others and our tendency to jump to conclusions about their motivations.** If we are overly sensitive, we will be quick to anger. **Seneca advises us to never get too comfortable. That the reason for things to be seen as unbearable is not because they are hard but because we are soft.** If we harden ourselves in this manner, we are much less likely to be disrupted. Humour can be used to prevent ourselves from becoming angry. Think of the bad things that happen to us as being funny rather than outrageous. Contemplate the impermanence of the world around us. Many of the things we think are important in fact aren't. **When we feel ourselves getting angry about something, we should pause to consider its cosmic (in)significance.** When angry, we should force ourselves to relax our face, soften our voice, and slow our pace of walking. Our internal state will soon come to resemble our external state. Buddhists when experiencing anger, force themselves to think about love. Two opposite thoughts cannot exist in one mind at the same time. **If we find ourselves battering whoever angered us, we should apologise**. This can almost instantly repair the social damage and can prevent us from subsequently obsessing over the thing that made us angry, we do also lessen the chance that we will make mistakes again in the future. ### Personal values. On seeking fame Fame comes at a price, you'd be much better off if you'd be indifferent to social status. Stoics value their freedom, they are reluctant to do anything that wil give others power over them. If we seek social status, we give other people power over us. **Since we make it our goal to please others, we will no longer be free to please ourselves. We will have enslaved ourselves.** To retain freedom while dealing with other people we should remain indifferent to what they think of us. **We should be as dismissive of their approval as we are of their disapproval.** When others praise us, the proper response is to laugh at them says Epictetus. Marcus claims that seeking immortal fame is an \"empty, hollow thing\". Since we are dead, we will not be able to enjoy our fame. It would be foolish to think that future generations will praise us, when we find it so difficult to praise our contemporaries. We should concern ourselves with our present situation. In order to win the admiration of people, we will have to avoid their values. **We should stop to ask whether their notion of success is compatible with ours, if they are gaining the tranquility we seek.** Another way to overcome our obsession with winning the admiration of other people is to **go out of our way to do things likely to trigger their disdain**. Cato made a point of ignoring the dictates of fashion. To trigger the disdain of other people simply so you could practice ignoring their disdain. Fear of failure significantly constraints our freedom, so we even avoid attempting something to avoid the risk of public humiliation. Many people will want you to fail in your undertakings, as your success makes them look bad and uncomfortable. If you can succeed, why can't they? **Work on becoming indifferent to that others think about you.** Ironically by refusing to seek the admiration of other people you may actually succeed in gaining admiration. For many people indifference ot public opinion is a sign of self-confidence. ### Personal values. On luxurious living People typically value wealth. The material goods our wealth can buy us will win the admiration of other people and thereby confer us a degree of fame. Wealth shouldn't be worth pursuing either. **Possessing wealth won't enable to live without sorrow and won't console us in our old age. It can never bring us contentment or banish our grief.** Not needing wealth is more valuable than wealth itself. People use their wealth to finance a luxurious lifestyle, one that will win them the admiration of others. **There is a danger that if we are exposed to luxurious lifestyle, we will lose our ability to take delight in simple things.** When people become hard to please a curious things happens, they **take pride in their newly gained inability to enjoy anything but \"the best\".** These individuals have a seriously impaired their ability to enjoy life. **Stoics value highly their ability to enjoy ordinary life.** Musonius advocated a simple diet. Best to eat foods that needed little preparation; to avoid meat, a food more appropriate for wild animals. To choose food \"not for pleasure but for nourishments\". **Rather than living to eat, we should eat to live.** We should guard against acquiring a taste for delicacies, it will be difficult to stop. The more often we are tempted by a pleasure, the more danger there is that we wil succumb to it. **We should favour simple clothing, housing, and furnishings.** Dress to protect our bodies, not to impress other people. Our housing should be functional: It should do little more than keep out extreme heat and cold. People who achieve luxurious lifestyles are rarely satisfied: Experiencing luxury only whets their appetite for even more luxury. **Luxuries are not a natural desire. Natural desires like water when we are thirsty, can be satisfied; unnatural desires cannot.** **Luxury promotes vices, it makes us want things that are inessential, then we want things that are injurious.** If we forgo luxurious living, we will find that our needs are easily met. Life's necessities are cheap and easily obtainable. Learn to restrain luxury, cultivate frugality, and \"view poverty with unprejudiced eyes\". The lifestyle of a Stoic should be between that of a sage and that of an ordinary person. One person's being richer than another does not mean that the first person is better than the other. Lao Tzu observed that \"he who knows contentment is rich\". Even though Stoics don't pursue wealth, it might nevertheless acquire it. Stoics might be quite effective helping others, and they might reward them for doing so. Practice of Stoicism may be financially rewarding. As an example, a Stoic is likely to retain a large portion of income and might thereby become wealthy (prosperity paradox). **Stoicism does not require to renounce wealth; it's allowed to enjoy it and use it to the benefit of yourself and those around you.** We should keep firmly in mind that our wealth can be snatched from us, so we should prepare for the loss of it. Keep in mind that enjoyment of wealth can undermine our character and our capacity to enjoy life, so is recommended to steer clear of a luxurious lifestyle. It's perfectly acceptable to acquire wealth, as long as you do not harm others to obtain it. It is also acceptable to enjoy it as long as you don't cling to it. **It is possible to enjoy something and at the same time be indifferent to it.** Musonius and Epictetus thought that even an minimal exposure to luxurious living would corrupt us, while Seneca and Marcus thought it possible ot live in a palace without being corrupted. Stoics would have been more wary of enjoying fame than enjoying fortune. The danger that **fame will corrupt us**, is even greater, as **it trigger in us a desire for even more fame, saying things and living in a manner calculated to gain the admiration of other people.** At the same time Stoics avoid enjoying the fame it comes their way, they will not hesitate to use this fame as a tool in the performance of what they take to be their social duty. ### Exile. On surviving a change of place Exile is nothing but a change of place. Even in the worst places of exile, the exiled person will find people who are there of their own free will. Seneca says that even depriving oneself from his country, friends and family, and his property, one keeps the things that matter the most: his place in Nature and his virtue. Musonius thinks that exile deprives a person of nothing that is truly valuable. ### Old age. On being banished to a nursing home Twentysomethings aren't willing to settle for \"mere tranquility\", for them Stoicism may sound like a philosophy for losers. In extreme cases, **young people harbour a profound sense of entitlement. When the path they have chosen gets bumpy and rutted, or even becomes impassible, they are astonished.** When the world does not hand them fame and fortune on a silver platter, they realise that they must work to get it. When we are young we might have wondered what it would be like to be old. A day will come when we won't need to wonder or imagine what it would be like to be old; we will know full well. We might find ourselves banished to a nursing home. Although our new environment is physically comfortable, it is likely to be quite challenging socially. Faced with the death, we might finally be willing to settle for \"mere tranquility\", and we might, as a result, be ripe for Stoicism. The downside of failing to develop an effective philosophy of life: you end up wasting the one life you have. Seneca argues, old age has its benefits. Consider lust, the desire for sexual gratification is a major distraction in daily living. **As we age, our feelings of lust and the state of distraction that accompanies them diminish.** By causing our bodies to deteriorate, **old age causes our vices and their accessories to decay. The same ageing process needn't cause our mind ot decay.** When we are old, we know full well that we will die soon. Stoics thought the prospect of death could make our days far more enjoyable. **In our youth, it takes effort to contemplate our own death; in our later years, it takes to _void_ contemplating it. Old age therefore has a way of making us do something that, we should have been doing all along.** Stoicism is particularly well suited to our later years. Old people are more likely than young people to value the tranquility offered by Stoics. Stoicism is the best way to prepare for old age. ### Dying. On a good end to a good life Both young and old people are disturbed by the prospect of dying. Some are disturbed what might come after death, many more thought they fear that they have mislived. Having a coherent philosophy of life can make us more accepting of death. **When Stoics contemplate their own death, it is not because they long for death but because they want to get the most out of life.** Stoics thought suicide was permissible only under certain circumstances. It is wrong for us to choose to die if our living \"is helpful to many\". To choose a good death at our own hands or a pointlessly painful death through natural processes. **Musonious counsel us to end that good life with a good death, when it is possible to do so.** ### On becoming stoic. Start now and prepare to be mocked Practicing Stoicism won't be easy. It will take effort, such abandon the attainment of fame and fortune, and replace them with a new goal, namely, the attainment of tranquility. **Although it indeed takes effort to practice Stoicism, it will require considerable more effort _not_ to practice it.** Having a philosophy of life can dramatically simplify everyday living. **Decision making is relative straightforward, after all, it is hard to know what to choose when you aren't really sure what you want.** You will be likely be mocked. So it may be a good idea to keep a low philosophical profile and practice what may be called _stealth Stoicism_. People that adopt a philosophy of life are usually mocked because by adopting Stoicism, people are demonstrating that they have different values that others do, challenging their surroundings to do something they are probably reluctant to do. The reward for practicing Stocism is to experience fewer negative emotions, such as anger, grief, disappointment, and anxiety; and as a result enjoy a degree of tranquility that we previously would have been unattainable. We will also increase our changes for positive emotion by taking delight in the world around us. By practicing negative visualisation we will appreciate things we already have. We would be able to enjoy things that _can't_ be taken from us, most notably our character. Enjoying the things that _can_ be taken from us at full extent while simultaneously preparing ourselves for the loss of such things. **We need to learn how to enjoy things without feeling entitled to them and without clinging to them.** Avoid becoming individuals incapable of taking delight in anything but \"the best\". We should aim enjoy a wide range of easily obtainable things. If life snatches one source of delight, we will quickly find another to take its place. Stoic enjoyment, unlike that of a connoisseur, is eminently transferable. Enjoy the mere fact of being alive, experience joy itself. ## Stoicism for modern lives ### The decline of Stoicism Increasing corruption and depravity of Roman society made Stoicism, which calls for considerable self-control, unattractive to many Romans. Another theory is that the lack of charismatic teachers of Stoicism after the death of Epictetus caused its decline. Stoicism was also undermined by the rise of Christianity, as its claims were similar to those made by Stoicism. Christianity had one big advantage over Stoicism: it promised an afterlife in which one would be infinitely satisfied for eternity. Stoicism occasionally emerged for example when René Descartes revealed his Stoic leaning in his _Discourse on Method_. Arthur Shopenhauer used a Stoical tone with his essays \"Wisdom of Life\" and \"Counsels and Maxims\". Henry David Thoureu's masterpiece _Walden_ reveals Stoic influence. Although most of the twentieth century, Stoicism was a neglected doctrine. Modern psychology has shown that grief is a perfectly natural response to a personal tragedy. And we have to say that Stoics did not advocate that we \"bottle up\" our emotions. They did advise to prevent negative emotions. If we prevent an emotion, there will be nothing to bottle up. When people experience personal catastrophes, it is perfectly natural to experience grief. A Stoic will try to dispel whatever grief remains in him by trying to reason it out of existence. Because grief is a negative emotion, the Stoics opposed it. Some grief is inevitable in the course of a lifetime. **The goal of the Stoics was therefore not to eliminate grief but to minimise it.** > I would challenge current psychological thinking on the best way to deal with our emotions. People are less brittle and more resilient, emotionally speaking, than therapists give them credit for. \"Forced grieving\" in accordance with the principles of grief therapy, rather than curing grief, can delay the natural healing process. Psychiatrist Sally Satel and the philosopher Christina Hoff Sommers, challenges certain aspects of modern psychological therapy. \"Reticence and suppression of feelings, far from compromising one's psychological well-being, can be healthy and adaptive. For many temperaments, an excessive focus on introspection and self-disclosure is depressing\". These authors reject the doctrine, that \"uninhibited emotional openness is essential to mental health\". Modern politics present another obstacle to the acceptance of Stoicism. Politicians tell us that if we are unhappy it isn't our fault. Our unhappiness is caused by something the government did to us or is failing to do for us. To resort to politics rather than philosophy. We are encouraged to vote for the candidate using the powers of government, to make us happy. Our government and oru society determine, to a considerable extent, our external circumstances, but is at best to loose connection between our external circumstances and how happy we are. We have a duty to fight against social injustice. Stoics don't think it is helpful for people to consider themselves victims. If you consider yourself a victim, you are not going to have a good life. Stoics thought it possible for a person to retain his tranquility despite being punished for attempting to reform the society in which he lived. Stoics believed in social reform, but they also believed in personal transformation. The first step transforming society is to make their happiness depend as little as possible on their external circumstances. The second step in transforming a society is to change people's external circumstances. **Only when we assume responsibility for our happiness we will have a reasonable chance of gaining it.** This is a message that many people, having been indoctrinated by therapists and politicians, don't want to hear. Philosophers not only lost interest in Stoicism but lost interest, more generally, in philosophies of life. Modern philosophers do not think telling people how to live their life is their business. Analytical philosophers respond not answering the question you ask, but analysing the question itself. There is also one final but quite significant obstacle to accept Stoicism, it requires a certain degree of self-control. Stoics advise us to do things we don't want to do, because it is our duty to do them. **For each desire we fulfil, a new desire will pop into our head to take its place.** No matter how hard we work to satisfy our desires, we will be no closer to satisfaction than if we had fulfilled none of them. **A less obvious way to gain satisfaction is not by working on satisfy our desires but by working to master them.** We need to slow down the desire-formation process within us. Rather than wanting new things, we need to work at wanting the things we already have. The word _sacrifice_ is a bit misleading. Stoic parents for example, don't think of parenting as a burdensome task requiring endless sacrifice; they think about how wonderful it is that they have children and can make a positive difference in the lives of these children. **Our best hope at gaining happiness is to live not a life of self-indulgence but a life of self-discipline and, to a degree, self-sacrifice.** The question isn't whether self-disciplined and duty-bound people can have a happy, meaningful life; it is whether those who lack self-control and who are convinced that nothing is bigger than they are can have such a life. ### Stoicism reconsidered Stoics thought tranquility was worth pursuing, a psychological state in which we experience few negative emotions, such as anxiety, grief and fear, but an abundance of positive emotions, especially joy. Their recommendations for seeking tranquility: * We should become self-aware: Observe ourselves, periodically reflect on how we responded to the day's events. * Use our reason to overcome negative emotions: We should use reason to master our desires, to convince ourselves that things such as fame and fortune aren't worth. That pleasurable activities will disrupt our tranquility, tranquility lost will outweigh the pleasure gained. * If we find ourselves wealthy, we should enjoy our affluence, not clinging to it. * We should form and maintain relationship with others. Avoid people whose values are corrupt. * The use of techniques for dealing with the insults of others and preventing them from angering us. * The sources of human unhappiness are the insatiability and worrying about things beyond our control. * To conquer our insatiability, the Stoics advise us to engage in negative visualisation. * To curb our tendency to worry about things beyond our control, Stoics advise us to perform a kind of triage. Things we have no control over, things we have complete control, and things we have some control. **We should spend some of our time dealing with things over which we have complete control, such as goals and values, and spend most of our time dealing with things over which we have some but not complete control**. * When dealing with things that we have some but not complete control, we should be careful to internalise our goals. * Be fatalistic with respect to the external world. What happened to us in the past and what is happening to us right now are beyond our control. It's foolish to get upset about these things. How our evolutionary past contributes to our current psychological makeup. * Pain: Ancestors for whom injuries were painful were much more likely to avoid such injuries, and therefore much more likely to survive and reproduce. * Fear: Ancestors who feared lions were less likely to be eaten by one. * Anxiety: Ancestors who felt anxious about whether they had enough food were less likely to starve. * Insatiable: Ancestors who were never satisfied, who always wanted more food or better shelter, were more likely to survive and reproduce. * Pleasure: Ancestors who found sex to be pleasurable were far more likely to reproduce. * Social: Ancestors who felt drawn to other people, and who therefore joined groups of individuals, were more likely to survive and reproduce. * Status: Our ancestors formed social hierarchies within groups. Low status ran the risk of being deprived of resources or even of being driven from the group, events that could threaten his survival. Low-status males were unlikely to reproduce. Gaining social status felt good and losing it felt bad. * Reason: Ancestors who had reason ability were more likely to survive and reproduce. **Thanks precisely to our reasoning ability, we have it in our power to \"misuse\" our evolutionary inheritance.** Consider for example our ability to hear. Our ancestors had a better chance of surviving and reproducing than those who didn't, and yet modern humans rarely use their hearing ability for this purpose. **We can misuse our ability to reason so we can circumvent the behavioural tendencies that have been programmed into us by evolution.** We can use our reasoning ability to conclude that social status and more of anything we already have, may be valuable if our goal is simply to survive and reproduce, but aren't at all valuable if our goal is instead to experience tranquility while we are alive. **Evolution made us susceptible to suffering but also gave us a tool by which we can prevent much of this suffering, our reason ability.** If our goal is not merely to survive and reproduce but to enjoy a tranquil existence, the pain associated with a loss of social status isn't just useless, it is counterproductive. Other people will do things to put us in our place, socially speaking. We must use oru intellect to override the evolutionary programming that makes insults painful to us. Along similar lines, we can misuse our intellect to control our insatiability. Instead of using it to devise clever strategies to get more of everything, engage in negative visualisation. Consider anxiety too, in developed countries were people live in a remarkably safe and predictable environment, there is much less for us to worry about. We should \"misuse\" our intellect to overcome this tendency. Stoicism, understood properly, is a cure for a disease. Anxiety, grief, fear, and various other negative emotions can prevent people from experiencing a joyful existence. What about rather than going to our book-store to buy a copy of Seneca, we go to our doctor for a Xanax prescription? Wouldn't that work? Stoicism is safer than the medical alternatives, as any number of Xanax addicts will attest. Stoicism does have benefits that spill over into other areas of our life. It will cause us to gain self-confidence to handle whatever life throws our way. It will also help us appreciate our life and circumstances and may enable us to experience joy. Individual Stoics were unafraid to \"customise\" Stoicism. Stoics regarded the principles of Stoicism not as being chiselled into stone but as being moulded into clay that could, within limits, be remoulded into a form of Stoicism that people would find useful. Stoicism is a tool that would enable a person to live a good life. Who, them, should give Stoicism a try? Someone who, to begin with, seeks tranquility. No one philosophy of life is ideal for everyone. **A person is better of to adopt a less than ideal philosophy of life than to try to live with no philosophy at all.** ### Practicing Stoicism It is recommended to practice _stealth Stoicism_, keep it a secret. You can gain its benefits while avoiding the teasing and outright mockery of your surroundings. You might choose to reveal the sordid truth to them at some point. Do not try to master all the Stoic techniques all at once. A good start may be negative visualisation, to contemplate the loss of whatever you value in life. Is easy to forget to engage in negative visualisation, it is decidedly unnatural for someone who is satisfied with life to spend time thinking about the bad things that can happen. It is as important to engage in negative visualisation when times are good as it is when times are bad. You can practice at bed time or while driving to work, transforming idle time into time well spent. After mastering negative visualisation, you can become proficient in applying the trichotomy of control. Things we have no control over, thigns we have complete control over, and things we have some but not complete control over; focus your attention on the last two categories. Is an effective technique for allaying the anxieties for the non-Stoics around. \"What can you do about this situation? Nothing!\" works like a charm to avoid anxiety. Practice internalising your goals. Instead of trying to win something, try your hardest; you can reduce distress in your life by removing the feeling that you have failed to accomplish some goal. Become a psychological fatalist about the past and the present, but not about the future. Refuse to spend time engaging in \"if only\" thoughts. It is pointless to wish they could be different. Accept the past and embrace the present. Other people are the enemy of our battle for tranquility. One of the things that makes insults difficult to deal with is that they generally come as surprises. Self-deprecating humour has become the standard for insults. Matters are even worse than whoever suggests it. We become impervious to insults. One of the worst things we can do when other people annoy us is to get angry. It is a major obstacle to our tranquility. As anger can lie dormant within us and venting it feels good, iyr anger will be difficult to overcome, and learning to overcome it is one of the biggest challenges a Stoic practitioner faces. **The more you think about and understand anger, the easier it is to control it.** It is quite useful to use humour as a defence against anger. One wonderful way to avoid getting angry is to imagine yourself as a character in an absurd play. Stoics besides advising us to imagine bad things happening to us, advise us to cause bad things to happen as a result of our undertaking a program of voluntary discomfort. It requires a greater degree of self-discipline. **Whenever you undertake an activity in which public failure is a possibility, you are likely to experience butterflies in your stomach.** These feelings are an important component of the fear of failure, so by dealing with them you are working to overcome it. It is a wonderful opportunity to cause yourself psychological discomfort and to confront, and hopefully vanquish, your fear of failing. In causing yourself anxiety, you have precluded much future anxiety in your life. A new challenge is nothing, if you have survived the first time, you surely will survive the next. You play a game were can see yourself as two different people. You and your self-opponent that wants nothing more than be comfortable and take advantage of opportunities for pleasure, lacks self-discipline and always takes the path of least resistance, basically a simple-minded pleasure seeker and a coward. To win points, you must establish dominance over him. To cause him experience discomfort he could easily have avoided, and you must prevent him from experience pleasures he might otherwise have enjoyed. When he is scared of doing something, you must force him to confront his fears and overcome them. Most people come to the mall not because there is something specific that they need to buy. Rather they come in the hope that doing so will trigger a desire for something that, before going to the mall, they didn't want. By triggering a desire, they can enjoy the rush that comes when they extinguish that desire by buying something. Acquiring things makes zero difference in how happy you are. You might find yourself wishing that your Stoicism would be put to test so you can see whether you in fact possess the skills at hardship management that you have worked to acquire. A Stoic might welcome death, inasmuch as it represents the ultimate test of Stoicism. You may end up putting yourself in situations that test your courage and willpower, in part to see whether you can pass such tests. At old age it may be easier to find _negative_ role models to _avoid_ ending up like them. For some old people nonexistence is preferable ot old age. Seneca claimed that old age is one of the most delightful stages of life, a stage that is \"full of pleasure if one knows how to use it\". For Marcus, \"life is more like wrestling than like dancing\". The goal of Stoicism is the attainment of tranquility. No matter what you do, you might be making a mistake. You might be making a mistake by practicing Stoicism, You might also be making a mistake if you reject Stoicism but the biggest mistake is to have no philosophy of life at all. A better life is possible than seeking out what feels good and avoiding what feels bad. Less comfort and pleasure, but considerably more joy. **Stoic techniques can improve a life when times are good, but it is when times are bad that the efficacy of these techniques becomes most apparent.** Your biggest tests in life lie ahead. There is little to lose by giving Stoicism a try as one's philosophy of life, and there is potentially much to gain. ## A Stoic reading program - [Seneca: Dialogues and Essays](https://www.goodreads.com/book/show/1933080.Dialogues_and_Essays) - [Musonius Rufus: Stoic Fragments](https://www.goodreads.com/book/show/25895156-musonius-rufus) - [Musonius Rufus: Lectures and Sayings](https://www.goodreads.com/book/show/11041525-musonius-rufus) - [Epictetus: The Manual](https://www.goodreads.com/book/show/34946912-the-manual) - [Marcus Aurelius: Meditations](https://www.goodreads.com/book/show/35629070-the-meditations) - [The Stoics](https://www.goodreads.com/book/show/25431100-the-stoics) - [The Wisdoms of Life and Counsels and Maxims](https://www.goodreads.com/book/show/52075.The_Wisdom_of_Life_and_Counsels_and_Maxims) - [A Man in Full](https://www.goodreads.com/book/show/86172.A_Man_in_Full) - [Courage Under Fire: Testing Epictetus's Doctrines in a Laboratory of Human Behavior ](https://www.goodreads.com/book/show/196158.Courage_Under_Fire) ",
    "url": "/learning-notes/books/a-guide-to-the-good-life/",
    "relUrl": "/books/a-guide-to-the-good-life/"
  },"8": {
    "doc": "An Elegant Puzzle: Systems of Engineering Management",
    "title": "An Elegant Puzzle: Systems of Engineering Management",
    "content": "# [An Elegant Puzzle: Systems of Engineering Management](https://www.goodreads.com/book/show/45303387-an-elegant-puzzle) - [Introduction](#introduction) - [Organisations](#organisations) - [Sizing teams](#sizing-teams) - [Staying on the path to high-performing teams](#staying-on-the-path-to-high-performing-teams) - [A case against top-down global optimisation](#a-case-against-top-down-global-optimisation) - [Productivity in the age of hypergrowth](#productivity-in-the-age-of-hypergrowth) - [Where to stash your organisation risk?](#where-to-stash-your-organisation-risk) - [Succession planning](#succession-planning) - [Tools](#tools) - [Introduction to systems thinking](#introduction-to-systems-thinking) - [Product management: exploration, selection, validation](#product-management-exploration-selection-validation) - [Vision and strategies](#vision-and-strategies) - [Metrics and baselines](#metrics-and-baselines) - [Guiding broad organisational change with metrics](#guiding-broad-organisational-change-with-metrics) - [Migrations: the sole scalable fix to tech debt](#migrations-the-sole-scalable-fix-to-tech-debt) - [Running an engineering reorg](#running-an-engineering-reorg) - [Identify your controls](#identify-your-controls) - [Career narratives](#career-narratives) - [Model, document and share](#model-document-and-share) - [Scaling consistency: designing centralised decision-making groups](#scaling-consistency-designing-centralised-decision-making-groups) - [Presenting to senior leadership](#presenting-to-senior-leadership) - [Time management](#time-management) - [Communities of learning](#communities-of-learning) - [Approaches](#approaches) - [Work the policy not the exceptions](#work-the-policy-not-the-exceptions) - [Saying no](#saying-no) - [Your philosophy of management](#your-philosophy-of-management) - [Managing in the growth plates](#managing-in-the-growth-plates) - [Ways engineering managers get stuck](#ways-engineering-managers-get-stuck) - [Partnering with your manager](#partnering-with-your-manager) - [Finding managerial scope](#finding-managerial-scope) - [Setting organisation direction](#setting-organisation-direction) - [Close out, solve, or delegate](#close-out-solve-or-delegate) - [Culture](#culture) - [Opportunity and membership](#opportunity-and-membership) - [Select project leads](#select-project-leads) - [Make your peers your first team](#make-your-peers-your-first-team) - [Consider the team you have for senior positions](#consider-the-team-you-have-for-senior-positions) - [Company culture and managing freedoms](#company-culture-and-managing-freedoms) - [Kill your heroes, stop doing it harder](#kill-your-heroes-stop-doing-it-harder) - [Careers](#careers) - [Roles over rocket ships, and why hypergrowth is weak predictor of personal growth](#roles-over-rocket-ships-and-why-hypergrowth-is-weak-predictor-of-personal-growth) - [Running a humane interview process](#running-a-humane-interview-process) - [Cold sourcing: hire someone you don't know](#cold-sourcing-hire-someone-you-dont-know) - [Hiring funnel](#hiring-funnel) - [Performance management systems](#performance-management-systems) - [Career levels, designation momentum, level splits, etc.](#career-levels-designation-momentum-level-splits-etc) - [Creating specialised roles, like SRE or TPMs](#creating-specialised-roles-like-sre-or-tpms) - [Designing an interview loop](#designing-an-interview-loop) - [Appendix](#appendix) - [Tools for operating a growing organisation](#tools-for-operating-a-growing-organisation) - [Useful books](#useful-books) - [Useful papers](#useful-papers) ## Introduction Organisational design gets the right people in the right places, empowers them to make decisions, and then holds them accountable for their results. ## Organisations An organisation is a collection of people working toward a shared goal. Organisational design is the attempt ot understand why some create such energy and others create mostly heat: friction, frustration, and politics. ### Sizing teams The fundamental challenge of organisational design is sizing teams. **Managers should support six to eight engineers.** This gives them enough time for active coaching, coordinating, and furthering their team's mission by writing strategies, leading change and so on. * Fewer than four engineers tend to function as **Tech Lead Managers**, taking on a share of design and implementation work. * Managers supporting more than eight or nine engineers act as **coaches** and safety nets for problems. **Managers-of-managers should support four to six managers.** This gives them enough time to coach, align with stakeholders, and to do a reasonable amount of investment in their organisation. * Managers supporting fewer than four other managers should be in a period of active learning. * Supporting a large team of managers leaves you functioning purely as a problem-solving coach. **On-call rotations want eight engineers.** It is sometimes necessary to pool multiple teams together to reach the eight engineers necessary for a 24/7 on-call rotation. **Small teams ( If you're offered a seat on a rocket ship, don't ask what seat! Just get on. **Growth only comes from change, and that is something you can influence.** ### Running a humane interview process There is still a lot of whiteboard programming out there, mainly due inertia and coarse-grained analytics. Interviewing well is far from easy, but is fairly _simple_: 1. Be kind 2. Ensure that all interviewers agree on the role's requirements 3. Understand the signal your interviewer is checking for 4. Come prepared 5. Express interest in candidates 6. Create feedback loops for interviewers 7. Instrument and optimise as you would any conversion funnel #### Be kind Allow the candidate a few minutes to ask questions. You can't be kind if your interviewers are tightly time-constrained. **Unkind interviewers usually suffer interview burnout, give them an interview sabbatical for a month or two.** #### Finding signal Break your interview into a series of interview slots that together cover all the signals. Each skill is covered by two different interviewers for redundancy in signal detection. Effective interview formats: 1. Prepared presentations, talk about a given topic for 30 minutes. 2. Debugging or extending an existing codebase on a laptop (ideally on _their_ laptop). 3. Giving demos of an existing product or feature (ideally they one they've be working on). 4. Roleplays The key point i to keep trying new approaches that improve your chance of finding signal. #### Be prepared Being unprepared shows a disinterest in the candidate's time, your team's time, _and_ your own time. #### Deliberately express interest Make your candidate know that you're excited about them. Have every interviewer send a note to them saying that they enjoyed the interview. #### Feedback loops Pair interviews, practice interviews, and weekly sync-ups between everyone strategically involved in recruiting, work actively to improve your process. Ask every candidate to fill an anonymous Glassdoor review. #### Optimise the funnel Instrumenting the process at each phase (sourcing, phone screens, take-home tests, onsites, offers, and so on). Don't obsess about it, optimising the funnel should be the last priority. You should measure first, and optimise second. ### Cold sourcing: hire someone you don't know Sources of candidates are _referrals_ from your existing team, _inbound_ applications who apply on your jobs page, and _sourced_ candidates whom you proactively bring into your funnel. **Referrals come with two major drawbacks: personal network will always be quite small and folks tend to have relatively uniform networks.** #### Moving beyond your personal networks Hiring managers freeze up when their referral network starts to dry up. Could sourcing is reaching out directly to people you don't know. A concise, thoughtful invitation to discuss a job opportunity is an opportunity, not an infringement, especially for those who are on sites like Linkedin. #### Your first cold sourcing recipe 1. Join Linkedin (or other networks like GitHub) 2. Seed your network with some people you know because it'll increase the reach of your second-degree network. 3. Be patient, you'll get throttled pretty frequently. 4. Use the search function to identify second-degree connections to connect to. 5. When someone accepts your connection request, grab their email and send them a short, polite note inviting them to coffee or a phone call. Customisation matters less, people mostly choose to respond based on their circumstances, not on the quality of your note. Hi $THEIR_NAME I'm an engineering manager at $COMPANY, and I think you would be a great fit for $ROLE (link to your job description). Would you be willing to grab a coffee or do a phone call to discuss sometime in the next week? Best. $YOUR_NAME Run an A/B test with something more personalised or sophisticated. 6. Schedule an enjoy your coffees and chats. Figure out if there is a good mutual fit between the candidate and the role, if there is a good fit, try to get them to move forward with your process. Tell them why are you excited about the company and role discussed. 7. Keep spending an hour each week adding more connections and following up with folks who have connected. It is a practice that rewards consistency. #### Is this high-leverage work? Candidates are more excited to chat with someone who/d be managing them than they are to chat with a recruiter. It gives a signal showing that managers care about hiring enough to invest their personal energy and attention into it. Be concerned if an engineering manager spends more than an hour a week on sourcing (not including follow up chats). A clear indicator of strong recruiting organization is a close and respectful partnership between recruiting and engineering. ### Hiring funnel #### Funnel fundamentals * **Identify** where candidates tend to come from. * _Inbound_ are candidates who apply to you directly, via the jobs website, job postings on LinkedIn and other job sites. These tend to be high on volume and low on quality. * _Sourced_ are candidates you proactively find an engage with by using LinkedIn, colleagues and networking at conferences and meetups. * _Referrals_ are candidates someone at your company already knows. This is usually the most efficient source of candidates. * **Motivate** people to come interview * _Spend time_ with them, get them excited about learning from each other. * _Clearly define the role_, what they'd be doing. Be honest and a bit optimistic. * **Evaluate** if they'll be a successful addition to your team. * Be as confident as possible that the candidate will be a success in your company. * You want their motivation to increase as they're evaluated. * Minimise the amount of time invested by both your team and the candidate. * **Close**, from compensation packages to benefits, to making them feel needed. #### Instrument and optimise The first step is looking at the conversion rates across each phase and investing in improving those rate. Find what the upper bounds for each section should be, benchmarking against your peer companies is the only way. #### Extending the funnel Instead of ending your funnel at closing candidates, add a few more phases: * **Onboard**. How long does it take new hires to get up to speed? Pick a productivity metric, like commits per week. How long does it take to reach P40 productivity? * **Impact**. How impactful are the people you're hiring? You can look out at performance rating distributions on time since hiring. * **Promote**. How long does it take to get promoted after they're hired? * **Retain**. Are the people you hire staying? ### Performance management systems #### Career ladders These are the foundation of an effective performance management system. They describe the expected behaviours and responsibilities for a role. Try to make a ladder for each unique role that have at least 10 individuals. One method for reducing the fixed cost of maintaining ladders is to establish a _template_ and _shared themes_ across every ladder. This also focuses on a set of common values across the company. Each ladder is composed of levels, which describes how the role evolves in responsibility and complexity as the practitioner becomes more senior. Crisp level boundaries reduce ambiguity while promoting people. These boundaries are also important because they provide to those on the ladder where they are on the journey. Level definitions are quite effective at defining the behaviours. A good ladder allows individuals to accurately self-access. A bad ladder is ambiguous and requires deep knowledge of precedent to apply correctly. #### Performance designations The next step is to apply the ladder. People can use the ladder as a guide for self-reflection, but you'll also want to create formal feedback in the form of a performance designation, which is how an individual is performing against the expectations of their career ladder at their current level. More important than the scale used for rating, is how the ratings are calculated 1. **Self-review** written by the individual receiving the designation. Explicitly compare and contrast against their appropriate ladder and level. A [\"Brag Document\"](https://jvns.ca/blog/2017/12/31/2017--year-in-review/) may be a good idea. 2. **Peer reviews** written by peers, which are useful for recognising mentorship and leadership contributions. They are also useful for identifying problems. Peers are generally not comfortable providing negative feedback. 3. **Upward reviews**. Similar to a _peer review_, these are used to ensure manager's performance includes the perspective of the individuals they manage. 4. **Manager reviews**, written by the individuals' manager, typically a synthesis of the self-peer, and upwards reviews. With these, you can establish a _provisional designation_, which you can use as input to a calibration system. **Calibrations are rounds of reviewing performance designations and reviews, with the aim of ensuring that ratings are consistent and fair across teams.** Promotions are typically also considered during calibration process. They are pretty hard to do well. Some useful rules for calibrations: 1. **Adopt a shared quest for consistency** as a community of co-workers working together toward the correct designations. 2. **Read, don't present**, managers are effective, concise presenters. Don't allow managers to pitch their candidates, have everyone read the manager review. 3. **Compare against the ladder, not against others** 4. **Study distribution, don't enforce it**. It may be useful to review the distributions across different organisations and discuss why they appear to be deviating. Feedback for weak performance should be delivered immediately. #### Performance cycles Most companies do either annual or biannual performance cycles. ### Career levels, designation momentum, level splits, etc. If designation momentum is not taking you a direction you're happy with, set clear goals to your manager, and iterate together toward an explicit agreement on the expectations to hit the designation you are aiming for. **The goals need to be ambitious enough that your manager can successfully pitch the difficulty to their peers during calibration.** **Tit-for-that.** Calibration systems without strong process and fair referees can degrade into tit-for-that favour trading. **Level expansion**. As your company ages, it will inevitably add levels to support career progression. **Level drift.** Levels added at the top create downward pressure on existing levels. Expectations at a given level decrease over time. **Opening of the gates.** The combination of _level expansion_ and _level drift_ leads to periodic burst in which a cohort of individuals cross level boundaries together. **Career level.** The level where most individuals are not expected to progress beyond. **Time-at-level limits.** Employees who haven't yet reached _career level_ are expected to progress toward the career level at a consistent pace. **Crisis designations.** Companies sometimes find themselves in difficult situations where key individuals or even key teams that they consider to be at-risk. One of the tools for addressing this situation is to recognise these individuals' importance through elevated performance designations. These sacrifice long-term usefulness to manage short-term difficulty. Generally try to avoid this if possible. ### Creating specialised roles, like SRE or TPMs #### Challenges The major challenges while rolling out new roles are: * **Class systems**. Folks often look at new roles as less important. Sometimes are intended to reduce work for another role as opposed to having an empowering mission of their own. * **Brittle organisation**. As you move away from generalised roles and toward specialists, your organisation has far more single points of failure. * **Pattern matching**. Folks pattern match on how they've seen the role done elsewhere. People will avoid taking any steps to learn how the role is intended to function. * **Task offloading**. Individuals will view it as an opportunity to offload tasks that they find challenging, difficult or uninteresting. * **Roles too \"trivial\" to value**. Many roles start by taking on work that is viewed uninteresting. Individuals tend to view that work as trivial and unimportant. * **Roles too \"trivial\" to promote**. The work done by new roles is often valued very highly, but not viewed sufficiently \"strategic\" to merit promotion. * **Head count obstacles**. * **Recruiting rare humans**. People want the first hire for a new role to be a strong role model, which usually makes impossible for any candidate to pass the bar. * **Inability to evaluate**. The existing organisation has so little experience with the new function that don't know how to properly evaluate. This leads to evaluations focused on qualities that are independent to what the candidate would be doing. #### Facilitating success The ingredients to bootstrap a new role successfully: * **Executive sponsor**. Find an authorised, senior leader who is committed to the success of the new function. If you can't find a sponsor, it may mean that leadership doesn't believe on the new role. * **Recruiting partner**. Make sure that your recruiting team is able to support a new role. * **Self-sustaining mission**. You might describe TPMs as offloading project management responsibilities for engineering managers. This frames the role as auxiliary, which makes it difficult to recognise impact. You must be able to frame the role's work without referencing other existing roles in order for it to succeed long-term. * **Career ladder**. New roles should have a career ladder from the beginning, this is the foundation of a successful performance management. * **Role model**. You want to have a person you can point to. * **Dedicated calibrations**. Sometimes managers try to perform calibration with mixed roles in a single session, which leads to smaller roles being treated as afterthoughts. Consider dedicated calibration sessions for the one role, or consider all smaller roles together. #### Advantages Creating a new role has some advantages: * **Efficiency**. Specialised roles are able to spend more time doing a smaller set of tasks, which leads to great expertise that can be translated in significantly more overall throughput without increasing head count. * **Efficiency solve constraints**. You can add exactly the kind of capacity you are missing. If your organisation is low on project management bandwidth, you could add five new managers who can each other take on a bit of it, or you might be able to add a single project manager who individually adds as much relevant bandwidth as the five managers combined. * **Recognition**. It provides additional structural mechanisms to support recognition. * **Evaluating for strengths**. It's often challenging to interview specialists because you'll typically evaluate them based on your generalist position. Creating a new role makes it possible to target the interview process on the ares that are most important. * **Increased hiring pool**. * **Specialised compensation**. #### What to do **Create a new role if it immediately be covered with at least 20 individuals, be reluctant to create a new role if it would take 2 years to hire 20 individuals.** ### Designing an interview loop * **Metrics first**. Do not start designing a new interview loop without instrumenting your hiring funnel. * **Understand the current loop's performance**. What you think does and doesn't work well in your current process. * Funnel performance * Employee trajectory * Candidate debriefs * **Learn from peers**. Chat with folks who have interviewed at other companies for the kind of role you're hoping to evaluate. * **Find role models**. * **Identify skills** that are essential to candidate's success and rank them from most to least important. * **Test for each skill**. For each skill, design a test to evaluate the candidates' strengths. * **For each test, a rubric**. Write a rubric to assess performance on each test. These should include explicit scores and criteria for reaching each score. * **Group tests into interviews**. Group them into sets that can be performed together in a single 45-minute interview. * **Run the loop**. Early on, you should be asking candidates what did or didn't work well. * **Review the hiring funnel**. Review the funnel metrics to see how it's working out. * **Schedule an annual refresh**. Schedule a review for a year out. A few more general pieces of guidance: * **Try to avoid design by committee**. Prefer a working group of one or two people. * **Don't hire for potential**. It is a major vector for bias. * **Use your career ladder**. * **Iterate on the interview a little**. Spend time iterating on the interview format. * **Iterate on the rubric a lot**. Pay attention and incorporate edge cases and ambiguities. * **A/B testing loops**. * **Hiring committees** as an alternative to A/B testing. A centralised hiring committee that identifies trends across new loops. Avoid reusing stuff that you know doesn't work and approach the matter with creativity and iteration. ## Appendix ### Tools for operating a growing organisation One tension in management is staying far enough out of the details to let folks innovate, yet staying near enough to keep the work well-aligned with your company's value structures. #### Line management Around the time your team reaches three engineers, you'll want to be running a **spring process**. You can evaluate if a team's sprint works well: 1. _Team_ knows what they should be working on. 2. _Team_ knows why their work is valuable. 3. _Team_ can determine if their work is complete. 4. _Team_ knows how to figure out what to work on next. 5. _Stakeholders_ can learn what the team is working on. 6. _Stakeholders_ can learn what the team plans to work on next. 7. _Stakeholders_ know how to influence the team's plans. The **backlog** is a context-rich interface that you'll use to negotiate changes in direction and priority with your stakeholders. You also want to develop a **roadmap** describing your high-level plans over the three to twelve months. Your backlog will be a bit more detailed while your roadmap will look further into the future. #### Middle management As you move into middle management, you'll become responsible for two to five line managers. **You'll need to shift away from day-to-day execution to give your line managers room to make an impact**. You'll spend more time on your _roadmaps_: * From receiving asks from stakeholders to deeply understanding what is motivating those asks. * Continuously validate that your teams' efforts are valuable. * You'll want to start a **weekly staff meeting** with your managers. You can do brief updates from each attendee, at most a couple of minutes per person, and then move into group discussions on shared topics, like running effective sprints planning, career development or whatever else proves useful. * Once you start observing misalignment, it's time for each team to write a **vision document**. * **Start skip-level on-on-ones** to ensure that there are direct, open channels for feedback about your managers and your teams. #### Managing an organisation Once your organisation gets larger, you may end up managing middle managers. You have to ensure that every team has a clear set of **directional metrics** in an easily discoverable dashboard. It should cover longer-term goals of the team (user adoption, revenue, return users, etc.), and the operational baselines necessary to know if the team is functioning well (on-call load, incidents, availability, cost, etc.). The dashboard should make things clear: the current value, the goal value, and the trend between them. Your staff meetings can start with a quick metrics review to give a sense of whether there is somewhere you need to drill in. You can focus your attention on projects that are exceeding or struggling. Another interesting mechanism is **team snippets**. Every two to four weeks give snapshots of each team's sprints: what are they doing, and what they're planning to do next. ### Useful books * [Thinking in Systems: A Primer](https://www.goodreads.com/book/show/3828902-thinking-in-systems) * [Don't Think of an Elephant! Know Your values and Frame the Debate](https://www.goodreads.com/book/show/13455.Don_t_Think_of_an_Elephant_Know_Your_Values_and_Frame_the_Debate) * [Peopleware: Productive Projects and Teams](https://www.goodreads.com/book/show/67825.Peopleware) * [Slack: Getting Past Burnout, Busywork, and the Myth of Total Efficiency](https://www.goodreads.com/book/show/123715.Slack) * [The Mythical Man-Month](https://www.goodreads.com/book/show/13629.The_Mythical_Man_Month) * [Good Strategy/Bad Strategy. The Difference and Why it Matters](https://www.goodreads.com/book/show/11721966-good-strategy-bad-strategy) * [The Goal: A Process of Ongoing Improvement](https://www.goodreads.com/book/show/113934.The_Goal) * [The Five Dysfunctions of a Team](https://www.goodreads.com/book/show/21343.The_Five_Dysfunctions_of_a_Team) * [The Three Signs of a Miserable Job](https://www.goodreads.com/book/show/749937.The_Three_Signs_of_a_Miserable_Job) * [Finite and Infinite Games](https://www.goodreads.com/book/show/189989.Finite_and_Infinite_Games) * [Inspired: How to Create Tech Products Customers Love](https://www.goodreads.com/book/show/35249663-inspired) * [The Innovator's Dilemma: When New Technologies Cause Great Firms to Fail](https://www.goodreads.com/book/show/18750852-the-innovator-s-dilemma) * [The E-Myth Revisited: Why Most Small Businesses Don't Work and What to do About it](https://www.goodreads.com/book/show/81948.The_E_Myth_Revisited) * [Fierce Conversations: Achieving Success at Work and in Life, One Conversation at a Time](https://www.goodreads.com/book/show/15017.Fierce_Conversations) * [Becoming a Technical Leader: An Organic Problem-Solving Approach](https://www.goodreads.com/book/show/714344.Becoming_a_Technical_Leader) * [Designing with the Mind in Mind](https://www.goodreads.com/book/show/8564020-designing-with-the-mind-in-mind) * [The Leadership Pipeline: How to Build the Leadership Powered Company](https://www.goodreads.com/book/show/1254.The_Leadership_Pipeline) * [The Manager's Path: A Guide for Tech Leaders Navigating Growth and Change](https://www.goodreads.com/book/show/33369254-the-manager-s-path) * [High Output Management](https://www.goodreads.com/book/show/324750.High_Output_Management) * [The First 90 Days: Proven Strategies for Getting Up to Speed Faster and Smarter](https://www.goodreads.com/book/show/18491275-the-first-90-days-updated-and-expanded) * [The Effective Executive: The Definitive Guide to Getting the Right Things Done](https://www.goodreads.com/book/show/48019.The_Effective_Executive) * [Don't Make Me Think: A Common Sense Approach to Web Usability](https://www.goodreads.com/book/show/18197267-don-t-make-me-think-revisited) * [The Deadline: A Novel About Project Management](https://www.goodreads.com/book/show/123716.The_Deadline) * [The Psychology of Computer Programming](https://www.goodreads.com/book/show/1660754.The_Psychology_of_Computer_Programming) * [Adrenaline Junkies and Template Zombies: Understanding Patterns of Project Behaviour](https://www.goodreads.com/book/show/2342271.Adrenaline_Junkies_and_Template_Zombies) * [The Secrets of Consulting: A Guide to Giving and Getting Advice Successfully](https://www.goodreads.com/book/show/566213.The_Secrets_of_Consulting) * [Death by Meeting](https://www.goodreads.com/book/show/49040.Death_by_Meeting) * [The Advantage: Why Organizational Health Trumps Everything Else in Business](https://www.goodreads.com/book/show/12975375-the-advantage) * [Rise: 3 Practical Steps for Advancing Your Career, Standing Out as a Leader, and Liking Your Life](https://www.goodreads.com/book/show/12838919-rise) * [The Innovator's Solution: Creating and Sustaining Successful Growth](https://www.goodreads.com/book/show/2618.The_Innovator_s_Solution) * [The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win](https://www.goodreads.com/book/show/17255186-the-phoenix-project) * [Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organisations](https://www.goodreads.com/book/show/39080433-accelerate) ### Useful papers * [Dynamo: Amazon's Highly Available Key-Value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf) * [Hints for Computer System Design](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf) * [Big Ball of Mud](https://joeyoder.com/PDFs/mud.pdf) * [The Google File System](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf) * [On Designing and Deploying Internet-Scale Services](https://mvdirona.com/jrh/TalksAndPapers/JamesRH_Lisa.pdf) * [CAP Twelve Years Later: How the \"Rules\" Have Changed](https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/) * [Harvest, Yield, and Scalable Tolerant Systems](https://radlab.cs.berkeley.edu/people/fox/static/pubs/pdf/c18.pdf) * [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) * [Dapper, a Large-Scale Distributed Systems Tracing Infrastructure](https://research.google/pubs/pub36356/) * [Kafka: a Distributed Messaging System for Log Processing](https://notes.stephenholiday.com/Kafka.pdf) * [Wormhole: Reliable Pub-Sub to Support Geo-Replicated Internet Services](https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-sharma.pdf) * [Borg, Omega, and Kubernetes](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44843.pdf) * [Large-Scale Cluster Management at Google with Borg](https://research.google/pubs/pub43438/) * [Omega: Flexible, Scalable Schedulers for Large Compute Clusters](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41684.pdf) * [Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center](https://people.eecs.berkeley.edu/~alig/papers/mesos.pdf) * [Design Patterns for Container-Based Distributed Systems](https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf) * [Raft: In Search of an Understandable Consensus Algorithm](https://raft.github.io/raft.pdf) * [Paxos Made Simple](https://www.microsoft.com/en-us/research/publication/paxos-made-simple/) * [SWIM: Scalable Weakly-Consistent Infection-Style Process Group Membership Protocol](https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf) * [The Byzantine Generals Problem](https://www.microsoft.com/en-us/research/publication/byzantine-generals-problem/) * [Out of the Tar Pit](https://www.microsoft.com/en-us/research/publication/byzantine-generals-problem/) * [The Chubby Lock Service for Loosely-Coupled Distributed Systems](https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf) * [Bigtable: A Distributed Storage System for Structured Data](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf) * [Spanner: Google's Globally-Distributed Database](https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf) * [Security Keys: Practical Cryptographic Second Factors for the Modern Web](https://fc16.ifca.ai/preproceedings/25_Lang.pdf) * [BeyondCorp: Design to Deployment at Google](https://research.google/pubs/pub44860/) * [Availability in Globally Distributed Storage Systems](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36737.pdf) * [Still All on One Server: Perforce at Scale](https://info.perforce.com/rs/perforce/images/GoogleWhitePaper-StillAllonOneServer-PerforceatScale.pdf) * [Large-Scale Automated Refactoring Using ClangMR](https://research.google/pubs/pub41342/) * [Source Code Rejuvenation is not Refactoring](https://s3.amazonaws.com/systemsandpapers/papers/sofsem10.pdf) * [Searching for Build Debt: Experiences Managing Techncial Debt at Google](https://s3.amazonaws.com/systemsandpapers/papers/sofsem10.pdf) * [No Silver Bullet, Essence and Accident in Software Engineering](https://s3.amazonaws.com/systemsandpapers/papers/sofsem10.pdf) * [The UNIX Time-Sharing System](https://people.eecs.berkeley.edu/~brewer/cs262/unix.pdf) ",
    "url": "/learning-notes/books/an-elegant-puzzle/",
    "relUrl": "/books/an-elegant-puzzle/"
  },"9": {
    "doc": "Designing Data-Intensive Applications",
    "title": "Designing Data-Intensive Applications",
    "content": "# [Designing Data-Intensive Applications](https://www.goodreads.com/book/show/23463279-designing-data-intensive-applications) - [Reliable, scalable, and maintainable applications](#reliable-scalable-and-maintainable-applications) - [Reliability](#reliability) - [Scalability](#scalability) - [Maintainability](#maintainability) - [Data models and query language](#data-models-and-query-language) - [Relational model vs document model](#relational-model-vs-document-model) - [Query languages for data](#query-languages-for-data) - [Graph-like data models](#graph-like-data-models) - [Storage and retrieval](#storage-and-retrieval) - [Data structures that power up your database](#data-structures-that-power-up-your-database) - [Transaction processing or analytics?](#transaction-processing-or-analytics) - [Column-oriented storage](#column-oriented-storage) - [Encoding and evolution](#encoding-and-evolution) - [Formats for encoding data](#formats-for-encoding-data) - [Modes of dataflow](#modes-of-dataflow) - [Replication](#replication) - [Leaders and followers](#leaders-and-followers) - [Problems with replication lag](#problems-with-replication-lag) - [Multi-leader replication](#multi-leader-replication) - [Leaderless replication](#leaderless-replication) - [Partitioning](#partitioning) - [Partitioning and replication](#partitioning-and-replication) - [Partition of key-value data](#partition-of-key-value-data) - [Partitioning and secondary indexes](#partitioning-and-secondary-indexes) - [Rebalancing partitions](#rebalancing-partitions) - [Request routing](#request-routing) - [Transactions](#transactions) - [The slippery concept of a transaction](#the-slippery-concept-of-a-transaction) - [Weak isolation levels](#weak-isolation-levels) - [Serializability](#serializability) - [The trouble with distributed systems](#the-trouble-with-distributed-systems) - [Faults and partial failures](#faults-and-partial-failures) - [Unreliable networks](#unreliable-networks) - [Unreliable clocks](#unreliable-clocks) - [Knowledge, truth and lies](#knowledge-truth-and-lies) - [Consistency and consensus](#consistency-and-consensus) - [Consistency guarantees](#consistency-guarantees) - [Linearizability](#linearizability) - [Ordering guarantees](#ordering-guarantees) - [Distributed transactions and consensus](#distributed-transactions-and-consensus) - [Batch processing](#batch-processing) - [Batch processing with Unix tools](#batch-processing-with-unix-tools) - [Map reduce and distributed filesystems](#map-reduce-and-distributed-filesystems) - [Beyond MapReduce](#beyond-mapreduce) - [Stream processing](#stream-processing) - [Transmitting event streams](#transmitting-event-streams) - [Databases and streams](#databases-and-streams) - [Processing Streams](#processing-streams) - [The future of data systems](#the-future-of-data-systems) - [Data integration](#data-integration) - [Unbundling databases](#unbundling-databases) - [Aiming for correctness](#aiming-for-correctness) - [Doing the right thing](#doing-the-right-thing) ## Reliable, scalable, and maintainable applications A data-intensive application is typically built from standard building blocks. They usually need to: * Store data (_databases_) * Speed up reads (_caches_) * Search data (_search indexes_) * Send a message to another process asynchronously (_stream processing_) * Periodically crunch data (_batch processing_) * **Reliability**. To work _correctly_ even in the face of _adversity_. * **Scalability**. Reasonable ways of dealing with growth. * **Maintainability**. Be able to work on it _productively_. ### Reliability Typical expectations: * Application performs the function the user expected * Tolerate the user making mistakes * Its performance is good * The system prevents abuse Systems that anticipate faults and can cope with them are called _fault-tolerant_ or _resilient_. **A fault is usually defined as one component of the system deviating from its spec**, whereas _failure_ is when the system as a whole stops providing the required service to the user. You should generally **prefer tolerating faults over preventing faults**. * **Hardware faults**. Until recently redundancy of hardware components was sufficient for most applications. As data volumes increase, more applications use a larger number of machines, proportionally increasing the rate of hardware faults. **There is a move towards systems that tolerate the loss of entire machines**. A system that tolerates machine failure can be patched one node at a time, without downtime of the entire system (_rolling upgrade_). * **Software errors**. It is unlikely that a large number of hardware components will fail at the same time. Software errors are a systematic error within the system, they tend to cause many more system failures than uncorrelated hardware faults. * **Human errors**. Humans are known to be unreliable. Configuration errors by operators are a leading cause of outages. You can make systems more reliable: - Minimising the opportunities for error, peg: with admin interfaces that make easy to do the \"right thing\" and discourage the \"wrong thing\". - Provide fully featured non-production _sandbox_ environments where people can explore and experiment safely. - Automated testing. - Quick and easy recovery from human error, fast to rollback configuration changes, roll out new code gradually and tools to recompute data. - Set up detailed and clear monitoring, such as performance metrics and error rates (_telemetry_). - Implement good management practices and training. ### Scalability This is how do we cope with increased load. We need to succinctly describe the current load on the system; only then we can discuss growth questions. --- #### Twitter example Twitter main operations - Post tweet: a user can publish a new message to their followers (4.6k req/sec, over 12k req/sec peak) - Home timeline: a user can view tweets posted by the people they follow (300k req/sec) Two ways of implementing those operations: 1. Posting a tweet simply inserts the new tweet into a global collection of tweets. When a user requests their home timeline, look up all the people they follow, find all the tweets for those users, and merge them (sorted by time). This could be done with a SQL `JOIN`. 2. Maintain a cache for each user's home timeline. When a user _posts a tweet_, look up all the people who follow that user, and insert the new tweet into each of their home timeline caches. Approach 1, systems struggle to keep up with the load of home timeline queries. So the company switched to approach 2. The average rate of published tweets is almost two orders of magnitude lower than the rate of home timeline reads. Downside of approach 2 is that posting a tweet now requires a lot of extra work. Some users have over 30 million followers. A single tweet may result in over 30 million writes to home timelines. Twitter moved to an hybrid of both approaches. Tweets continue to be fanned out to home timelines but a small number of users with a very large number of followers are fetched separately and merged with that user's home timeline when it is read, like in approach 1. --- #### Describing performance What happens when the load increases: * How is the performance affected? * How much do you need to increase your resources? In a batch processing system such as Hadoop, we usually care about _throughput_, or the number of records we can process per second. > ##### Latency and response time > The response time is what the client sees. Latency is the duration that a request is waiting to be handled. It's common to see the _average_ response time of a service reported. However, the mean is not very good metric if you want to know your \"typical\" response time, it does not tell you how many users actually experienced that delay. **Better to use percentiles.** * _Median_ (_50th percentile_ or _p50_). Half of user requests are served in less than the median response time, and the other half take longer than the median * Percentiles _95th_, _99th_ and _99.9th_ (_p95_, _p99_ and _p999_) are good to figure out how bad your outliners are. Amazon describes response time requirements for internal services in terms of the 99.9th percentile because the customers with the slowest requests are often those who have the most data. The most valuable customers. On the other hand, optimising for the 99.99th percentile would be too expensive. _Service level objectives_ (SLOs) and _service level agreements_ (SLAs) are contracts that define the expected performance and availability of a service. An SLA may state the median response time to be less than 200ms and a 99th percentile under 1s. **These metrics set expectations for clients of the service and allow customers to demand a refund if the SLA is not met.** Queueing delays often account for large part of the response times at high percentiles. **It is important to measure times on the client side.** When generating load artificially, the client needs to keep sending requests independently of the response time. > ##### Percentiles in practice > Calls in parallel, the end-user request still needs to wait for the slowest of the parallel calls to complete. > The chance of getting a slow call increases if an end-user request requires multiple backend calls. #### Approaches for coping with load * _Scaling up_ or _vertical scaling_: Moving to a more powerful machine * _Scaling out_ or _horizontal scaling_: Distributing the load across multiple smaller machines. * _Elastic_ systems: Automatically add computing resources when detected load increase. Quite useful if load is unpredictable. Distributing stateless services across multiple machines is fairly straightforward. Taking stateful data systems from a single node to a distributed setup can introduce a lot of complexity. Until recently it was common wisdom to keep your database on a single node. ### Maintainability The majority of the cost of software is in its ongoing maintenance. There are three design principles for software systems: * **Operability**. Make it easy for operation teams to keep the system running. * **Simplicity**. Easy for new engineers to understand the system by removing as much complexity as possible. * **Evolvability**. Make it easy for engineers to make changes to the system in the future. #### Operability: making life easy for operations A good operations team is responsible for * Monitoring and quickly restoring service if it goes into bad state * Tracking down the cause of problems * Keeping software and platforms up to date * Keeping tabs on how different systems affect each other * Anticipating future problems * Establishing good practices and tools for development * Perform complex maintenance tasks, like platform migration * Maintaining the security of the system * Defining processes that make operations predictable * Preserving the organisation's knowledge about the system **Good operability means making routine tasks easy.** #### Simplicity: managing complexity When complexity makes maintenance hard, budget and schedules are often overrun. There is a greater risk of introducing bugs. Making a system simpler means removing _accidental_ complexity, as non inherent in the problem that the software solves (as seen by users). One of the best tools we have for removing accidental complexity is _abstraction_ that hides the implementation details behind clean and simple to understand APIs and facades. #### Evolvability: making change easy _Agile_ working patterns provide a framework for adapting to change. --- * _Functional requirements_: what the application should do * _Nonfunctional requirements_: general properties like security, reliability, compliance, scalability, compatibility and maintainability. --- ## Data models and query language Most applications are built by layering one data model on top of another. Each layer hides the complexity of the layers below by providing a clean data model. These abstractions allow different groups of people to work effectively. ### Relational model vs document model The roots of relational databases lie in _business data processing_, _transaction processing_ and _batch processing_. The goal was to hide the implementation details behind a cleaner interface. _Not Only SQL_ has a few driving forces: * Greater scalability * preference for free and open source software * Specialised query optimisations * Desire for a more dynamic and expressive data model **With a SQL model, if data is stored in a relational tables, an awkward translation layer is translated, this is called _impedance mismatch_.** JSON model reduces the impedance mismatch and the lack of schema is often cited as an advantage. JSON representation has better _locality_ than the multi-table SQL schema. All the relevant information is in one place, and one query is sufficient. In relational databases, it's normal to refer to rows in other tables by ID, because joins are easy. In document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak. If the database itself does not support joins, you have to emulate a join in application code by making multiple queries. The most popular database for business data processing in the 1970s was the IBM's _Information Management System_ (IMS). IMS used a _hierarchical model_ and like document databases worked well for one-to-many relationships, but it made many-to-,any relationships difficult, and it didn't support joins. #### The network model Standardised by a committee called the Conference on Data Systems Languages (CODASYL) model was a generalisation of the hierarchical model. In the tree structure of the hierarchical model, every record has exactly one parent, while in the network model, a record could have multiple parents. The links between records are like pointers in a programming language. The only way of accessing a record was to follow a path from a root record called _access path_. A query in CODASYL was performed by moving a cursor through the database by iterating over a list of records. If you didn't have a path to the data you wanted, you were in a difficult situation as it was difficult to make changes to an application's data model. #### The relational model By contrast, the relational model was a way to lay out all the data in the open\" a relation (table) is simply a collection of tuples (rows), and that's it. The query optimiser automatically decides which parts of the query to execute in which order, and which indexes to use (the access path). The relational model thus made it much easier to add new features to applications. --- **The main arguments in favour of the document data model are schema flexibility, better performance due to locality, and sometimes closer data structures to the ones used by the applications. The relation model counters by providing better support for joins, and many-to-one and many-to-many relationships.** If the data in your application has a document-like structure, then it's probably a good idea to use a document model. The relational technique of _shredding_ can lead unnecessary complicated application code. The poor support for joins in document databases may or may not be a problem. If you application does use many-to-many relationships, the document model becomes less appealing. Joins can be emulated in application code by making multiple requests. Using the document model can lead to significantly more complex application code and worse performance. #### Schema flexibility Most document databases do not enforce any schema on the data in documents. Arbitrary keys and values can be added to a document, when reading, **clients have no guarantees as to what fields the documents may contain.** Document databases are sometimes called _schemaless_, but maybe a more appropriate term is _schema-on-read_, in contrast to _schema-on-write_. Schema-on-read is similar to dynamic (runtime) type checking, whereas schema-on-write is similar to static (compile-time) type checking. The schema-on-read approach if the items on the collection don't have all the same structure (heterogeneous) * Many different types of objects * Data determined by external systems #### Data locality for queries If your application often needs to access the entire document, there is a performance advantage to this _storage locality_. The database typically needs to load the entire document, even if you access only a small portion of it. On updates, the entire document usually needs to be rewritten, it is recommended that you keep documents fairly small. #### Convergence of document and relational databases PostgreSQL does support JSON documents. RethinkDB supports relational-like joins in its query language and some MongoDB drivers automatically resolve database references. Relational and document databases are becoming more similar over time. ### Query languages for data SQL is a _declarative_ query language. In an _imperative language_, you tell the computer to perform certain operations in order. In a declarative query language you just specify the pattern of the data you want, but not _how_ to achieve that goal. A declarative query language hides implementation details of the database engine, making it possible for the database system to introduce performance improvements without requiring any changes to queries. Declarative languages often lend themselves to parallel execution while imperative code is very hard to parallelise across multiple cores because it specifies instructions that must be performed in a particular order. Declarative languages specify only the pattern of the results, not the algorithm that is used to determine results. #### Declarative queries on the web In a web browser, using declarative CSS styling is much better than manipulating styles imperatively in JavaScript. Declarative languages like SQL turned out to be much better than imperative query APIs. #### MapReduce querying _MapReduce_ is a programming model for processing large amounts of data in bulk across many machines, popularised by Google. Mongo offers a MapReduce solution. ```js db.observations.mapReduce( function map() { 2 var year = this.observationTimestamp.getFullYear(); var month = this.observationTimestamp.getMonth() + 1; emit(year + \"-\" + month, this.numAnimals); 3 }, function reduce(key, values) { 4 return Array.sum(values); 5 }, { query: { family: \"Sharks\" }, 1 out: \"monthlySharkReport\" 6 } ); ``` The `map` and `reduce` functions must be _pure_ functions, they cannot perform additional database queries and they must not have any side effects. These restrictions allow the database to run the functions anywhere, in any order, and rerun them on failure. A usability problem with MapReduce is that you have to write two carefully coordinated functions. A declarative language offers more opportunities for a query optimiser to improve the performance of a query. For there reasons, MongoDB 2.2 added support for a declarative query language called _aggregation pipeline_ ```js db.observations.aggregate([ { $match: { family: \"Sharks\" } }, { $group: { _id: { year: { $year: \"$observationTimestamp\" }, month: { $month: \"$observationTimestamp\" } }, totalAnimals: { $sum: \"$numAnimals\" } } } ]); ``` ### Graph-like data models If many-to-many relationships are very common in your application, it becomes more natural to start modelling your data as a graph. A graph consists of _vertices_ (_nodes_ or _entities_) and _edges_ (_relationships_ or _arcs_). Well-known algorithms can operate on these graphs, like the shortest path between two points, or popularity of a web page. There are several ways of structuring and querying the data. The _property graph_ model (implemented by Neo4j, Titan, and Infinite Graph) and the _triple-store_ model (implemented by Datomic, AllegroGraph, and others). There are also three declarative query languages for graphs: Cypher, SPARQL, and Datalog. #### Property graphs Each vertex consists of: * Unique identifier * Outgoing edges * Incoming edges * Collection of properties (key-value pairs) Each edge consists of: * Unique identifier * Vertex at which the edge starts (_tail vertex_) * Vertex at which the edge ends (_head vertex_) * Label to describe the kind of relationship between the two vertices * A collection of properties (key-value pairs) Graphs provide a great deal of flexibility for data modelling. Graphs are good for evolvability. --- * _Cypher_ is a declarative language for property graphs created by Neo4j * Graph queries in SQL. In a relational database, you usually know in advance which joins you need in your query. In a graph query, the number if joins is not fixed in advance. In Cypher `:WITHIN*0...` expresses \"follow a `WITHIN` edge, zero or more times\" (like the `*` operator in a regular expression). This idea of variable-length traversal paths in a query can be expressed using something called _recursive common table expressions_ (the `WITH RECURSIVE` syntax). --- #### Triple-stores and SPARQL In a triple-store, all information is stored in the form of very simple three-part statements: _subject_, _predicate_, _object_ (peg: _Jim_, _likes_, _bananas_). A triple is equivalent to a vertex in graph. #### The SPARQL query language _SPARQL_ is a query language for triple-stores using the RDF data model. #### The foundation: Datalog _Datalog_ provides the foundation that later query languages build upon. Its model is similar to the triple-store model, generalised a bit. Instead of writing a triple (_subject_, _predicate_, _object_), we write as _predicate(subject, object)_. We define _rules_ that tell the database about new predicates and rules can refer to other rules, just like functions can call other functions or recursively call themselves. Rules can be combined and reused in different queries. It's less convenient for simple one-off queries, but it can cope better if your data is complex. ## Storage and retrieval Databases need to do two things: store the data and give the data back to you. ### Data structures that power up your database Many databases use a _log_, which is append-only data file. Real databases have more issues to deal with tho (concurrency control, reclaiming disk space so the log doesn't grow forever and handling errors and partially written records). > A _log_ is an append-only sequence of records In order to efficiently find the value for a particular key, we need a different data structure: an _index_. An index is an _additional_ structure that is derived from the primary data. Well-chosen indexes speed up read queries but every index slows down writes. That's why databases don't index everything by default, but require you to choose indexes manually using your knowledge on typical query patterns. #### Hash indexes Key-value stores are quite similar to the _dictionary_ type (hash map or hash table). Let's say our storage consists only of appending to a file. The simplest indexing strategy is to keep an in-memory hash map where every key is mapped to a byte offset in the data file. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote. Bitcask (the default storage engine in Riak) does it like that. The only requirement it has is that all the keys fit in the available RAM. Values can use more space than there is available in memory, since they can be loaded from disk. A storage engine like Bitcask is well suited to situations where the value for each key is updated frequently. There are a lot of writes, but there are too many distinct keys, you have a large number of writes per key, but it's feasible to keep all keys in memory. As we only ever append to a file, so how do we avoid eventually running out of disk space? **A good solution is to break the log into segments of certain size by closing the segment file when it reaches a certain size, and making subsequent writes to a new segment file. We can then perform _compaction_ on these segments.** Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key. We can also merge several segments together at the sae time as performing the compaction. Segments are never modified after they have been written, so the merged segment is written to a new file. Merging and compaction of frozen segments can be done in a background thread. After the merging process is complete, we switch read requests to use the new merged segment instead of the old segments, and the old segment files can simply be deleted. Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to find a value for a key, we first check the most recent segment hash map; if the key is not present we check the second-most recent segment and so on. The merging process keeps the number of segments small, so lookups don't need to check many hash maps. Some issues that are important in a real implementation: * File format. It is simpler to use binary format. * Deleting records. Append special deletion record to the data file (_tombstone_) that tells the merging process to discard previous values. * Crash recovery. If restarted, the in-memory hash maps are lost. You can recover from reading each segment but that would take long time. Bitcask speeds up recovery by storing a snapshot of each segment hash map on disk. * Partially written records. The database may crash at any time. Bitcask includes checksums allowing corrupted parts of the log to be detected and ignored. * Concurrency control. As writes are appended to the log in a strictly sequential order, a common implementation is to have a single writer thread. Segments are immutable, so they can be read concurrently by multiple threads. Append-only design turns out to be good for several reasons: * Appending and segment merging are sequential write operations, much faster than random writes, especially on magnetic spinning-disks. * Concurrency and crash recovery are much simpler. * Merging old segments avoids files getting fragmented over time. Hash table has its limitations too: * The hash table must fit in memory. It is difficult to make an on-disk hash map perform well. * Range queries are not efficient. #### SSTables and LSM-Trees We introduce a new requirement to segment files: we require that the sequence of key-value pairs is _sorted by key_. We call this _Sorted String Table_, or _SSTable_. We require that each key only appears once within each merged segment file (compaction already ensures that). SSTables have few big advantages over log segments with hash indexes 1. **Merging segments is simple and efficient** (we can use algorithms like _mergesort_). When multiple segments contain the same key, we can keep the value from the most recent segment and discard the values in older segments. 2. **You no longer need to keep an index of all the keys in memory.** For a key like `handiwork`, when you know the offsets for the keys `handback` and `handsome`, you know `handiwork` must appear between those two. You can jump to the offset for `handback` and scan from there until you find `handiwork`, if not, the key is not present. You still need an in-memory index to tell you the offsets for some of the keys. One key for every few kilobytes of segment file is sufficient. 3. Since read requests need to scan over several key-value pairs in the requested range anyway, **it is possible to group those records into a block and compress it** before writing it to disk. How do we get the data sorted in the first place? With red-black trees or AVL trees, you can insert keys in any order and read them back in sorted order. * When a write comes in, add it to an in-memory balanced tree structure (_memtable_). * When the memtable gets bigger than some threshold (megabytes), write it out to disk as an SSTable file. Writes can continue to a new memtable instance. * On a read request, try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc. * From time to time, run merging and compaction in the background to discard overwritten and deleted values. If the database crashes, the most recent writes are lost. We can keep a separate log on disk to which every write is immediately appended. That log is not in sorted order, but that doesn't matter, because its only purpose is to restore the memtable after crash. Every time the memtable is written out to an SSTable, the log can be discarded. **Storage engines that are based on this principle of merging and compacting sorted files are often called LSM structure engines (Log Structure Merge-Tree).** Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a similar method for storing its _term dictionary_. LSM-tree algorithm can be slow when looking up keys that don't exist in the database. To optimise this, storage engines often use additional _Bloom filters_ (a memory-efficient data structure for approximating the contents of a set). There are also different strategies to determine the order and timing of how SSTables are compacted and merged. Mainly two _size-tiered_ and _leveled_ compaction. LevelDB and RocksDB use leveled compaction, HBase use size-tiered, and Cassandra supports both. In size-tiered compaction, newer and smaller SSTables are successively merged into older and larger SSTables. In leveled compaction, the key range is split up into smaller SSTables and older data is moved into separate \"levels\", which allows the compaction to use less disk space. #### B-trees This is the most widely used indexing structure. B-tress keep key-value pairs sorted by key, which allows efficient key-value lookups and range queries. The log-structured indexes break the database down into variable-size _segments_ typically several megabytes or more. B-trees break the database down into fixed-size _blocks_ or _pages_, traditionally 4KB. One page is designated as the _root_ and you start from there. The page contains several keys and references to child pages. If you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk. If you want to add new key, find the page and add it to the page. If there isn't enough free space in the page to accommodate the new key, it is split in two half-full pages, and the parent page is updated to account for the new subdivision of key ranges. Trees remain _balanced_. A B-tree with _n_ keys always has a depth of _O_(log _n_). The basic underlying write operation of a B-tree is to overwrite a page on disk with new data. It is assumed that the overwrite does not change the location of the page, all references to that page remain intact. This is a big contrast to log-structured indexes such as LSM-trees, which only append to files. Some operations require several different pages to be overwritten. When you split a page, you need to write the two pages that were split, and also overwrite their parent. If the database crashes after only some of the pages have been written, you end up with a corrupted index. It is common to include an additional data structure on disk: a _write-ahead log_ (WAL, also know as the _redo log_). Careful concurrency control is required if multiple threads are going to access, typically done protecting the tree internal data structures with _latches_ (lightweight locks). #### B-trees and LSM-trees LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads. Reads are typically slower on LSM-tress as they have to check several different data structures and SSTables at different stages of compaction. Advantages of LSM-trees: * LSM-trees are typically able to sustain higher write throughput than B-trees, party because they sometimes have lower write amplification: a write to the database results in multiple writes to disk. The more a storage engine writes to disk, the fewer writes per second it can handle. * LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. B-trees tend to leave disk space unused due to fragmentation. Downsides of LSM-trees: * Compaction process can sometimes interfere with the performance of ongoing reads and writes. B-trees can be more predictable. The bigger the database, the the more disk bandwidth is required for compaction. Compaction cannot keep up with the rate of incoming writes, if not configured properly you can run out of disk space. * On B-trees, each key exists in exactly one place in the index. This offers strong transactional semantics. Transaction isolation is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree. #### Other indexing structures We've only discussed key-value indexes, which are like _primary key_ index. There are also _secondary indexes_. A secondary index can be easily constructed from a key-value index. The main difference is that in a secondary index, the indexed values are not necessarily unique. There are two ways of doing this: making each value in the index a list of matching row identifiers or by making a each entry unique by appending a row identifier to it. #### Full-text search and fuzzy indexes Indexes don't allow you to search for _similar_ keys, such as misspelled words. Such _fuzzy_ querying requires different techniques. Full-text search engines allow synonyms, grammatical variations, occurrences of words near each other. Lucene uses SSTable-like structure for its term dictionary. Lucene, the in-memory index is a finite state automaton, similar to a _trie_. #### Keeping everything in memory Disks have two significant advantages: they are durable, and they have lower cost per gigabyte than RAM. It's quite feasible to keep them entirely in memory, this has lead to _in-memory_ databases. Key-value stores, such as Memcached are intended for cache only, it's acceptable for data to be lost if the machine is restarted. Other in-memory databases aim for durability, with special hardware, writing a log of changes to disk, writing periodic snapshots to disk or by replicating in-memory sate to other machines. When an in-memory database is restarted, it needs to reload its state, either from disk or over the network from a replica. The disk is merely used as an append-only log for durability, and reads are served entirely from memory. Products such as VoltDB, MemSQL, and Oracle TimesTime are in-memory databases. Redis and Couchbase provide weak durability. In-memory databases can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk. Another interesting area is that in-memory databases may provide data models that are difficult to implement with disk-based indexes. ### Transaction processing or analytics? A _transaction_ is a group of reads and writes that form a logical unit, this pattern became known as _online transaction processing_ (OLTP). _Data analytics_ has very different access patterns. A query would need to scan over a huge number of records, only reading a few columns per record, and calculates aggregate statistics. These queries are often written by business analysts, and fed into reports. This pattern became known for _online analytics processing_ (OLAP). #### Data warehousing A _data warehouse_ is a separate database that analysts can query to their heart's content without affecting OLTP operations. It contains read-only copy of the dat in all various OLTP systems in the company. Data is extracted out of OLTP databases (through periodic data dump or a continuous stream of update), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse (process _Extract-Transform-Load_ or ETL). A data warehouse is most commonly relational, but the internals of the systems can look quite different. Amazon RedShift is hosted version of ParAccel. Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill. Some of them are based on ideas from Google's Dremel. Data warehouses are used in fairly formulaic style known as a _star schema_. Facts are captured as individual events, because this allows maximum flexibility of analysis later. The fact table can become extremely large. Dimensions represent the _who_, _what_, _where_, _when_, _how_ and _why_ of the event. The name \"star schema\" comes from the fact than when the table relationships are visualised, the fact table is in the middle, surrounded by its dimension tables, like the rays of a star. Fact tables often have over 100 columns, sometimes several hundred. Dimension tables can also be very wide. ### Column-oriented storage In a row-oriented storage engine, when you do a query that filters on a specific field, the engine will load all those rows with all their fields into memory, parse them and filter out the ones that don't meet the requirement. This can take a long time. _Column-oriented storage_ is simple: don't store all the values from one row together, but store all values from each _column_ together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in a query, which can save a lot of work. Column-oriented storage often lends itself very well to compression as the sequences of values for each column look quite repetitive, which is a good sign for compression. A technique that is particularly effective in data warehouses is _bitmap encoding_. Bitmap indexes are well suited for all kinds of queries that are common in a data warehouse. > Cassandra and HBase have a concept of _column families_, which they inherited from Bigtable. Besides reducing the volume of data that needs to be loaded from disk, column-oriented storage layouts are also good for making efficient use of CPU cycles (_vectorised processing_). **Column-oriented storage, compression, and sorting helps to make read queries faster and make sense in data warehouses, where most of the load consist on large read-only queries run by analysts. The downside is that writes are more difficult.** An update-in-place approach, like B-tree use, is not possible with compressed columns. If you insert a row in the middle of a sorted table, you would most likely have to rewrite all column files. It's worth mentioning _materialised aggregates_ as some cache of the counts ant the sums that queries use most often. A way of creating such a cache is with a _materialised view_, on a relational model this is usually called a _virtual view_: a table-like object whose contents are the results of some query. A materialised view is an actual copy of the query results, written in disk, whereas a virtual view is just a shortcut for writing queries. When the underlying data changes, a materialised view needs to be updated, because it is denormalised copy of the data. Database can do it automatically, but writes would become more expensive. A common special case of a materialised view is know as a _data cube_ or _OLAP cube_, a grid of aggregates grouped by different dimensions. ## Encoding and evolution Change to an application's features also requires a change to data it stores. Relational databases conforms to one schema although that schema can be changed, there is one schema in force at any point in time. **Schema-on-read (or schemaless) contain a mixture of older and newer data formats.** In large applications changes don't happen instantaneously. You want to perform a _rolling upgrade_ and deploy a new version to a few nodes at a time, gradually working your way through all the nodes without service downtime. Old and new versions of the code, and old and new data formats, may potentially all coexist. We need to maintain compatibility in both directions * Backward compatibility, newer code can read data that was written by older code. * Forward compatibility, older code can read data that was written by newer code. ### Formats for encoding data Two different representations: * In memory * When you want to write data to a file or send it over the network, you have to encode it Thus, you need a translation between the two representations. In-memory representation to byte sequence is called _encoding_ (_serialisation_ or _marshalling_), and the reverse is called _decoding_ (_parsing_, _deserialisation_ or _unmarshalling_). Programming languages come with built-in support for encoding in-memory objects into byte sequences, but is usually a bad idea to use them. Precisely because of a few problems. * Often tied to a particular programming language. * The decoding process needs to be able to instantiate arbitrary classes and this is frequently a security hole. * Versioning * Efficiency Standardised encodings can be written and read by many programming languages. JSON, XML, and CSV are human-readable and popular specially as data interchange formats, but they have some subtle problems: * Ambiguity around the encoding of numbers and dealing with large numbers * Support of Unicode character strings, but no support for binary strings. People get around this by encoding binary data as Base64, which increases the data size by 33%. * There is optional schema support for both XML and JSON * CSV does not have any schema #### Binary encoding JSON is less verbose than XML, but both still use a lot of space compared to binary formats. There are binary encodings for JSON (MesagePack, BSON, BJSON, UBJSON, BISON and Smile), similar thing for XML (WBXML and Fast Infoset). **Apache Thrift and Protocol Buffers (protobuf) are binary encoding libraries.** Thrift offers two different protocols: * **BinaryProtocol**, there are no field names like `userName`, `favouriteNumber`. Instead the data contains _field tags_, which are numbers (`1`, `2`) * **CompactProtocol**, which is equivalent to BinaryProtocol but it packs the same information in less space. It packs the field type and the tag number into the same byte. Protocol Buffers are very similar to Thrift's CompactProtocol, bit packing is a bit different and that might allow smaller compression. Schemas inevitable need to change over time (_schema evolution_), how do Thrift and Protocol Buffers handle schema changes while keeping backward and forward compatibility changes? * **Forward compatible support**. As with new fields you add new tag numbers, old code trying to read new code, it can simply ignore not recognised tags. * **Backwards compatible support**. As long as each field has a unique tag number, new code can always read old data. Every field you add after initial deployment of schema must be optional or have a default value. Removing fields is just like adding a field with backward and forward concerns reversed. You can only remove a field that is optional, and you can never use the same tag again. What about changing the data type of a field? There is a risk that values will lose precision or get truncated. ##### Avro Apache Avro is another binary format that has two schema languages, one intended for human editing (Avro IDL), and one (based on JSON) that is more easily machine-readable. You go go through the fields in the order they appear in the schema and use the schema to tell you the datatype of each field. Any mismatch in the schema between the reader and the writer would mean incorrectly decoded data. What about schema evolution? When an application wants to encode some data, it encodes the data using whatever version of the schema it knows (_writer's schema_). When an application wants to decode some data, it is expecting the data to be in some schema (_reader's schema_). In Avro the writer's schema and the reader's schema _don't have to be the same_. The Avro library resolves the differences by looking at the writer's schema and the reader's schema. Forward compatibility means you can have a new version of the schema as writer and an old version of the schema as reader. Conversely, backward compatibility means that you can have a new version of the schema as reader and an old version as writer. To maintain compatibility, you may only add or remove a field that has a default value. If you were to add a field that has no default value, new readers wouldn't be able to read data written by old writers. Changing the datatype of a field is possible, provided that Avro can convert the type. Changing the name of a filed is tricky (backward compatible but not forward compatible). The schema is identified encoded in the data. In a large file with lots of records, the writer of the file can just include the schema at the beginning of the file. On a database with individually written records, you cannot assume all the records will have the same schema, so you have to include a version number at the beginning of every encoded record. While sending records over the network, you can negotiate the schema version on connection setup. Avro is friendlier to _dynamically generated schemas_ (dumping into a file the database). You can fairly easily generate an Avro schema in JSON. If the database schema changes, you can just generate a new Avro schema for the updated database schema and export data in the new Avro schema. By contrast with Thrift and Protocol Buffers, every time the database schema changes, you would have to manually update the mappings from database column names to field tags. --- Although textual formats such as JSON, XML and CSV are widespread, binary encodings based on schemas are also a viable option. As they have nice properties: * Can be much more compact, since they can omit field names from the encoded data. * Schema is a valuable form of documentation, required for decoding, you can be sure it is up to date. * Database of schemas allows you to check forward and backward compatibility changes. * Generate code from the schema is useful, since it enables type checking at compile time. ### Modes of dataflow Different process on how data flows between processes #### Via databases The process that writes to the database encodes the data, and the process that reads from the database decodes it. A value in the database may be written by a _newer_ version of the code, and subsequently read by an _older_ version of the code that is still running. When a new version of your application is deployed, you may entirely replace the old version with the new version within a few minutes. The same is not true in databases, the five-year-old data will still be there, in the original encoding, unless you have explicitly rewritten it. _Data outlives code_. Rewriting (_migrating_) is expensive, most relational databases allow simple schema changes, such as adding a new column with a `null` default value without rewriting existing data. When an old row is read, the database fills in `null`s for any columns that are missing. #### Via service calls You have processes that need to communicate over a network of _clients_ and _servers_. Services are similar to databases, each service should be owned by one team. and that team should be able to release versions of the service frequently, without having to coordinate with other teams. We should expect old and new versions of servers and clients to be running at the same time. _Remote procedure calls_ (RPC) tries to make a request to a remote network service look the same as calling a function or method in your programming language, it seems convenient at first but the approach is flawed: * A network request is unpredictable * A network request it may return without a result, due a _timeout_ * Retrying will cause the action to be performed multiple times, unless you build a mechanism for deduplication (_idempotence_). * A network request is much slower than a function call, and its latency is wildly variable. * Parameters need to be encoded into a sequence of bytes that can be sent over the network and becomes problematic with larger objects. * The RPC framework must translate datatypes from one language to another, not all languages have the same types. **There is no point trying to make a remote service look too much like a local object in your programming language, because it's a fundamentally different thing.** New generation of RPC frameworks are more explicit about the fact that a remote request is different from a local function call. Fiangle and Rest.li use _features_ (_promises_) to encapsulate asyncrhonous actions. RESTful API has some significant advantages like being good for experimentation and debugging. REST seems to be the predominant style for public APIs. The main focus of RPC frameworks is on requests between services owned by the same organisation, typically within the same datacenter. #### Via asynchronous message passing In an _asynchronous message-passing_ systems, a client's request (usually called a _message_) is delivered to another process with low latency. The message goes via an intermediary called a _message broker_ (_message queue_ or _message-oriented middleware_) which stores the message temporarily. This has several advantages compared to direct RPC: * It can act as a buffer if the recipient is unavailable or overloaded * It can automatically redeliver messages to a process that has crashed and prevent messages from being lost * It avoids the sender needing to know the IP address and port number of the recipient (useful in a cloud environment) * It allows one message to be sent to several recipients * **Decouples the sender from the recipient** The communication happens only in one direction. The sender doesn't wait for the message to be delivered, but simply sends it and then forgets about it (_asynchronous_). Open source implementations for message brokers are RabbitMQ, ActiveMQ, HornetQ, NATS, and Apache Kafka. One process sends a message to a named _queue_ or _topic_ and the broker ensures that the message is delivered to one or more _consumers_ or _subscribers_ to that queue or topic. Message brokers typically don't enforce a particular data model, you can use any encoding format. An _actor model_ is a programming model for concurrency in a single process. Rather than dealing with threads (and their complications), logic is encapsulated in _actors_. Each actor typically represent one client or entity, it may have some local state, and it communicates with other actors by sending and receiving asynchronous messages. Message deliver is not guaranteed. Since each actor processes only one message at a time, it doesn't need to worry about threads. In _distributed actor frameworks_, this programming model is used to scale an application across multiple nodes. It basically integrates a message broker and the actor model into a single framework. * _Akka_ uses Java's built-in serialisation by default, which does not provide forward or backward compatibility. You can replace it with something like Protocol Buffers and the ability to do rolling upgrades. * _Orleans_ by default uses custom data encoding format that does not support rolling upgrade deployments. * In _Erlang OTP_ it is surprisingly hard to make changes to record schemas. --- What happens if multiple machines are involved in storage and retrieval of data? Reasons for distribute a database across multiple machines: * Scalability * Fault tolerance/high availability * Latency, having servers at various locations worldwide ## Replication Reasons why you might want to replicate data: * To keep data geographically close to your users * Increase availability * Increase read throughput The difficulty in replication lies in handling _changes_ to replicated data. Popular algorithms for replicating changes between nodes: _single-leader_, _multi-leader_, and _leaderless_ replication. ### Leaders and followers Each node that stores a copy of the database is called a _replica_. Every write to the database needs to be processed by every replica. The most common solution for this is called _leader-based replication_ (_active/passive_ or _master-slave replication_). 1. One of the replicas is designated the _leader_ (_master_ or _primary_). Writes to the database must send requests to the leader. 2. Other replicas are known as _followers_ (_read replicas_, _slaves_, _secondaries_ or _hot stanbys_). The leader sends the data change to all of its followers as part of a _replication log_ or _change stream_. 3. Reads can be query the leader or any of the followers, while writes are only accepted on the leader. MySQL, Oracle Data Guard, SQL Server's AlwaysOn Availability Groups, MongoDB, RethinkDB, Espresso, Kafka and RabbitMQ are examples of these kind of databases. #### Synchronous vs asynchronous **The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. The disadvantage is that it the synchronous follower doesn't respond, the write cannot be processed.** It's impractical for all followers to be synchronous. If you enable synchronous replication on a database, it usually means that _one_ of the followers is synchronous, and the others are asynchronous. This guarantees up-to-date copy of the data on at least two nodes (this is sometimes called _semi-synchronous_). Often, leader-based replication is asynchronous. Writes are not guaranteed to be durable, the main advantage of this approach is that the leader can continue processing writes. #### Setting up new followers Copying data files from one node to another is typically not sufficient. Setting up a follower can usually be done without downtime. The process looks like: 1. Take a snapshot of the leader's database 2. Copy the snapshot to the follower node 3. Follower requests data changes that have happened since the snapshot was taken 4. Once follower processed the backlog of data changes since snapshot, it has _caught up_. #### Handling node outages How does high availability works with leader-based replication? #### Follower failure: catchup recovery Follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected. #### Leader failure: failover One of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader and followers need to start consuming data changes from the new leader. Automatic failover consists: 1. Determining that the leader has failed. If a node does not respond in a period of time it's considered dead. 2. Choosing a new leader. The best candidate for leadership is usually the replica with the most up-to-date changes from the old leader. 3. Reconfiguring the system to use the new leader. The system needs to ensure that the old leader becomes a follower and recognises the new leader. Things that could go wrong: * If asynchronous replication is used, the new leader may have received conflicting writes in the meantime. * Discarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents. * It could happen that two nodes both believe that they are the leader (_split brain_). Data is likely to be lost or corrupted. * What is the right time before the leader is declared dead? For these reasons, some operation teams prefer to perform failovers manually, even if the software supports automatic failover. #### Implementation of replication logs ##### Statement-based replication The leader logs every _statement_ and sends it to its followers (every `INSERT`, `UPDATE` or `DELETE`). This type of replication has some problems: * Non-deterministic functions such as `NOW()` or `RAND()` will generate different values on replicas. * Statements that depend on existing data, like auto-increments, must be executed in the same order in each replica. * Statements with side effects may result on different results on each replica. A solution to this is to replace any nondeterministic function with a fixed return value in the leader. ##### Write-ahead log (WAL) shipping The log is an append-only sequence of bytes containing all writes to the database. The leader can send it to its followers. This way of replication is used in PostgresSQL and Oracle. The main disadvantage is that the log describes the data at a very low level (like which bytes were changed in which disk blocks), coupling it to the storage engine. Usually is not possible to run different versions of the database in leaders and followers. This can have a big operational impact, like making it impossible to have a zero-downtime upgrade of the database. ##### Logical (row-based) log replication Basically a sequence of records describing writes to database tables at the granularity of a row: * For an inserted row, the new values of all columns. * For a deleted row, the information that uniquely identifies that column. * For an updated row, the information to uniquely identify that row and all the new values of the columns. A transaction that modifies several rows, generates several of such logs, followed by a record indicating that the transaction was committed. MySQL binlog uses this approach. Since logical log is decoupled from the storage engine internals, it's easier to make it backwards compatible. Logical logs are also easier for external applications to parse, useful for data warehouses, custom indexes and caches (_change data capture_). ##### Trigger-based replication There are some situations were you may need to move replication up to the application layer. A trigger lets you register custom application code that is automatically executed when a data change occurs. This is a good opportunity to log this change into a separate table, from which it can be read by an external process. Main disadvantages is that this approach has greater overheads, is more prone to bugs but it may be useful due to its flexibility. ### Problems with replication lag Node failures is just one reason for wanting replication. Other reasons are scalability and latency. In a _read-scaling_ architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this only realistically works on asynchronous replication. The more nodes you have, the likelier is that one will be down, so a fully synchronous configuration would be unreliable. With an asynchronous approach, a follower may fall behind, leading to inconsistencies in the database (_eventual consistency_). The _replication lag_ could be a fraction of a second or several seconds or even minutes. The problems that may arise and how to solve them. #### Reading your own writes _Read-after-write consistency_, also known as _read-your-writes consistency_ is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. How to implement it: * **When reading something that the user may have modified, read it from the leader.** For example, user profile information on a social network is normally only editable by the owner. A simple rule is always read the user's own profile from the leader. * You could track the time of the latest update and, for one minute after the last update, make all reads from the leader. * The client can remember the timestamp of the most recent write, then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp. * If your replicas are distributed across multiple datacenters, then any request needs to be routed to the datacenter that contains the leader. Another complication is that the same user is accessing your service from multiple devices, you may want to provide _cross-device_ read-after-write consistency. Some additional issues to consider: * Remembering the timestamp of the user's last update becomes more difficult. The metadata will need to be centralised. * If replicas are distributed across datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter. You may need to route requests from all of a user's devices to the same datacenter. #### Monotonic reads Because of followers falling behind, it's possible for a user to see things _moving backward in time_. When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward. Make sure that each user always makes their reads from the same replica. The replica can be chosen based on a hash of the user ID. If the replica fails, the user's queries will need to be rerouted to another replica. #### Consistent prefix reads If a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. This is a particular problem in partitioned (sharded) databases as there is no global ordering of writes. A solution is to make sure any writes casually related to each other are written to the same partition. #### Solutions for replication lag _Transactions_ exist so there is a way for a database to provide stronger guarantees so that the application can be simpler. ### Multi-leader replication Leader-based replication has one major downside: there is only one leader, and all writes must go through it. A natural extension is to allow more than one node to accept writes (_multi-leader_, _master-master_ or _active/active_ replication) where each leader simultaneously acts as a follower to the other leaders. #### Use cases for multi-leader replication It rarely makes sense to use multi-leader setup within a single datacenter. ##### Multi-datacenter operation You can have a leader in _each_ datacenter. Within each datacenter, regular leader-follower replication is used. Between datacenters, each datacenter leader replicates its changes to the leaders in other datacenters. Compared to a single-leader replication model deployed in multi-datacenters * **Performance.** With single-leader, every write must go across the internet to wherever the leader is, adding significant latency. In multi-leader every write is processed in the local datacenter and replicated asynchronously to other datacenters. The network delay is hidden from users and perceived performance may be better. * **Tolerance of datacenter outages.** In single-leader if the datacenter with the leader fails, failover can promote a follower in another datacenter. In multi-leader, each datacenter can continue operating independently from others. * **Tolerance of network problems.** Single-leader is very sensitive to problems in this inter-datacenter link as writes are made synchronously over this link. Multi-leader with asynchronous replication can tolerate network problems better. Multi-leader replication is implemented with Tungsten Replicator for MySQL, BDR for PostgreSQL or GoldenGate for Oracle. It's common to fall on subtle configuration pitfalls. Autoincrementing keys, triggers and integrity constraints can be problematic. Multi-leader replication is often considered dangerous territory and avoided if possible. ##### Clients with offline operation If you have an application that needs to continue to work while it is disconnected from the internet, every device that has a local database can act as a leader, and there will be some asynchronous multi-leader replication process (imagine, a Calendar application). CouchDB is designed for this mode of operation. #### Collaborative editing _Real-time collaborative editing_ applications allow several people to edit a document simultaneously. Like Etherpad or Google Docs. The user edits a document, the changes are instantly applied to their local replica and asynchronously replicated to the server and any other user. If you want to avoid editing conflicts, you must the lock the document before a user can edit it. For faster collaboration, you may want to make the unit of change very small (like a keystroke) and avoid locking. #### Handling write conflicts The biggest problem with multi-leader replication is when conflict resolution is required. This problem does not happen in a single-leader database. ##### Synchronous vs asynchronous conflict detection In single-leader the second writer can be blocked and wait the first one to complete, forcing the user to retry the write. On multi-leader if both writes are successful, the conflict is only detected asynchronously later in time. If you want synchronous conflict detection, you might as well use single-leader replication. ##### Conflict avoidance The simplest strategy for dealing with conflicts is to avoid them. If all writes for a particular record go through the sae leader, then conflicts cannot occur. On an application where a user can edit their own data, you can ensure that requests from a particular user are always routed to the same datacenter and use the leader in that datacenter for reading and writing. ##### Converging toward a consistent state On single-leader, the last write determines the final value of the field. In multi-leader, it's not clear what the final value should be. The database must resolve the conflict in a _convergent_ way, all replicas must arrive a the same final value when all changes have been replicated. Different ways of achieving convergent conflict resolution. * Five each write a unique ID (timestamp, long random number, UUID, or a has of the key and value), pick the write with the highest ID as the _winner_ and throw away the other writes. This is known as _last write wins_ (LWW) and it is dangerously prone to data loss. * Give each replica a unique ID, writes that originated at a higher-numbered replica always take precedence. This approach also implies data loss. * Somehow merge the values together. * Record the conflict and write application code that resolves it a to some later time (perhaps prompting the user). ##### Custom conflict resolution Multi-leader replication tools let you write conflict resolution logic using application code. * **On write.** As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler. * **On read.** All the conflicting writes are stored. On read, multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict. CouchDB works this way. #### Multi-leader replication topologies A _replication topology_ describes the communication paths along which writes are propagated from one node to another. The most general topology is _all-to-all_ in which every leader sends its writes to every other leader. MySQL uses _circular topology_, where each nodes receives writes from one node and forwards those writes to another node. Another popular topology has the shape of a _star_, one designated node forwards writes to all of the other nodes. In circular and star topologies a write might need to pass through multiple nodes before they reach all replicas. To prevent infinite replication loops each node is given a unique identifier and the replication log tags each write with the identifiers of the nodes it has passed through. When a node fails it can interrupt the flow of replication messages. In all-to-all topology fault tolerance is better as messages can travel along different paths avoiding a single point of failure. It has some issues too, some network links may be faster than others and some replication messages may \"overtake\" others. To order events correctly. there is a technique called _version vectors_. PostgresSQL BDR does not provide casual ordering of writes, and Tungsten Replicator for MySQL doesn't even try to detect conflicts. ### Leaderless replication Simply put, any replica can directly accept writes from clients. Databases like look like Amazon's in-house _Dynamo_ datastore. _Riak_, _Cassandra_ and _Voldemort_ follow the _Dynamo style_. In a leaderless configuration, failover does not exist. Clients send the write to all replicas in parallel. _Read requests are also sent to several nodes in parallel_. The client may get different responses. Version numbers are used to determine which value is newer. Eventually, all the data is copied to every replica. After a unavailable node come back online, it has two different mechanisms to catch up: * **Read repair.** When a client detect any stale responses, write the newer value back to that replica. * **Anti-entropy process.** There is a background process that constantly looks for differences in data between replicas and copies any missing data from one replica to he other. It does not copy writes in any particular order. #### Quorums for reading and writing If there are _n_ replicas, every write must be confirmed by _w_ nodes to be considered successful, and we must query at least _r_ nodes for each read. As long as _w_ + _r_ > _n_, we expect to get an up-to-date value when reading. _r_ and _w_ values are called _quorum_ reads and writes. Are the minimum number of votes required for the read or write to be valid. A common choice is to make _n_ and odd number (typically 3 or 5) and to set _w_ = _r_ = (_n_ + 1)/2 (rounded up). Limitations: * Sloppy quorum, the _w_ writes may end up on different nodes than the _r_ reads, so there is no longer a guaranteed overlap. * If two writes occur concurrently, and is not clear which one happened first, the only safe solution is to merge them. Writes can be lost due to clock skew. * If a write happens concurrently with a read, the write may be reflected on only some of the replicas. * If a write succeeded on some replicas but failed on others, it is not rolled back on the replicas where it succeeded. Reads may or may not return the value from that write. * If a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas storing the new value may break the quorum condition. **Dynamo-style databases are generally optimised for use cases that can tolerate eventual consistency.** #### Sloppy quorums and hinted handoff Leaderless replication may be appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads. It's likely that the client won't be able to connect to _some_ database nodes during a network interruption. * Is it better to return errors to all requests for which we cannot reach quorum of _w_ or _r_ nodes? * Or should we accept writes anyway, and write them to some nodes that are reachable but aren't among the _n_ nodes on which the value usually lives? The latter is known as _sloppy quorum_: writes and reads still require _w_ and _r_ successful responses, but those may include nodes that are not among the designated _n_ \"home\" nodes for a value. Once the network interruption is fixed, any writes are sent to the appropriate \"home\" nodes (_hinted handoff_). Sloppy quorums are useful for increasing write availability: as long as any _w_ nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of _n_. ##### Multi-datacenter operation Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgement from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on cross-datacenter link. #### Detecting concurrent writes In order to become eventually consistent, the replicas should converge toward the same value. If you want to avoid losing data, you application developer, need to know a lot about the internals of your database's conflict handling. * **Last write wins (discarding concurrent writes).** Even though the writes don' have a natural ordering, we can force an arbitrary order on them. We can attach a timestamp to each write and pick the most recent. There are some situations such caching on which lost writes are acceptable. If losing data is not acceptable, LWW is a poor choice for conflict resolution. * **The \"happens-before\" relationship and concurrency.** Whether one operation happens before another operation is the key to defining what concurrency means. **We can simply say that to operations are _concurrent_ if neither happens before the other.** Either A happened before B, or B happened before A, or A and B are concurrent. ##### Capturing the happens-before relationship The server can determine whether two operations are concurrent by looking at the version numbers. * The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along the value written. * Client reads a key, the server returns all values that have not been overwrite, as well as the latest version number. A client must read a key before writing. * Client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. * Server receives a write with a particular version number, it can overwrite all values with that version number or below, but it must keep all values with a higher version number. ##### Merging concurrently written values No data is silently dropped. It requires clients do some extra work, they have to clean up afterward by merging the concurrently written values. Riak calls these concurrent values _siblings_. Merging sibling values is the same problem as conflict resolution in multi-leader replication. A simple approach is to just pick one of the values on a version number or timestamp (last write wins). You may need to do something more intelligent in application code to avoid losing data. If you want to allow people to _remove_ things, union of siblings may not yield the right result. An item cannot simply be deleted from the database when it is removed, the system must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings (_tombstone_). Merging siblings in application code is complex and error-prone, there are efforts to design data structures that can perform this merging automatically (CRDTs). #### Version vectors We need a version number _per replica_ as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas. The collection of version numbers from all the replicas is called a _version vector_. Version vector are sent from the database replicas to clients when values are read, and need to be sent back to the database when a value is subsequently written. Riak calls this _casual context_. Version vectors allow the database to distinguish between overwrites and concurrent writes. ## Partitioning Replication, for very large datasets or very high query throughput is not sufficient, we need to break the data up into _partitions_ (_sharding_). Basically, each partition is a small database of its own. The main reason for wanting to partition data is _scalability_, query load can be load cabe distributed across many processors. Throughput can be scaled by adding more nodes. ### Partitioning and replication Each record belongs to exactly one partition, it may still be stored on several nodes for fault tolerance. A node may store more than one partition. ### Partition of key-value data Our goal with partitioning is to spread the data and the query load evenly across nodes. If partition is unfair, we call it _skewed_. It makes partitioning much less effective. A partition with disproportionately high load is called a _hot spot_. The simplest approach is to assign records to nodes randomly. The main disadvantage is that if you are trying to read a particular item, you have no way of knowing which node it is on, so you have to query all nodes in parallel. #### Partition by key range Assign a continuous range of keys, like the volumes of a paper encyclopaedia. Boundaries might be chose manually by an administrator, or the database can choose them automatically. On each partition, keys are in sorted order so scans are easy. The downside is that certain access patterns can lead to hot spots. #### Partitioning by hash of key A good hash function takes skewed data and makes it uniformly distributed. There is no need to be cryptographically strong (MongoDB uses MD5 and Cassandra uses Murmur3). You can assign each partition a range of hashes. The boundaries can be evenly spaced or they can be chosen pseudorandomly (_consistent hashing_). Unfortunately we lose the ability to do efficient range queries. Keys that were once adjacent are now scattered across all the partitions. Any range query has to be sent to all partitions. #### Skewed workloads and relieving hot spots You can't avoid hot spots entirely. For example, you may end up with large volume of writes to the same key. It's the responsibility of the application to reduce the skew. A simple technique is to add a random number to the beginning or end of the key. Splitting writes across different keys, makes reads now to do some extra work and combine them. ### Partitioning and secondary indexes The situation gets more complicated if secondary indexes are involved. A secondary index usually doesn't identify the record uniquely. They don't map neatly to partitions. #### Partitioning secondary indexes by document Each partition maintains its secondary indexes, covering only the documents in that partition (_local index_). You need to send the query to _all_ partitions, and combine all the results you get back (_scatter/gather_). This is prone to tail latency amplification and is widely used in MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud and VoltDB. #### Partitioning secondary indexes by term We construct a _global index_ that covers data in all partitions. The global index must also be partitioned so it doesn't become the bottleneck. It is called the _term-partitioned_ because the term we're looking for determines the partition of the index. Partitioning by term can be useful for range scans, whereas partitioning on a hash of the term gives a more even distribution load. The advantage is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. The downside of a global index is that writes are slower and complicated. ### Rebalancing partitions The process of moving load from one node in the cluster to another. Strategies for rebalancing: * **How not to do it: Hash mod n.** The problem with _mod N_ is that if the number of nodes _N_ changes, most of the keys will need to be moved from one node to another. * **Fixed number of partitions.** Create many more partitions than there are nodes and assign several partitions to each node. If a node is added to the cluster, we can _steal_ a few partitions from every existing node until partitions are fairly distributed once again. The number of partitions does not change, nor does the assignment of keys to partitions. The only thing that change is the assignment of partitions to nodes. This is used in Riak, Elasticsearch, Couchbase, and Voldemport. **You need to choose a high enough number of partitions to accomodate future growth.** Neither too big or too small. * **Dynamic partitioning.** The number of partitions adapts to the total data volume. An empty database starts with an empty partition. While the dataset is small, all writes have to processed by a single node while the others nodes sit idle. HBase and MongoDB allow an initial set of partitions to be configured (_pre-splitting_). * **Partitioning proportionally to nodes.** Cassandra and Ketama make the number of partitions proportional to the number of nodes. Have a fixed number of partitions _per node_. This approach also keeps the size of each partition fairly stable. #### Automatic versus manual rebalancing Fully automated rebalancing may seem convenient but the process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress. It can be good to have a human in the loop for rebalancing. You may avoid operational surprises. ### Request routing This problem is also called _service discovery_. There are different approaches: 1. Allow clients to contact any node and make them handle the request directly, or forward the request to the appropriate node. 2. Send all requests from clients to a routing tier first that acts as a partition-aware load balancer. 3. Make clients aware of the partitioning and the assignment of partitions to nodes. In many cases the problem is: how does the component making the routing decision learn about changes in the assignment of partitions to nodes? Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. The routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. HBase, SolrCloud and Kafka use ZooKeeper to track partition assignment. MongoDB relies on its own _config server_. Cassandra and Riak take a different approach: they use a _gossip protocol_. #### Parallel query execution _Massively parallel processing_ (MPP) relational database products are much more sophisticated in the types of queries they support. ## Transactions Implementing fault-tolerant mechanisms is a lot of work. ### The slippery concept of a transaction _Transactions_ have been the mechanism of choice for simplifying these issues. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (_commit_) or it fails (_abort_, _rollback_). The application is free to ignore certain potential error scenarios and concurrency issues (_safety guarantees_). #### ACID * **Atomicity.** Is _not_ about concurrency. It is what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed. _Abortability_ would have been a better term than _atomicity_. * **Consistency.** _Invariants_ on your data must always be true. The idea of consistency depends on the application's notion of invariants. Atomicity, isolation, and durability are properties of the database, whereas consistency (in an ACID sense) is a property of the application. * **Isolation.** Concurrently executing transactions are isolated from each other. It's also called _serializability_, each transaction can pretend that it is the only transaction running on the entire database, and the result is the same as if they had run _serially_ (one after the other). * **Durability.** Once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes. In a single-node database this means the data has been written to nonvolatile storage. In a replicated database it means the data has been successfully copied to some number of nodes. Atomicity can be implemented using a log for crash recovery, and isolation can be implemented using a lock on each object, allowing only one thread to access an object at any one time. **A transaction is a mechanism for grouping multiple operations on multiple objects into one unit of execution.** #### Handling errors and aborts A key feature of a transaction is that it can be aborted and safely retried if an error occurred. In datastores with leaderless replication is the application's responsibility to recover from errors. The whole point of aborts is to enable safe retries. ### Weak isolation levels Concurrency issues (race conditions) come into play when one transaction reads data that is concurrently modified by another transaction, or when two transactions try to simultaneously modify the same data. Databases have long tried to hide concurrency issues by providing _transaction isolation_. In practice, is not that simple. Serializable isolation has a performance cost. It's common for systems to use weaker levels of isolation, which protect against _some_ concurrency issues, but not all. Weak isolation levels used in practice: #### Read committed It makes two guarantees: 1. When reading from the database, you will only see data that has been committed (no _dirty reads_). Writes by a transaction only become visible to others when that transaction commits. 2. When writing to the database, you will only overwrite data that has been committed (no _dirty writes_). Dirty writes are prevented usually by delaying the second write until the first write's transaction has committed or aborted. Most databases prevent dirty writes by using row-level locks that hold the lock until the transaction is committed or aborted. Only one transaction can hold the lock for any given object. On dirty reads, requiring read locks does not work well in practice as one long-running write transaction can force many read-only transactions to wait. For every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value. #### Snapshot isolation and repeatable read There are still plenty of ways in which you can have concurrency bugs when using this isolation level. _Nonrepeatable read_ or _read skew_, when you read at the same time you committed a change you may see temporal and inconsistent results. There are some situations that cannot tolerate such temporal inconsistencies: * **Backups.** During the time that the backup process is running, writes will continue to be made to the database. If you need to restore from such a backup, inconsistencies can become permanent. * **Analytic queries and integrity checks.** You may get nonsensical results if they observe parts of the database at different points in time. _Snapshot isolation_ is the most common solution. Each transaction reads from a _consistent snapshot_ of the database. The implementation of snapshots typically use write locks to prevent dirty writes. The database must potentially keep several different committed versions of an object (_multi-version concurrency control_ or MVCC). Read committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot for an entire transaction. How do indexes work in a multi-version database? One option is to have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction. Snapshot isolation is called _serializable_ in Oracle, and _repeatable read_ in PostgreSQL and MySQL. #### Preventing lost updates This might happen if an application reads some value from the database, modifies it, and writes it back. If two transactions do this concurrently, one of the modifications can be lost (later write _clobbers_ the earlier write). ##### Atomic write operations A solution for this it to avoid the need to implement read-modify-write cycles and provide atomic operations such us ```sql UPDATE counters SET value = value + 1 WHERE key = 'foo'; ``` MongoDB provides atomic operations for making local modifications, and Redis provides atomic operations for modifying data structures. ##### Explicit locking The application explicitly lock objects that are going to be updated. ##### Automatically detecting lost updates Allow them to execute in parallel, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle. MySQL/InnoDB's repeatable read does not detect lost updates. ##### Compare-and-set If the current value does not match with what you previously read, the update has no effect. ```SQL UPDATE wiki_pages SET content = 'new content' WHERE id = 1234 AND content = 'old content'; ``` ##### Conflict resolution and replication With multi-leader or leaderless replication, compare-and-set do not apply. A common approach in replicated databases is to allow concurrent writes to create several conflicting versions of a value (also know as _siblings_), and to use application code or special data structures to resolve and merge these versions after the fact. #### Write skew and phantoms Imagine Alice and Bob are two on-call doctors for a particular shift. Imagine both the request to leave because they are feeling unwell. Unfortunately they happen to click the button to go off call at approximately the same time. ALICE BOB ┌─ BEGIN TRANSACTION ┌─ BEGIN TRANSACTION │ │ ├─ currently_on_call = ( ├─ currently_on_call = ( │ select count(*) from doctors │ select count(*) from doctors │ where on_call = true │ where on_call = true │ and shift_id = 1234 │ and shift_id = 1234 │ ) │ ) │ // now currently_on_call = 2 │ // now currently_on_call = 2 │ │ ├─ if (currently_on_call 2) { │ │ update doctors │ │ set on_call = false │ │ where name = 'Alice' │ │ and shift_id = 1234 ├─ if (currently_on_call >= 2) { │ } │ update doctors │ │ set on_call = false └─ COMMIT TRANSACTION │ where name = 'Bob' │ and shift_id = 1234 │ } │ └─ COMMIT TRANSACTION Since database is using snapshot isolation, both checks return 2. Both transactions commit, and now no doctor is on call. The requirement of having at least one doctor has been violated. Write skew can occur if two transactions read the same objects, and then update some of those objects. You get a dirty write or lost update anomaly. Ways to prevent write skew are a bit more restricted: * Atomic operations don't help as things involve more objects. * Automatically prevent write skew requires true serializable isolation. * The second-best option in this case is probably to explicitly lock the rows that the transaction depends on. ```sql BEGIN TRANSACTION; SELECT * FROM doctors WHERE on_call = true AND shift_id = 1234 FOR UPDATE; UPDATE doctors SET on_call = false WHERE name = 'Alice' AND shift_id = 1234; COMMIT; ``` ### Serializability This is the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, _serially_, without concurrency. Basically, the database prevents _all_ possible race conditions. There are three techniques for achieving this: * Executing transactions in serial order * Two-phase locking * Serializable snapshot isolation. #### Actual serial execution The simplest way of removing concurrency problems is to remove concurrency entirely and execute only one transaction at a time, in serial order, on a single thread. This approach is implemented by VoltDB/H-Store, Redis and Datomic. ##### Encapsulating transactions in stored procedures With interactive style of transaction, a lot of time is spent in network communication between the application and the database. For this reason, systems with single-threaded serial transaction processing don't allow interactive multi-statement transactions. The application must submit the entire transaction code to the database ahead of time, as a _stored procedure_, so all the data required by the transaction is in memory and the procedure can execute very fast. There are a few pros and cons for stored procedures: * Each database vendor has its own language for stored procedures. They usually look quite ugly and archaic from today's point of view, and they lack the ecosystem of libraries. * It's harder to debug, more awkward to keep in version control and deploy, trickier to test, and difficult to integrate with monitoring. Modern implementations of stored procedures include general-purpose programming languages instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis uses Lua. ##### Partitioning Executing all transactions serially limits the transaction throughput to the speed of a single CPU. In order to scale to multiple CPU cores you can potentially partition your data and each partition can have its own transaction processing thread. You can give each CPU core its own partition. For any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions. They will be vastly slower than single-partition transactions. #### Two-phase locking (2PL) > Two-phase locking (2PL) sounds similar to two-phase _commit_ (2PC) but be aware that they are completely different things. Several transactions are allowed to concurrently read the same object as long as nobody is writing it. When somebody wants to write (modify or delete) an object, exclusive access is required. Writers don't just block other writers; they also block readers and vice versa. It protects against all the race conditions discussed earlier. Blocking readers and writers is implemented by a having lock on each object in the database. The lock is used as follows: * if a transaction want sot read an object, it must first acquire a lock in shared mode. * If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. * If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. * After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). **First phase is when the locks are acquired, second phase is when all the locks are released.** It can happen that transaction A is stuck waiting for transaction B to release its lock, and vice versa (_deadlock_). **The performance for transaction throughput and response time of queries are significantly worse under two-phase locking than under weak isolation.** A transaction may have to wait for several others to complete before it can do anything. Databases running 2PL can have unstable latencies, and they can be very slow at high percentiles. One slow transaction, or one transaction that accesses a lot of data and acquires many locks can cause the rest of the system to halt. ##### Predicate locks With _phantoms_, one transaction may change the results of another transaction's search query. In order to prevent phantoms, we need a _predicate lock_. Rather than a lock belonging to a particular object, it belongs to all objects that match some search condition. Predicate locks applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms). ##### Index-range locks Predicate locks do not perform well. Checking for matching locks becomes time-consuming and for that reason most databases implement _index-range locking_. It's safe to simplify a predicate by making it match a greater set of objects. These locks are not as precise as predicate locks would be, but since they have much lower overheads, they are a good compromise. #### Serializable snapshot isolation (SSI) It provides full serializability and has a small performance penalty compared to snapshot isolation. SSI is fairly new and might become the new default in the future. ##### Pesimistic versus optimistic concurrency control Two-phase locking is called _pessimistic_ concurrency control because if anything might possibly go wrong, it's better to wait. Serial execution is also _pessimistic_ as is equivalent to each transaction having an exclusive lock on the entire database. Serializable snapshot isolation is _optimistic_ concurrency control technique. Instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. The database is responsible for checking whether anything bad happened. If so, the transaction is aborted and has to be retried. If there is enough spare capacity, and if contention between transactions is not too high, optimistic concurrency control techniques tend to perform better than pessimistic ones. SSI is based on snapshot isolation, reads within a transaction are made from a consistent snapshot of the database. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort. The database knows which transactions may have acted on an outdated premise and need to be aborted by: * **Detecting reads of a stale MVCC object version.** The database needs to track when a transaction ignores another transaction's writes due to MVCC visibility rules. When a transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted. * **Detecting writes that affect prior reads.** As with two-phase locking, SSI uses index-range locks except that it does not block other transactions. When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. It simply notifies the transactions that the data they read may no longer be up to date. ##### Performance of serializable snapshot isolation Compared to two-phase locking, the big advantage of SSI is that one transaction doesn't need to block waiting for locks held by another transaction. Writers don't block readers, and vice versa. Compared to serial execution, SSI is not limited to the throughput of a single CPU core. Transactions can read and write data in multiple partitions while ensuring serializable isolation. The rate of aborts significantly affects the overall performance of SSI. SSI requires that read-write transactions be fairly short (long-running read-only transactions may be okay). ## The trouble with distributed systems ### Faults and partial failures A program on a single computer either works or it doesn't. There is no reason why software should be flaky (non deterministic). In a distributed systems we have no choice but to confront the messy reality of the physical world. There will be parts that are broken in an unpredictable way, while others work. Partial failures are _nondeterministic_. Things will unpredicably fail. We need to accept the possibility of partial failure and build fault-tolerant mechanism into the software. **We need to build a reliable system from unreliable components.** ### Unreliable networks Focusing on _shared-nothing systems_ the network is the only way machines communicate. The internet and most internal networks are _asynchronous packet networks_. A message is sent and the network gives no guarantees as to when it will arrive, or whether it will arrive at all. Things that could go wrong: 1. Request lost 2. Request waiting in a queue to be delivered later 3. Remote node may have failed 4. Remote node may have temporarily stoped responding 5. Response has been lost on the network 6. The response has been delayed and will be delivered later If you send a request to another node and don't receive a response, it is _impossible_ to tell why. **The usual way of handling this issue is a _timeout_**: after some time you give up waiting and assume that the response is not going to arrive. Nobody is immune to network problems. You do need to know how your software reacts to network problems to ensure that the system can recover from them. It may make sense to deliberately trigger network problems and test the system's response. If you want to be sure that a request was successful, you need a positive response from the application itself. If something has gone wrong, you have to assume that you will get no response at all. #### Timeouts and unbounded delays A long timeout means a long wait until a node is declared dead. A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead (when it could be a slowdown). Premature declaring a node is problematic, if the node is actually alive the action may end up being performed twice. When a node is declared dead, its responsibilities need to be transferred to other nodes, which places additional load on other nodes and the network. #### Network congestion and queueing - Different nodes try to send packets simultaneously to the same destination, the network switch must queue them and feed them to the destination one by one. The switch will discard packets when filled up. - If CPU cores are busy, the request is queued by the operative system, until applications are ready to handle it. - In virtual environments, the operative system is often paused while another virtual machine uses a CPU core. The VM queues the incoming data. - TCP performs _flow control_, in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node. This means additional queuing at the sender. You can choose timeouts experimentally by measuring the distribution of network round-trip times over an extended period. Systems can continually measure response times and their variability (_jitter_), and automatically adjust timeouts according to the observed response time distribution. #### Synchronous vs ashynchronous networks A telephone network estabilishes a _circuit_, we say is _synchronous_ even as the data passes through several routers as it does not suffer from queing. The maximum end-to-end latency of the network is fixed (_bounded delay_). A circuit is a fixed amount of reserved bandwidth which nobody else can use while the circuit is established, whereas packets of a TCP connection opportunistically use whatever network bandwidth is available. **Using circuits for bursty data transfers wastes network capacity and makes transfer unnecessary slow. By contrast, TCP dinamycally adapts the rate of data transfer to the available network capacity.** We have to assume that network congestion, queueing, and unbounded delays will happen. Consequently, there's no \"correct\" value for timeouts, they need to be determined experimentally. ### Unreliable clocks The time when a message is received is always later than the time when it is sent, we don't know how much later due to network delays. This makes difficult to determine the order of which things happened when multiple machines are involved. Each machine on the network has its own clock, slightly faster or slower than the other machines. It is possible to synchronise clocks with Network Time Protocol (NTP). * **Time-of-day clocks**. Return the current date and time according to some calendar (_wall-clock time_). If the local clock is toof ar ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time. **This makes it is unsuitable for measuring elapsed time.** * **Monotonic clocks**. Peg: `System.nanoTime()`. They are guaranteed to always move forward. The difference between clock reads can tell you how much time elapsed beween two checks. **The _absolute_ value of the clock is meaningless.** NTP allows the clock rate to be speeded up or slowed down by up to 0.05%, but **NTP cannot cause the monotonic clock to jump forward or backward**. **In a distributed system, using a monotonic clock for measuring elapsed time (peg: timeouts), is usually fine**. If some piece of sofware is relying on an accurately synchronised clock, the result is more likely to be silent and subtle data loss than a dramatic crash. You need to carefully monitor the clock offsets between all the machines. #### Timestamps for ordering events **It is tempting, but dangerous to rely on clocks for ordering of events across multiple nodes.** This usually imply that _last write wins_ (LWW), often used in both multi-leader replication and leaderless databases like Cassandra and Riak, and data-loss may happen. The definition of \"recent\" also depends on local time-of-day clock, which may well be incorrect. _Logical clocks_, based on counters instead of oscillating quartz crystal, are safer alternative for ordering events. Logical clocks do not measure time of the day or elapsed time, only relative ordering of events. This contrasts with time-of-the-day and monotic clocks (also known as _physical clocks_). #### Clock readings have a confidence interval It doesn't make sense to think of a clock reading as a point in time, it is more like a range of times, within a confidence internval: for example, 95% confident that the time now is between 10.3 and 10.5. The most common implementation of snapshot isolation requires a monotonically increasing transaction ID. Spanner implements snapshot isolation across datacenters by using clock's confidence interval. If you have two confidence internvals where ``` A = [A earliest, A latest] B = [B earliest, B latest] ``` And those two intervals do not overlap (`A earliest` v` Read from register _x_, database returns value _v_. * `write(x,v) => r` _r_ could be _ok_ or _error_. If one client read returns the new value, all subsequent reads must also return the new value. * `cas(x_old, v_old, v_new) => r` an atomic _compare-and-set_ operation. If the value of the register _x_ equals _v_old_, it is atomically set to _v_new_. If `x != v_old` the registers is unchanged and it returns an error. **Serializability**: Transactions behave the same as if they had executed _some_ serial order. **Linearizability**: Recency guarantee on reads and writes of a register (individual object). #### Locking and leader election To ensure that there is indeed only one leader, a lock is used. It must be linearizable: all nodes must agree which nodes owns the lock; otherwise is useless. Apache ZooKeepr and etcd are often used for distributed locks and leader election. #### Constraints and uniqueness guarantees Unique constraints, like a username or an email address require a situation similiar to a lock. A hard uniqueness constraint in relational databases requires linearizability. #### Implementing linearizable systems The simplest approach would be to have a single copy of the data, but this would not be able to tolerate faults. * Single-leader repolication is potentially linearizable. * Consensus algorithms is linearizable. * Multi-leader replication is not linearizable. * Leaderless replication is probably not linearizable. Multi-leader replication is often a good choice for multi-datacenter replication. On a network interruption betwen data-centers will force a choice between linearizability and availability. With multi-leader configuraiton, each data center can operate normally with interruptions. With single-leader replication, the leader must be in one of the datacenters. If the application requires linearizable reads and writes, the network interruption causes the application to become unavailable. * If your applicaiton _requires_ linearizability, and some replicas are disconnected from the other replicas due to a network problem, the some replicas cannot process request while they are disconnected (unavailable). * If your application _does not require_, then it can be written in a way tha each replica can process requests independently, even if it is disconnected from other replicas (peg: multi-leader), becoming _available_. **If an application does not require linearizability it can be more tolerant of network problems.** #### The unhelpful CAP theorem CAP is sometimes presented as _Consistency, Availability, Partition tolerance: pick 2 out of 3_. Or being said in another way _either Consistency or Available when Partitioned_. CAP only considers one consistency model (linearizability) and one kind of fault (_network partitions_, or nodes that are alive but disconnected from each other). It doesn't say anything about network delays, dead nodes, or other trade-offs. CAP has been historically influential, but nowadays has little practical value for designing systems. --- The main reason for dropping linearizability is _performance_, not fault tolerance. Linearizabilit is slow and this is true all the time, not on only during a network fault. ### Ordering guarantees Cause comes before the effect. Causal order in the system is what happened before what (_causally consistent_). _Total order_ allows any two elements to be compared. Peg, natural numbers are totally ordered. Some cases one set is greater than another one. Different consistency models: * Linearizablity. _total order_ of operations: if the system behaves as if there is only a single copy of the data. * Causality. Two events are ordered if they are causally related. Causality defines _a partial order_, not a total one (incomparable if they are concurrent). Linearizability is not the only way of preserving causality. **Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.** You need to know which operation _happened before_. In order to determine the causal ordering, the database needs to know which version of the data was read by the application. **The version number from the prior operation is passed back to the database on a write.** We can create sequence numbers in a total order that is _consistent with causality_. With a single-leader replication, the leader can simply increment a counter for each operation, and thus assign a monotonically increasing sequence number to each operation in the replication log. If there is not a single leader (multi-leader or leaderless database): * Each node can generate its own independent set of sequence numbers. One node can generate only odd numbers and the other only even numbers. * Attach a timestamp from a time-of-day clock. * Preallocate blocks of sequence numbers. The only problem is that the sequence numbers they generate are _not consistent with causality_. They do not correctly capture ordering of operations across different nodes. There is simple method for generating sequence numbers that _is_ consistent with causality: _Lamport timestamps_. Each node has a unique identifier, and each node keeps a counter of the number of operations it has processed. The lamport timestamp is then simply a pair of (_counter_, _node ID_). It provides total order, as if you have two timestamps one with a greater counter value is the greater timestamp. If the counter values are the same, the one with greater node ID is the greater timestamp. Every node and every client keeps track of the _maximum_ counter value it has seen so far, and includes that maximum on every request. When a node receives a request of response with a maximum counter value greater than its own counter value, it inmediately increases its own counter to that maximum. As long as the maximum counter value is carried along with every operation, this scheme ensure that the ordering from the lamport timestamp is consistent with causality. Total order of oepration only emerges after you have collected all of the operations. Total order broadcast: * Reliable delivery: If a message is delivered to one node, it is delivered to all nodes. * Totally ordered delivery: Mesages are delivered to every node in the same order. ZooKeeper and etcd implement total order broadcast. If every message represents a write to the database, and every replica processes the same writes in the same order, then the replcias will remain consistent with each other (_state machine replication_). A node is not allowed to retroactgively insert a message into an earlier position in the order if subsequent messages have already been dlivered. Another way of looking at total order broadcast is that it is a way of creating a _log_. Delivering a message is like appending to the log. If you have total order broadcast, you can build linearizable storage on top of it. Because log entries are delivered to all nodes in the same order, if therer are several concurrent writes, all nodes will agree on which one came first. Choosing the first of the conflicting writes as the winner and aborting later ones ensures that all nodes agree on whether a write was commited or aborted. This procedure ensures linearizable writes, it doesn't guarantee linearizable reads. To make reads linearizable: * You can sequence reads through the log by appending a message, reading the log, and performing the actual read when the message is delivered back to you (etcd works something like this). * Fetch the position of the latest log message in a linearizable way, you can query that position to be delivered to you, and then perform the read (idea behind ZooKeeper's `sync()`). * You can make your read from a replica that is synchronously updated on writes. For every message you want to send through total order broadcast, you increment-and-get the linearizable integer and then attach the value you got from the register as a sequence number to the message. YOu can send the message to all nodes, and the recipients will deliver the message consecutively by sequence number. ### Distributed transactions and consensus Basically _getting several nodes to agree on something_. There are situations in which it is important for nodes to agree: * Leader election: All nodes need to agree on which node is the leader. * Atomic commit: Get all nodes to agree on the outcome of the transacction, either they all abort or roll back. #### Atomic commit and two-phase commit (2PC) A transaction either succesfully _commit_, or _abort_. Atomicity prevents half-finished results. On a single node, transaction commitment depends on the _order_ in which data is writen to disk: first the data, then the commit record. 2PC uses a coordinartor (_transaction manager_). When the application is ready to commit, the coordinator begins phase 1: it sends a _prepare_ request to each of the nodes, asking them whether are able to commit. * If all participants reply \"yes\", the coordinator sends out a _commit_ request in phase 2, and the commit takes place. * If any of the participants replies \"no\", the coordinator sends an _abort_ request to all nodes in phase 2. When a participant votes \"yes\", it promises that it will definitely be able to commit later; and once the coordiantor decides, that decision is irrevocable. Those promises ensure the atomicity of 2PC. If one of the participants or the network fails during 2PC (prepare requests fail or time out), the coordinator aborts the transaction. If any of the commit or abort request fail, the coordinator retries them indefinitely. If the coordinator fails before sending the prepare requests, a participant can safely abort the transaction. The only way 2PC can complete is by waiting for the coordinator to revover in case of failure. This is why the coordinator must write its commit or abort decision to a transaction log on disk before sending commit or abort requests to participants. #### Three-phase commit 2PC is also called a _blocking_ atomic commit protocol, as 2Pc can become stuck waiting for the coordinator to recover. There is an alternative called _three-phase commit_ (3PC) that requires a _perfect failure detector_. --- Distributed transactions carry a heavy performance penalty due the disk forcing in 2PC required for crash recovery and additional network round-trips. XA (X/Open XA for eXtended Architecture) is a standard for implementing two-phase commit across heterogeneous technologies. Supported by many traditional relational databases (PostgreSQL, MySQL, DB2, SQL Server, and Oracle) and message brokers (ActiveMQ, HornetQ, MSQMQ, and IBM MQ). The problem with _locking_ is that database transactions usually take a row-level exclusive lock on any rows they modify, to prevent dirty writes. While those locks are held, no other transaction can modify those rows. When a coordinator fails, _orphaned_ in-doubt transactions do ocurr, and the only way out is for an administrator to manually decide whether to commit or roll back the transaction. #### Fault-tolerant consensus One or more nodes may _propose_ values, and the consensus algorithm _decides_ on those values. Consensus algorithm must satisfy the following properties: * Uniform agreement: No two nodes decide differently. * Integrity: No node decides twice. * Validity: If a node decides the value _v_, then _v_ was proposed by some node. * Termination: Every node that does not crash eventually decides some value. If you don't care about fault tolerance, then satisfying the first three properties is easy: you can just hardcode one node to be the \"dictator\" and let that node make all of the decisions. The termination property formalises the idea of fault tolerance. Even if some nodes fail, the other nodes must still reach a decision. Termination is a liveness property, whereas the other three are safety properties. **The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft and Zab.** Total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes. So total order broadcast is equivalent to repeated rounds of consensus: * Due to agreement property, all nodes decide to deliver the same messages in the same order. * Due to integrity, messages are not duplicated. * Due to validity, messages are not corrupted. * Due to termination, messages are not lost. ##### Single-leader replication and consensus All of the consensus protocols dicussed so far internally use a leader, but they don't guarantee that the lader is unique. Protocols define an _epoch number_ (_ballot number_ in Paxos, _view number_ in Viewstamped Replication, and _term number_ in Raft). Within each epoch, the leader is unique. Every time the current leader is thought to be dead, a vote is started among the nodes to elect a new leader. This election is given an incremented epoch number, and thus epoch numbers are totallly ordered and monotonically increasing. If there is a conflic, the leader with the higher epoch number prevails. A node cannot trust its own judgement. It must collect votes from a _quorum_ of nodes. For every decision that a leader wants to make, it must send the proposed value to the other nodes and wait for a quorum of nodes to respond in favor of the proposal. There are two rounds of voting, once to choose a leader, and second time to vote on a leader's proposal. The quorums for those two votes must overlap. The biggest difference with 2PC, is that 2PC requires a \"yes\" vote for _every_ participant. The benefits of consensus come at a cost. The process by which nodes vote on proposals before they are decided is kind of synchronous replication. Consensus always require a strict majority to operate. Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can't just add or remove nodes in the cluster. _Dynamic membership_ extensions are much less well understood than static membership algorithms. Consensus systems rely on timeouts to detect failed nodes. In geographically distributed systems, it often happens that a node falsely believes the leader to have failed due to a network issue. This implies frequest leader elecctions resulting in terrible performance, spending more time choosing a leader than doing any useful work. #### Membership and coordination services ZooKeeper or etcd are often described as \"distributed key-value stores\" or \"coordination and configuration services\". They are designed to hold small amounts of data that can fit entirely in memory, you wouldn't want to store all of your application's data here. Data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm. ZooKeeper is modeled after Google's Chubby lock service and it provides some useful features: * Linearizable atomic operations: Usuing an atomic compare-and-set operation, you can implement a lock. * Total ordering of operations: When some resource is protected by a lock or lease, you need a _fencing token_ to prevent clients from conflicting with each other in the case of a process pause. The fencing token is some number that monotonically increases every time the lock is acquired. * Failure detection: Clients maintain a long-lived session on ZooKeeper servers. When a ZooKeeper node fails, the session remains active. When ZooKeeper declares the session to be dead all locks held are automatically released. * Change notifications: Not only can one client read locks and values, it can also watch them for changes. ZooKeeper is super useful for distributed coordination. ZooKeeper/Chubby model works well when you have several instances of a process or service, and one of them needs to be chosen as a leader or primary. If the leader fails, one of the other nodes should take over. This is useful for single-leader databases and for job schedulers and similar stateful systems. ZooKeeper runs on a fixed number of nodes, and performs its majority votes among those nodes while supporting a potentially large number of clients. The kind of data managed by ZooKeeper is quite slow-changing like \"the node running on 10.1.1.23 is the leader for partition 7\". It is not intended for storing the runtime state of the application. If application state needs to be replicated there are other tools (like Apache BookKeeper). ZooKeeper, etcd, and Consul are also often used for _service discovery_, find out which IP address you need to connect to in order to reach a particular service. In cloud environments, it is common for virtual machines to continually come an go, you often don't know the IP addresses of your services ahead of time. Your services when they start up they register their network endpoints ina service registry, where they can then be found by other services. ZooKeeper and friends can be seen as part of a long history of research into _membership services_, determining which nodes are currently active and live members of a cluster. ## Batch processing * Service (online): waits for a request, sends a response back * Batch processing system (offline): takes a large amount of input data, runs a _job_ to process it, and produces some output. * Stream processing systems (near-real-time): a stream processor consumes input and produces outputs. A stream job operates on events shortly after they happen. ### Batch processing with Unix tools We can build a simple log analysis job to get the five most popular pages on your site ``` cat /var/log/nginx/access.log | awk '{print $7}' | sort | uniq -c | sort -r -n | head -n 5 | ``` You could write the same thing with a simpel program. The difference is that with Unix commands automatically handle larger-than-memory datasets and automatically paralelizes sorting across multiple CPU cores. Programs must have the same data format to pass information to one another. In Unix, that interface is a file (file descriptor), an ordered sequence of bytes. By convention Unix programs treat this sequence of bytes as ASCII text. The unix approach works best if a program simply uses `stdin` and `stdout`. This allows a shell user to wire up the input and output in whatever way they want; the program doesn't know or care where the input is coming from and where the output is going to. Part of what makes Unix tools so successful is that they make it quite easy to see what is going on. ### Map reduce and distributed filesystems A single MapReduce job is comparable to a single Unix process. Running a MapReduce job normally does not modify the input and does not have any side effects other than producing the output. While Unix tools use `stdin` and `stdout` as input and output, MapReduce jobs read and write files on a distributed filesystem. In Hadoop, that filesystem is called HDFS (Haddoop Distributed File System). HDFS is based on the _shared-nothing_ principe. Implemented by centralised storage appliance, often using custom hardware and special network infrastructure. HDFS consists of a daemon process running on each machine, exposing a network service that allows other nodes to access files stored on that machine. A central server called the _NameNode_ keeps track of which file blocks are stored on which machine. File blocks are replciated on multiple machines. Reaplication may mean simply several copies of the same data on multiple machines, or an _erasure coding_ scheme such as Reed-Solomon codes, which allow lost data to be recovered. MapReduce is a programming framework with which you can write code to process large datasets in a distributed filesystem like HDFS. 1. Read a set of input files, and break it up into _records_. 2. Call the mapper function to extract a key and value from each input record. 3. Sort all of the key-value pairs by key. 4. Call the reducer function to iterate over the sorted key-value pairs. * Mapper: Called once for every input record, and its job is to extract the key and value from the input record. * Reducer: Takes the key-value pairs produced by the mappers, collects all the values belonging to the same key, and calls the reducer with an interator over that collection of vaues. MapReduce can parallelise a computation across many machines, without you having ot write code to explicitly handle the parallelism. THe mapper and reducer only operate on one record at a time; they don't need to know where their input is coming from or their output is going to. In Hadoop MapReduce, the mapper and reducer are each a Java class that implements a particular interface. The MapReduce scheduler tries to run each mapper on one of the machines that stores a replica of the input file, _putting the computation near the data_. The reduce side of the computation is also partitioned. While the number of map tasks is determined by the number of input file blocks, the number of reduce tasks is configured by the job author. To ensure that all key-value pairs with the same key end up in the same reducer, the framework uses a hash of the key. The dataset is likely too large to be sorted with a conventional sorting algorithm on a single machine. Sorting is performed in stages. Whenever a mapper finishes reading its input file and writing its sorted output files, the MapReduce scheduler notifies the reducers that they can start fetching the output files from that mapper. The reducers connect to each of the mappers and download the files of sorted key-value pairs for their partition. Partitioning by reducer, sorting and copying data partitions from mappers to reducers is called _shuffle_. The reduce task takes the files from the mappers and merges them together, preserving the sort order. MapReduce jobs can be chained together into _workflows_, the output of one job becomes the input to the next job. In Hadoop this chaining is done implicitly by directory name: the first job writes its output to a designated directory in HDFS, the second job reads that same directory name as its input. Compared with the Unix example, it could be seen as in each sequence of commands each command output is written to a temporary file, and the next command reads from the temporary file. It is common in datasets for one record to have an association with another record: a _foreign key_ in a relational model, a _document reference_ in a document model, or an _edge_ in graph model. If the query involves joins, it may require multiple index lookpus. MapReduce has no concept of indexes. When a MapReduce job is given a set of files as input, it reads the entire content of all of those files, like a _full table scan_. In analytics it is common to want to calculate aggregates over a large number of records. Scanning the entire input might be quite reasonable. In order to achieve good throughput in a batch process, the computation must be local to one machine. Requests over the network are too slow and nondeterministic. Queries to other database for example would be prohibitive. A better approach is to take a copy of the data (peg: the database) and put it in the same distributed filesystem. MapReduce programming model has separated the physical network communication aspects of the computation (getting the data to the right machine) from the application logic (processing the data once you have it). In an example of a social network, small number of celebrities may have many millions of followers. Such disproportionately active database records are known as _linchpin objects_ or _hot keys_. A single reducer can lead to significant _skew_ that is, one reducer that must process significantly more records than the others. The _skewed join_ method in Pig first runs a sampling job to determine which keys are hot and then records related to the hot key need to be replicated to _all_ reducers handling that key. Handling the hot key over several reducers is called _shared join_ method. In Crunch is similar but requires the hot keys to be specified explicitly. Hive's skewed join optimisation requries hot keys to be specified explicitly and it uses map-side join. If you _can_ make certain assumptions about your input data, it is possible to make joins faster. A MapReducer job with no reducers and no sorting, each mapper simply reads one input file and writes one output file. The output of a batch process is often not a report, but some other kind of structure. Google's original use of MapReduce was to build indexes for its search engine. Hadoop MapReduce remains a good way of building indexes for Lucene/Solr. If you need to perform a full-text search, a batch process is very effective way of building indexes: the mappers partition the set of documents as needed, each reducer builds the index for its partition, and the index files are written to the distributed filesystem. It pararellises very well. Machine learning systems such as clasifiers and recommendation systems are a common use for batch processing. #### Key-value stores as batch process output The output of those batch jobs is often some kind of database. So, how does the output from the batch process get back into a database? Writing from the batch job directly to the database server is a bad idea: * Making a network request for every single record is magnitude slower than the normal throughput of a batch task. * Mappers or reducers concurrently write to the same output database an it can be easily overwhelmed. * You have to worry about the results from partially completed jobs being visible to other systems. A much better solution is to build a brand-new database _inside_ the batch job an write it as files to the job's output directory, so it can be loaded in bulk into servers that handle read-only queries. Various key-value stores support building database files in MapReduce including Voldemort, Terrapin, ElephanDB and HBase bulk loading. --- By treating inputs as immutable and avoiding side effects (such as writing to external databases), batch jobs not only achieve good performance but also become much easier to maintain. Design principles that worked well for Unix also seem to be working well for Hadoop. The MapReduce paper was not at all new. The sections we've seen had been already implemented in so-called _massively parallel processing_ (MPP) databases. The biggest difference is that MPP databases focus on parallel execution of analytic SQL queries on a cluster of machines, while the combination of MapReduce and a distributed filesystem provides something much more like a general-purpose operating system that can run arbitraty programs. Hadoop opened up the possibility of indiscriminately dumpint data into HDFS. MPP databases typically require careful upfront modeling of the data and query patterns before importing data into the database's proprietary storage format. In MapReduce instead of forcing the producer of a dataset to bring it into a standarised format, the interpretation of the data becomes the consumer's problem. If you have HDFS and MapReduce, you _can_ build a SQL query execution engine on top of it, and indeed this is what the Hive project did. If a node crashes while a query is executing, most MPP databases abort the entire query. MPP databases also prefer to keep as much data as possible in memory. MapReduce can tolerate the failure of a map or reduce task without it affecting the job. It is also very eager to write data to disk, partly for fault tolerance, and partly because the dataset might not fit in memory anyway. MapReduce is more appropriate for larger jobs. At Google, a MapReduce task that runs for an hour has an approximately 5% risk of being terminated to make space for higher-priority process. Ths is why MapReduce is designed to tolerate frequent unexpected task termination. ### Beyond MapReduce In response to the difficulty of using MapReduce directly, various higher-level programming models emerged on top of it: Pig, Hive, Cascading, Crunch. MapReduce has poor performance for some kinds of processing. It's very robust, you can use it to process almost arbitrarily large quantities of data on an unreliable multi-tenant system with frequent task terminations, and it will still get the job done. The files on the distributed filesystem are simply _intermediate state_: a means of passing data from one job to the next. The process of writing out the intermediate state to files is called _materialisation_. MapReduce's approach of fully materialising state has some downsides compared to Unix pipes: * A MapReduce job can only start when all tasks in the preceding jobs have completed, whereas rocesses connected by a Unix pipe are started at the same time. * Mappers are often redundant: they just read back the same file that was just written by a reducer. * Files are replicated across several nodes, which is often overkill for such temporary data. To fix these problems with MapReduce, new execution engines for distributed batch computations were developed, Spark, Tez and Flink. These new ones can handle an entire workflow as one job, rather than breaking it up into independent subjobs (_dataflow engines_). These functions need not to take the strict roles of alternating map and reduce, they are assembled in flexible ways, in functions called _operators_. Spark, Flink, and Tex avoid writing intermediate state to HDFS, so they take a different approach to tolerating faults: if a machine fails and the intermediate state on that machine is lost, it is recomputed from other data that is still available. The framework must keep track of how a given piece of data was computed. Spark uses the resilient distributed dataset (RDD) to track ancestry data, while Flink checkpoints operator state, allowing it to resume running an operator that ran into a fault during its execution. #### Graphs and iterative processing It's interesting to look at graphs in batch processing context, where the goal is to perform some kind of offline processing or analysis on an entire graph. This need often arises in machine learning applications such as recommednation engines, or in ranking systems. \"repeating until done\" cannot be expressed in plain MapReduce as it runs in a single pass over the data and some extra trickery is necessary. An optimisation for batch processing graphs, the _bulk synchronous parallel_ (BSP) has become popular. It is implemented by Apache Giraph, Spark's GraphX API, and Flink's Gelly API (_Pregel model, as Google Pregel paper popularised it). One vertex can \"send a message\" to another vertex, and typically those messages are sent along the edges in a graph. The difference from MapReduce is that a vertex remembers its state in memory from one iteration to the next. The fact that vertices can only communicate by message passing helps improve the performance of Pregel jobs, since messages can be batched. Fault tolerance is achieved by periodically checkpointing the state of all vertices at the end of an interation. The framework may partition the graph in arbitrary ways. Graph algorithms often have a lot of cross-machine communication overhead, and the intermediate state is often bigger than the original graph. If your graph can fit into memory on a single computer, it's quite likely that a single-machine algorithm will outperform a distributed batch process. If the graph is too big to fit on a single machine, a distributed approach such as Pregel is unavoidable. ## Stream processing We can run the processing continuously, abandoning the fixed time slices entirely and simply processing every event as it happens, that's the idea behind _stream processing_. Data that is incrementally made available over time. ### Transmitting event streams A record is more commonly known as an _event_. Something that happened at some point in time, it usually contains a timestamp indicating when it happened acording to a time-of-day clock. An event is generated once by a _producer_ (_publisher_ or _sender_), and then potentially processed by multiple _consumers_ (_subcribers_ or _recipients_). Related events are usually grouped together into a _topic_ or a _stream_. A file or a database is sufficient to connect producers and consumers: a producer writes every event that it generates to the datastore, and each consumer periodically polls the datastore to check for events that have appeared since it last ran. However, when moving toward continual processing, polling becomes expensive. It is better for consumers to be notified when new events appear. Databases offer _triggers_ but they are limited, so specialised tools have been developed for the purpose of delivering event notifications. #### Messaging systems ##### Direct messaging from producers to consumers Within the _publish_/_subscribe_ model, we can differentiate the systems by asking two questions: 1. _What happens if the producers send messages faster than the consumers can process them?_ The system can drop messages, buffer the messages in a queue, or apply _backpressure_ (_flow control_, blocking the producer from sending more messages). 2. _What happens if nodes crash or temporarily go offline, are any messages lost?_ Durability may require some combination of writing to disk and/or replication. A number of messaging systems use direct communication between producers and consumers without intermediary nodes: * UDP multicast, where low latency is important, application-level protocols can recover lost packets. * Brokerless messaging libraries such as ZeroMQ * StatsD and Brubeck use unreliable UDP messaging for collecting metrics * If the consumer expose a service on the network, producers can make a direct HTTP or RPC request to push messages to the consumer. This is the idea behind webhooks, a callback URL of one service is registered with another service, and makes a request to that URL whenever an event occurs These direct messaging systems require the application code to be aware of the possibility of message loss. The faults they can tolerate are quite limited as they assume that producers and consumers are constantly online. If a consumer if offline, it may miss messages. Some protocols allow the producer to retry failed message deliveries, but it may break down if the producer crashes losing the buffer or messages. ##### Message brokers An alternative is to send messages via a _message broker_ (or _message queue_), which is a kind of database that is optimised for handling message streams. It runs as a server, with producers and consumers connecting to it as clients. Producers write messages to the broker, and consumers receive them by reading them from the broker. By centralising the data, these systems can easily tolerate clients that come and go, and the question of durability is moved to the broker instead. Some brokers only keep messages in memory, while others write them down to disk so that they are not lost inc ase of a broker crash. A consequence of queueing is that consuemrs are generally _asynchronous_: the producer only waits for the broker to confirm that it has buffered the message and does not wait for the message to be processed by consumers. Some brokers can even participate in two-phase commit protocols using XA and JTA. This makes them similar to databases, aside some practical differences: * Most message brokers automatically delete a message when it has been successfully delivered to its consumers. This makes them not suitable for long-term storage. * Most message brokers assume that their working set is fairly small. If the broker needs to buffer a lot of messages, each individual message takes longer to process, and the overall throughput may degrade. * Message brokers often support some way of subscribing to a subset of topics matching some pattern. * Message brokers do not support arbitrary queries, but they do notify clients when data changes. This is the traditional view of message brokers, encapsulated in standards like JMS and AMQP, and implemented in RabbitMQ, ActiveMQ, HornetQ, Qpid, TIBCO Enterprise Message Service, IBM MQ, Azure Service Bus, and Google Cloud Pub/Sub. When multiple consumers read messages in the same topic, to main patterns are used: * Load balancing: Each message is delivered to _one_ of the consumers. The broker may assign messages to consumers arbitrarily. * Fan-out: Each message is delivered to _all_ of the consumers. In order to ensure that the message is not lost, message brokers use _acknowledgements_: a client must explicitly tell the broker when it has finished processing a message so that the broker can remove it from the queue. The combination of laod balancing with redelivery inevitably leads to messages being reordered. To avoid this issue, youc an use a separate queue per consumer (not use the load balancing feature). ##### Partitioned logs A key feature of barch process is that you can run them repeatedly without the risk of damaging the input. This is not the case with AMQP/JMS-style messaging: receiving a message is destructive if the acknowledgement causes it to be deleted from the broker. If you add a new consumer to a messaging system, any prior messages are already gone and cannot be recovered. We can have a hybrid, combining the durable storage approach of databases with the low-latency notifications facilities of messaging, this is the idea behind _log-based message brokers_. A log is simply an append-only sequence of records on disk. The same structure can be used to implement a message broker: a producer sends a message by appending it to the end of the log, and consumer receives messages by reading the log sequentially. If a consumer reaches the end of the log, it waits for a notification that a new message has been appended. To scale to higher throughput than a single disk can offer, the log can be _partitioned_. Different partitions can then be hosted on different machines. A topic can then be defined as a group of partitions that all carry messages of the same type. Within each partition, the broker assigns monotonically increasing sequence number, or _offset_, to every message. Apache Kafka, Amazon Kinesis Streams, and Twitter's DistributedLog, are log-based message brokers that work like this. The log-based approach trivially supports fan-out messaging, as several consumers can independently read the log reading without affecint each other. Reading a message does not delete it from the log. To eachieve load balancing the broker can assign entire partitions to nodes in the consumer group. Each client then consumes _all_ the messages in the partition it has been assigned. This approach has some downsides. * The number of nodes sharing the work of consuming a topic can be at most the number of log partitions in that topic. * If a single message is slow to process, it holds up the processing of subsequent messages in that partition. In situations where messages may be expensive to process and you want to pararellise processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. In situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well. It is easy to tell which messages have been processed: al messages with an offset less than a consumer current offset have already been processed, and all messages with a greater offset have not yet been seen. The offset is very similar to the _log sequence number_ that is commonly found in single-leader database replication. The message broker behaves like a leader database, and the consumer like a follower. If a consumer node fails, another node in the consumer group starts consuming messages at the last recorded offset. If the consumer had processed subsequent messages but not yet recorded their offset, those messages will be processed a second time upon restart. If you only ever append the log, you will eventually run out of disk space. From time to time old segments are deleted or moved to archive. If a slow consumer cannot keep with the rate of messages, and it falls so far behind that its consumer offset poitns to a deleted segment, it will miss some of the messages. The throughput of a log remains more or less constant, since every message is written to disk anyway. This is in contrast to messaging systems that keep messages in memory by default and only write them to disk if the queue grows too large: systems are fast when queues are short and become much slower when they start writing to disk, throughput depends on the amount of history retained. If a consumer cannot keep up with producers, the consumer can drop messages, buffer them or applying backpressure. You can monitor how far a consumer is behind the head of the log, and raise an alert if it falls behind significantly. If a consumer does fall too far behind and start missing messages, only that consumer is affected. With AMQP and JMS-style message brokers, processing and acknowledging messages is a destructive operation, since it causes the messages to be deleted on the broker. In a log-based message broker, consuming messages is more like reading from a file. The offset is under the consumer's control, so you can easily be manipulated if necessary, like for replaying old messages. ### Databases and streams A replciation log is a stream of a database write events, produced by the leader as it processes transactions. Followers apply that stream of writes to their own copy of the database and thus end up with an accurate copy of the same data. If periodic full database dumps are too slow, an alternative that is sometimes used is _dual writes_. For example, writing to the database, then updating the search index, then invalidating the cache. Dual writes have some serious problems, one of which is race conditions. If you have concurrent writes, one value will simply silently overwrite another value. One of the writes may fail while the other succeeds and two systems will become inconsistent. The problem with most databases replication logs is that they are considered an internal implementation detail, not a public API. Recently there has been a growing interest in _change data capture_ (CDC), which is the process of observing all data changes written to a database and extracting them in a form in which they can be replicated to other systems. For example, you can capture the changes in a database and continually apply the same changes to a search index. We can call log consumers _derived data systems_: the data stored in the search index and the data warehouse is just another view. Change data capture is a mechanism for ensuring that all changes made to the system of record are also reflected in the derived data systems. Change data capture makes one database the leader, and turns the others into followers. Database triggers can be used to implement change data capture, but they tend to be fragile and have significant performance overheads. Parsing the replication log can be a more robust approach. LinkedIn's Databus, Facebook's Wormhole, and Yahoo!'s Sherpa use this idea at large scale. Bottled Watter implements CDC for PostgreSQL decoding the write-ahead log, Maxwell and Debezium for something similar for MySQL by parsing the binlog, Mongoriver reads the MongoDB oplog, and GoldenGate provide similar facilities for Oracle. Keeping all changes forever would require too much disk space, and replaying it would take too long, so the log needs to be truncated. You can start with a consistent snapshot of the database, and it must correspond to a known position or offset in the change log. The storage engine periodically looks for log records with the same key, throws away any duplicates, and keeps only the most recent update for each key. An update with a special null value (a _tombstone_) indicates that a key was deleted. The same idea works in the context of log-based mesage brokers and change data capture. RethinkDB allows queries to subscribe to notifications, Firebase and CouchDB provide data synchronisation based on change feed. Kafka Connect integrates change data capture tools for a wide range of database systems with Kafka. #### Event sourcing There are some parallels between the ideas we've discussed here and _event sourcing_. Similarly to change data capture, event sourcing involves storing all changes to the application state as a log of change events. Event sourcing applyies the idea at a different level of abstraction. Event sourcing makes it easier to evolve applications over time, helps with debugging by making it easier to understand after the fact why something happened, and guards against application bugs. Specialised databases such as Event Store have been developed to support applications using event sourcing. Applications that use event sourcing need to take the log of evetns and transform it into application state that is suitable for showing to a user. Replying the event log allows you to reconstruct the current state of the system. Applications that use event sourcing typically have some mechanism for storing snapshots. Event sourcing philosophy is careful to distinguis between _events_ and _commands_. When a request from a user first arrives, it is initially a command: it may still fail (like some integrity condition is violated). If the validation is successful, it becomes an event, which is durable and immutable. A consumer of the event stream is not allowed to reject an event: Any validation of a command needs to happen synchronously, before it becomes an event. For example, by using a serializable transaction that atomically validates the command and publishes the event. Alternatively, the user request to serve a seat could be split into two events: first a tentative reservation, and then a separate confirmation event once the reservation has been validated. This split allows the validation to take place in an asynchronous process. Whenever you have state changes, that state is the result of the events that mutated it over time. Mutable state and an append-only log of immutable events do not contradict each other. As an example, financial bookkeeping is recorded as an append-only _ledger_. It is a log of events describing money, good, or services that have changed hands. Profit and loss or the balance sheet are derived from the ledger by adding them up. If a mistake is made, accountants don't erase or change the incorrect transaction, instead, they add another transaction that compensates for the mistake. If buggy code writes bad data to a database, recovery is much harder if the code is able to destructively overwrite data. Immutable events also capture more information than just the current state. If you persisted a cart into a regular database, deleting an item would effectively loose that event. You can derive views from the same event log, Druid ingests directly from Kafka, Pistachio is a distributed key-value sotre that uses Kafka as a commit log, Kafka Connect sinks can export data from Kafka to various different databases and indexes. Storing data is normally quite straightforward if you don't have to worry about how it is going to be queried and accessed. You gain a lot of flexibility by separating the form in which data is written from the form it is read, this idea is known as _command query responsibility segregation_ (CQRS). There is this fallacy that data must be written in the same form as it will be queried. The biggest downside of event sourcing and change data capture is that consumers of the event log are usually asynchronous, a user may make a write to the log, then read from a log derived view and find that their write has not yet been reflected. The limitations on immutable event history depends on the amount of churn in the dataset. Some workloads mostly add data and rarely update or delete; they are wasy to make immutable. Other workloads have a high rate of updates and deletes on a comparaively small dataset; in these cases immutable history becomes an issue because of fragmentation, performance compaction and garbage collection. There may also be circumstances in which you need data to be deleted for administrative reasons. Sometimes you may want to rewrite history, Datomic calls this feature _excision_. ### Processing Streams What you can do with the stream once you have it: 1. You can take the data in the events and write it to the database, cache, search index, or similar storage system, from where it can thenbe queried by other clients. 2. You can push the events to users in some way, for example by sending email alerts or push notifications, or to a real-time dashboard. 3. You can process one or more input streams to produce one or more output streams. Processing streams to produce other, derived streams is what an _operator job_ does. The one crucial difference to batch jobs is that a stream never ends. _Complex event processing_ (CEP) is an approach for analising event streams where you can specify rules to search for certain patterns of events in them. When a match is found, the engine emits a _complex event_. Queries are stored long-term, and events from the input streams continuously flow past them in search of a query that matches an event pattern. Implementations of CEP include Esper, IBM InfoSphere Streams, Apama, TIBCO StreamBase, and SQLstream. The boundary between CEP and stream analytics is blurry, analytics tends to be less interested in finding specific event sequences and is more oriented toward aggregations and statistical metrics. Frameworks with analytics in mind are: Apache Storm, Spark Streaming, Flink, Concord, Samza, and Kafka Streams. Hosted services include Google Cloud Dataflow and Azure Stream Analytics. Sometimes there is a need to search for individual events continually, such as full-text search queries over streams. Message-passing ystems are also based on messages and events, we normally don't think of them as stream processors. There is some crossover area between RPC-like systems and stream processing. Apache Storm has a feature called _distributed RPC_. In a batch process, the time at which the process is run has nothing to do with the time at which the events actually occurred. Many stream processing frameworks use the local system clock on the processing machine (_processing time_) to determine windowing. It is a simple approach that breaks down if there is any significant processing lag. Confusing event time and processing time leads to bad data. Processing time may be unreliable as the stream processor may queue events, restart, etc. It's better to take into account the original event time to count rates. You can never be sure when you have received all the events. You can time out and declare a window ready after you have not seen any new events for a while, but it could still happen that some events are delayed due a network interruption. You need to be able to handle such _stranggler_ events that arrive after the window has already been declared complete. 1. You can ignore the stranggler events, tracking the number of dropped events as a metric. 2. Publish a _correction_, an updated value for the window with stranglers included. You may also need to retrat the previous output. To adjust for incofrrect device clocks, one approach is to log three timestamps: * The time at which the event occurred, according to the device clock * The time at which the event was sent to the server, according to the device clock * The time at which the event was received by the server, according to the server clock. You can estimate the offset between the device clock and the server clock, then apply that offset to the event timestamp, and thus estimate the true time at which the event actually ocurred. Several types of windows are in common use: * Tumbling window: Fixed length. If you have a 1-minute tumbling window, all events between 10:03:00 and 10:03:59 will be grouped in one window, next window would be 10:04:00-10:04:59 * Hopping window: Fixed length, but allows windows to overlap in order to provide some smoothing. If you have a 5-minute window with a hop size of 1 minute, it would contain the events between 10:03:00 and 10:07:59, next window would cover 10:04:00-10:08:59 * Sliding window: Events that occur within some interval of each other. For example, a 5-minute sliding window would cover 10:03:39 and 10:08:12 because they are less than 4 minutes apart. * Session window: No fixed duration. All events for the same user, the window ends when the user has been inactive for some time (30 minutes). Common in website analytics The fact that new events can appear anytime on a stream makes joins on stream challenging. #### Stream-stream joins You want to detect recent trends in searched-for URLs. You log an event containing the query. Someone clicks one of the search results, you log another event recording the click. You need to bring together the events for the search action and the click action. For this type of join, a stream processor needs to maintain _state_: All events that occurred in the last hour, indexed by session ID. Whenever a search event or click event occurs, it is added to the appropriate index, and the stream processor also checks the other index to see if another event for the same session ID has already arrived. If there is a matching event, you emit an event saying search result was clicked. #### Stream-table joins Sometimes know as _enriching_ the activity events with information from the database. Imagine two datasets: a set of usr activity events, and a database of user profiles. Activity events include the user ID, and the the resulting stream should have the augmented profile information based upon the user ID. The stream process needs to look at one activity event at a time, look up the event's user ID in the database, and add the profile information to the activity event. THe database lookup could be implemented by querying a remote database., however this would be slow and risk overloading the database. Another approach is to load a copy of the database into the stream processor so that it can be queried locally without a network round-trip. The stream processor's local copy of the database needs to be kept up to date; this can be solved with change data capture. #### Table-table join The stream process needs to maintain a database containing the set of followers for each user so it knows which timelines need to be updated when a new tweet arrives. #### Time-dependence join The previous three types of join require the stream processor to maintain some state. If state changes over time, and you join with some state, what point in time do you use for the join? If the ordering of events across streams is undetermined, the join becomes nondeterministic. This issue is known as _slowly changing dimension_ (SCD), often addressed by using a unique identifier for a particular version of the joined record. For example, we can turn the system deterministic if every time the tax rate changes, it is given a new identifier, and the invoice includes the identifier for the tax rate at the time of sale. But as a consequence makes log compation impossible. #### Fault tolerance Batch processing frameworks can tolerate faults fairly easy:if a task in a MapReduce job fails, it can simply be started again on another machine, input files are immutable and the output is written to a separate file. Even though restarting tasks means records can be processed multiple times, the visible effect in the output is as if they had only been processed once (_exactly-once-semantics_ or _effectively-once_). With stream processing waiting until a tasks if finished before making its ouput visible is not an option, stream is infinite. One solution is to break the stream into small blocks, and treat each block like a minuature batch process (_micro-batching_). This technique is used in Spark Streaming, and the batch size is typically around one second. An alternative approach, used in Apache Flint, is to periodically generate rolling checkpoints of state and write them to durable storage. If a stream operator crashes, it can restart from its most recent checkpoint. Microbatching and chekpointing approaches provide the same exactly-once semantics as batch processing. However, as soon as output leaves the stream processor, the framework is no longer able to discard the output of a failed batch. In order to give appearance of exactly-once processing, things either need to happen atomically or none of must happen. Things should not go out of sync of each other. Distributed transactions and two-phase commit can be used. This approach is used in Google Cloud Dataflow and VoltDB, and there are plans to add similar features to Apache Kafka. Our goal is to discard the partial output of failed tasks so that they can be safely retired without taking effect twice. Distributed transactions are one way of achieving that goal, but another way is to rely on _idempotence_. An idempotent operation is one that you can perform multiple times, and it has the same effect as if you performed it only once. Even if an operation is not naturally idempotent, it can often be made idempotent with a bit of extra metadata. You can tell wether an update has already been applied. Idempotent operations can be an effective way of achieving exactly-once semantics with only a small overhead. Any stream process that requires state must ensure tha this state can be recovered after a failure. One option is to keep the state in a remote datastore and replicate it, but it is slow. An alternative is to keep state local to the stream processor and replicate it periodically. Flink periodically captures snapshots and writes them to durable storage such as HDFS; Samza and Kafka Streams replicate state changes by sending them to a dedicated Kafka topic with log compaction. VoltDB replicates state by redundantly processing each input message on several nodes. ## The future of data systems ### Data integration Updating a derived data system based on an event log can often be made determinisitic and idempotent. Distributed transactions decide on an ordering of writes by using locks for mutual exclusion, while CDC and event sourcing use a log for ordering. Distributed transactions use atomic commit to ensure exactly once semantics, while log-based systems are based on deterministic retry and idempotence. Transaction systems provide linearizability, useful guarantees as reading your own writes. On the other hand, derived systems are often updated asynchronously, so they do not by default offer the same timing guarantees. In the absence of widespread support for a good distributed transaction protocol, log-based derived data is the most promising approach for integrating different data systems. However, as systems are scaled towards bigger and more coplex worloads, limitiations emerge: * Constructing a totally ordered log requires all events to pass through a _single leader node_ that decides on the ordering. * An undefined ordering of events that originate on multiple datacenters. * When two events originate in different services, there is no defined order for those events. * Some applications maintain client-side state. Clients and servers are very likely to see events in different orders. Deciding on a total order of events is known as _total order broadcast_, which is equivalent to consensus. It is still an open research problem to design consensus algorithms that can scale beyond the throughput of a single node. #### Batch and stream processing The fundamental difference between batch processors and batch processes is that the stream processors operate on unbounded datasets whereas batch processes inputs are of a known finite size. Spark performs stream processing on top of batch processing. Apache Flink performs batch processing in top of stream processing. Batch processing has a quite strong functional flavour. The output depends only on the input, there are no side-effects. Stream processing is similar but it allows managed, fault-tolerant state. Derived data systems could be maintained synchronously. However, asynchrony is what makes systems based on event logs robust: it allows a fault in one part of the system to be contained locally. Stream processing allows changes in the input to be reflected in derived views with low delay, whereas batch processing allows large amounts of accumulated historical data to be reprocessed in order to derive new views onto an existing dataset. Derived views allow _gradual_ evolution. If you want to restructure a dataset, you do not need to perform the migration as a sudden switch. Instead, you can maintain the old schema and the new schema side by side as two independent derived views onto the same underlying data, eventually you can drop the old view. #### Lambda architecture The whole idea behind lambda architecture is that incoming data should be recorded by appending immutable events to an always-growing dataset, similarly to event sourcing. From these events, read-optimised vuews are derived. Lambda architecture proposes running two different systems in parallel: a batch processing system such as Hadoop MapReduce, and a stream-processing system as Storm. The stream processor produces an approximate update to the view: the batch processor produces a corrected version of the derived view. The stream process can use fast approximation algorithms while the batch process uses slower exact algorithms. ### Unbundling databases #### Creating an index Batch and stream processors are like elaborate implementations of triggers, stored procedures, and materialised view maintenance routines. The derived data systems they maintain are like different index types. There are two avenues by which different storate and processing tools can nevertheless be composed into a cohesive system: * Federated databases: unifying reads. It is possible to provide a unified query interface to a wide variety of underlying storate engines and processing methods, this is known as _federated database_ or _polystore_. An example is PostgreSQL's _foreign data wrapper_. * Unbundled databases: unifying writes. When we compose several storage systems, we need to ensure that all data changes end up in all the right places, even in the face of faults, it is like _unbundling_ a database's index-maintenance features in a way that can synchronise writes across disparate technologies. Keeping the writes to several storage systems in sync is the harder engineering problem. Synchronising writes requires distributed transactions across heterogeneous storage systems which may be the wrong solution. An asynchronous event log with idempotent writes is a much more robust and practical approach. The big advantage is _loose coupling_ between various components: 1. Asynchronous event streams make the system as a whole more robust to outages or performance degradation of individual components. 2. Unbundling data systems allows different software components and services to be developed, improved and maintained independently from each other by different teams. If there is a single technology that does everything you need, you're most likely best off simply using that product rather than trying to reimplement it yourself from lower-level components. The advantages of unbundling and composition only come into the picture when there is no single piece of software that satisfies all your requirements. #### Separation of application code and state It makes sense to have some parts of a system that specialise in durable data storage, and other parts that specialise in running application code. The two can interact while still remaining independent. The trend has been to keep stateless application logic separate from state management (databases): not putting application logic in the database and not putting persistent state in the application. #### Dataflow, interplay between state changes and application code Instead of treating the database as a passive variable that is manipulated by the application, application code responds to state changes in one place by triggering state changes in another place. #### Stream processors and services A customer is purchasing an item that is priced in one currency but paid in another currency. In order to perform the currency conversion, you need to know the current exchange rate. This could be implemented in two ways: * Microservices approach, the code that processes the purchase would probably wuery an exchange-rate service or a database in order to obtain the current rate for a particular currency. * Dataflow approach, the code that processes purchases would subscribe to a stream of exchange rate updates ahead of time, and record the current rate in a local database whenever it changes. When it comes to processing the purchase, it only needs to query the local database. The dataflow is not only faster, but it is also more robust to the failure of another service. #### Observing derived state ##### Materialised views and caching A full-text search index is a good example: the write path updates the index, and the read path searches the index for keywords. If you don't have an index, a search query would have to scan over all documents, which is very expensive. No index means less work on the write path (no index to update), but a lot more work on the read path. Another option would be to precompute the search results for only a fixed set of the most common queries. The uncommon queries can still be served from the inxed. This is what we call a _cache_ although it could also be called a materialised view. ##### Read are events too It is also possible to represent read requests as streams of events, and send both the read events and write events through a stream processor; the processor responds to read events by emiting the result of the read to an output stream. It would allow you to reconstruct what the user saw before they made a particular decision. Enables better tracking of casual dependencies. ### Aiming for correctness If your application can tolerate occasionally corrupting or losing data in unpredictable ways, life is a lot simpler. If you need stronger assurances of correctness, the serializability and atomic commit are established approaches. While traditional transaction approach is not going away, there are some ways of thinking about correctness in the context of dataflow architectures. #### The end-to-end argument for databases Bugs occur, and people make mistakes. Favour of immutable and append-only data, because it is easier to recover from such mistakes. We've seen the idea of _exactly-once_ (or _effectively-once_) semantics. If something goes wrong while processing a message, you can either give up or try again. If you try again, there is the risk that it actually succeeded the first time, the message ends up being processed twice. _Exactly-once_ means arranging the computation such that the final effect is the same as if no faults had occurred. One of the most effective approaches is to make the operation _idempotent_, to ensure that it has the same effect, no matter whether it is executed once or multiple times. Idempotence requires some effort and care: you may need to maintain some additional metadata (operation IDs), and ensure fencing when failing over from one node to another. Two-phase commit unfortunately is not sufficient to ensure that the transaction will only be executed once. You need to consider _end-to-end_ flow of the request. You can generate a unique identifier for an operation (such as a UUID) and include it as a hidden form field in the client application, or calculate a hash of all the relevant form fields to derive the operation ID. If the web browser submits the POST request twice, the two requests will have the same operation ID. You can then pass that operation ID all the way through to the database and check that you only ever execute one operation with a given ID. You can then save those requests to be processed, uniquely identified by the operation ID. Is not enough to prevent a user from submitting a duplicate request if the first one times out. Solving the problem requires an end-to-end solution: a transaction indentifier that is passed all the way from the end-user client to the database. Low-level reliability mechanisms such as those in TCP, work quite well, and so the remaining higher-level faults occur fairly rarely. Transactions have long been seen as a good abstraction, they are useful but not enough. It is worth exploring F=fault-tolerance abstractions that make it easy to provide application-specific end-to-end correctness properties, but also maintain good performance and good operational characteristics. #### Enforcing constraints ##### Uniqueness constraints require consensus The most common way of achieving consensus is to make a single node the leadder, and put it in charge of making all decisions. If you need to tolerate the leader failing, you're back at the consensus problem again. Uniqueness checking can be scaled out by partitioning based on the value that needs to be unique. For example, if you need usernames to be unique, you can partition by hash or username. Asynchronous multi-master replication is ruled out as different masters concurrently may accept conflicting writes, so values are no longer unique. If you want to be able to immediately reject any writes that would violate the constraint, synchronous coordination is unavoidable. ##### Uniqueness in log-based messaging A stream processor consumes all the messages in a log partition sequentially on a single thread. A stream processor can unambiguously and deterministically decide which one of several conflicting operations came first. 1. Every request for a username is encoded as a message. 2. A stream processor sequentially reads the requests in the log. For every request for a username tht is available, it records the name as taken and emits a success message to an output stream. For every request for a username that is already taken, it emits a rejection message to an output stream. 3. The client waits for a success or rejection message corresponding to its request. The approach works not only for uniqueness constraints, but also for many other kinds of constraints. ##### Multi-partition request processing There are potentially three partitions: the one containing the request ID, the one containing the payee account, and one containing the payer account. The traditional approach to databases, executing this transaction would require an atomic commit across all three partitions. Equivalent correctness can be achieved with partitioned logs, and without an atomic commit. 1. The request to transfer money from account A to account B is given a unique request ID by the client, and appended to a log partition based on the request ID. 2. A stream processor reads the log of requests. For each request message it emits two messages to output streams: a debit instruction to the payer account A (partitioned by A), and a credit instruction to the payee account B (partitioned by B). The original request ID is included in those emitted messages. 3. Further processors consume the streams of credit and debit instructions, deduplicate by request ID, and apply the chagnes to the account balances. #### Timeliness and integrity Consumers of a log are asynchronous by design, so a sender does not wait until its message has been proccessed by consumers. However, it is possible for a client to wait for a message to appear on an output stream. _Consistency_ conflates two different requirements: * Timeliness: users observe the system in an up-to-date state. * Integrity: Means absence of corruption. No data loss, no contradictory or false data. The derivation must be correct. Violations of timeless are \"eventual consistency\" whereas violations of integrity are \"perpetual inconsistency\". #### Correctness and dataflow systems When processing event streams asynchronously, there is no guarantee of timeliness, unless you explicitly build consumers that wait for a message to arrive before returning. But integrity is in fact central to streaming systems. _Exactly-once_ or _effectively-once_ semantics is a mechanism for preserving integrity. Fault-tolerant message delivery and duplicate supression are important for maintaining the integrity of a data system in the face of faults. Stream processing systems can preserve integrity without requireing distributed transactions and an atomic commit protocol, which means they can potentially achieve comparable correctness with much better performance and operational robustness. Integrity can be achieved through a combination of mechanisms: * Representing the content of the write operation as a single message, this fits well with event-sourcing * Deriving all other state updates from that single message using deterministic derivation functions * Passing a client-generated request ID, enabling end-to-end duplicate supression and idempotence * Making messages immutable and allowing derived data to be reprocessed from time to time In many businesses contexts, it is actually acceptable to temporarily violate a constraint and fix it up later apologising. The cost of the apology (money or reputation), it is often quite low. #### Coordination-avoiding data-systems 1. Dataflow systems can maintain integrity guarantees on derived data without atomic commit, linearizability, or synchronous cross-partition coordination. 2. Although strict uniqueness constraints require timeliness and coordination, many applications are actually fine with loose constraints than may be temporarily violated and fixed up later. Dataflow systems can provide the data management services for many applications without requiring coordination, while still giving strong integrity guarantees. _Coordination-avoiding_ data systems can achieve better performance and fault tolerance than systems that need to perform synchronous coordination. #### Trust, but verify Checking the integrity of data is know as _auditing_. If you want to be sure that your data is still there, you have to actually read it and check. It is important to try restoring from your backups from time to time. Don't just blindly trust that it is working. _Self-validating_ or _self-auditing_ systems continually check their own integrity. ACID databases has led us toward developing applications on the basis of blindly trusting technology, neglecting any sort of auditability in the process. By contrast, event-based systems can provide better auditability (like with event sourcing). Cryptographic auditing and integrity checking often relies on _Merkle trees_. Outside of the hype for cryptocurrencies, _certificate transparency_ is a security technology that relies on Merkle trees to check the validity of TLS/SSL certificates. ### Doing the right thing Many datasets are about people: their behaviour, their interests, their identity. We must treat such data with humanity and respect. Users are humans too, and human dignitity is paramount. There are guidelines to navigate these issues such as ACM's Software Engineering Code of Ethics and Professional Practice It is not sufficient for software engineers to focus exclusively on the technology and ignore its consequences: the ethical responsibility is ours to bear also. In countries that respect human rights, the criminal justice system presumes innocence until proven guilty; on the other hand, automated systems can systematically and artbitrarily exclude a person from participating in society without any proof of guilt, and with little chance of appeal. If there is a systematic bias in the input to an algorithm, the system will most likely learn and amplify bias in its output. It seems ridiculous to believe that an algorithm could somehow take biased data as input and produce fair and impartial output from it. Yet this believe often seems to be implied by proponents of data-driven decision making. If we want the future to be better than the past, moral imagination is required, and that's something only humans can provide. Data and models should be our tools, not our masters. If a human makes a mistake, they can be held accountable. Algorithms make mistakes too, but who is accountable if they go wrong? A credit score summarises \"How did you behave in the past?\" whereas predictive analytics usually work on the basis of \"Who is similar to you, and how did people like you behave in the past?\" Drawing parallels to others' behaviour implies stereotyping people. We will also need to figure outhow to prevent data being used to harm people, and realise its positive potential instead, this power could be used to focus aid an support to help people who most need it. When services become good at predicting what content users want to se, they may end up showing people only opinions they already agree with, leading to echo chambers in which stereotypes, misinformation and polaristaion can breed. Many consequences can be predicted by thinking about the entire system (not just the computerised parts), an approach known as _systems thinking_. #### Privacy and tracking When a system only stores data that a user has explicitly entered, because they want the system to store and process it in a certain way, the system is performing a service for the user: the user is the customer. But when a user's activity is tracked and logged as a side effect of other things they are doing, the relationship is less clear. The service no longer just does what the users tells it to do, but it takes on interests of its own, which may conflict with the user's interest. If the service is funded through advertising, the advertirsers are the actual customers, and the users' interests take second place. The user is given a free service and is coaxed into engaging with it as much as possible. The tracking of the user serves the needs of the advertirses who are funding the service. This is basically _surveillance_. As a thougt experiment, try replacing the word _data_ with _surveillance_. Even themost totalitarian and repressive regimes could only dream of putting a microphone in every room and forcing every person to constantly carry a device capable of tracking their location and movements. Yet we apparently voluntarily, even enthusiastically, throw ourselves into this world of total surveillance. The difference is just that the data is being collected by corporations rather than government agencies. Perhaps you feel you have nothing to hide, you are totally in line with existing power structures, you are not a marginalised minority, and you needn't fear persecution. Not everyone is so fortunate. Without understanding what happens to their data, users cannot give any meaningful consent. Often, data from one user also says things about other people who are not users of the service and who have not agreed to any terms. For a user who does not consent to surveillance, the only real alternative is simply to not user the service. But this choice is not free either: if a service is so popular that it is \"regarded by most people as essential for basic social participation\", then it is not reasonable to expect people to opt out of this service. Especially when a service has network effects, there is a social cost to people choosing _not_ to use it. Declining to use a service due to its tracking of users is only an option for the small number of people who are priviledged enough to have the time and knowledge to understand its privacy policy, and who can affort to potentially miss out on social participation or professional opportunities that may have arisen if they ahd participated in the service. For people in a less priviledged position, there is no meaningful freedom of choice: surveillance becomes inescapable. Having privacy does not mean keeping everything secret; it means having the freedom to choose which things to reveal to whom, what to make public, and what to keep secret. Companies that acquire data essentially say \"trust us to do the right thing with your data\" which means that the right to decide what to reveal and what to keep secret is transferred from the individual to the company. Even if the service promises not to sell the data to third parties, it usually grants itself unrestricted rights to process and analyse the data internally, often going much further than what is overtly visible to users. If targeted advertising is what pays for a service, then behavioral data about people is the service's core asset. When collecting data, we need to consider not just today's political environment, but also future governments too. There is no guarantee that every government elected in the future will respect human rights and civil liberties, so \"it is poor civic hygiene to install technologies that could someday facilitate a police state\". To scrutinise others while avoiding scrutiny oneself is one of the most important forms of power. In the industrial revolution tt took a long time before safeguards were established, such as environmental protection regulations, safety protocols for workplaces, outlawing child labor, and health inspections for food. Undoubtedly the cost of doing business increased when factories could no longer dump their waste into rivers, sell tainted foods, or exploit workers. But society as a whole benefited hugely, and few of us would want to return to a time before those regulations. We should stop regarding users as metrics to be optimised, and remember that they are humans who deserve respect, dignity, and agency. We should self-regulate our data collection and processing practices in order to establish an maintain the trust of the people who depend on our software. And we should take it upon ourselves to educate end users about how their data is used, rather than keeping them in the dark. We should allow each individual to maintain their privacy, their control over their own data, and not steal that control from them through surveillance. We should not retain data forever, but purge it as soon as it is no longer needed. ",
    "url": "/learning-notes/books/designing-data-intensive-applications/",
    "relUrl": "/books/designing-data-intensive-applications/"
  },"10": {
    "doc": "Distributed Systems Observability",
    "title": "Distributed Systems Observability",
    "content": "# [Distributed Systems Observability](https://www.goodreads.com/book/show/40182805-distributed-systems-observability) - [The need of observability](#the-need-of-observability) - [Monitoring and observability](#monitoring-and-observability) - [Coding and testing for observability](#coding-and-testing-for-observability) - [Testing for failure](#testing-for-failure) - [The three pillars of observability](#the-three-pillars-of-observability) - [Event logs](#event-logs) - [Metrics](#metrics) - [Tracing](#tracing) Most failures not addressed by application layers will arise from the complex interactions between various applications. Observability isn't purely an operational concern. ## The need of observability **Observability is not just about logs, metrics and traces it's about bringing better visibility into systems.** Observability is a property of the system. It does acknowledge the following - No complex system is ever fully healthy. - Distributed systems are unpredictable. - It's impossible to predict the state of different parts of the system. - Failure needs to be embraced. - Easy debugging is fundamental. Observability is a feature that needs to be enshrined into a system. - Lends itself well to being tested in a realistic manner (a degree of testing in production). - Failure modes can be surfaced during the time of testing. - Deployments can happen incrementally so the rollback can be triggered if a key of metrics deviate from the baseline. - Report enough data points, so the system can be understood. ## Monitoring and observability **Observability is a superset of monitoring.** It provides highly granular insights into the implicit failure modes of the system. An observable system provides contexts about its inner workings. **Monitoring reports the overall health of systems and derives alerts** > ### Blackbox and whitebox monitoring > _Blackbox monitoring_ refers to observing a system from the outside. Useful for identify the _symptoms_ of a problem like \"error rate is up\". > _Whitebox monitoring_ refers to techniques of reporting data form inside a system. Monitoring data should always provide a bird's-eye view of the overall health of a distributed system by recording and exposing high-level metrics over time across all components of the system. **All alerts need to be _actionable_.** A good set of metrics are the USE metrics and the RED metrics. * USE methodology is for analysing system performance like utilisation, saturation, errors of resources, free memory, CPU or device errors. * RED methodology is about application level metrics like request rate, error rate, and duration of the requests. Debugging is often an iterative process and involves * Start with a high-level metric * Drill down observations * Make the right deductions * Testing the theory Observability doesn't obviate the need for careful thought. The process of knowing what information to examine (observations), requires a good understanding of the system and domain, as well as a good sense of intuition. ## Coding and testing for observability The idea of experimenting with live traffic is either seen as the preserve of operations engineers or is something that's met with alarm. Some amount of regression testing to post-production monitoring requires coding and testing for failure. Acknowledging that systems will fail, being able to debug such failures is of paramount importance, and embedding debuggability into the system from the ground up. Understanding service dependencies and abstractions better, allows you to improve reliability massively just by changing a single line of configuration. ### Testing for failure Unit tests only ever test the behaviour of a system against specified set of inputs. End-to-end testing might allow for some degree of holistic testing of the system, but complex systems fail in complex ways. The tooling aimed to understand the behaviour of our services in production does not obviate the need for pre-production testing. **Certain types of failures can only be surfaced in the production environment.** | Pre-production | Deploy | Release | Post-release |------------------------|---------------------|--------------------|------------------------| - Unit tests | - Integration tests | - Canarying | - Teeing | - Functional tests | - Tap compare | - Monitoring | - Profiling | - Component tests | - Load tests | - Traffic shaping | - Logs/events | - Stress tests | - Shadowing | - Feature flagging | - Chaos testing | - Static analysis | - Soak tests | - Monitoring | - Property based tests | | - A/B tests | - Coverage tests | | - Tracing | - Benchmark tests | | - Dynamic exploration | - Regression tests | | - Real user monitoring | - Contract tests | | - Auditing | - Lint tests | | - On-call experience | - Acceptance tests | | | - Mutation tests | | | - Smoke tests | | | - UI/UX tests | | | - Usability tests | | | - Penetration tests | | | - Threat modelling | | Testing in production (deploy, release and post-release phases) is impossible to do without measuring how the system under test is performing in production. ## The three pillars of observability ### Event logs An _event log_ is an immutable, timestamped record of discrete events that happened over time. Usually a pair of a timestamp and a payload of some context. There are three forms: plain text, structured (peg: JSON) and binary (peg: Protobuf format in MySQL binlogs). Event logs are especially helpful for uncovering emergent and unpredictable behaviours exhibited by components of a distributed system. Traces and metrics are an abstraction built on top of logs. **Pros:** - Easiest to generate - Easy to instrument **Cons:** - Suboptimal performance due to the overhead of logging. - Messages can be lost unless one uses a protocol like RELP to guarantee delivery of messages. Logging excessively has the capability to adversely affect application performance as a whole. A solution to this is to do _Sampling_, picking a small subset of the total population of event logs. Raw logs are almost always normalised, filtered, and processed by a tool like Logstash, fluentd, Scribe or Heka before they're persisted in a data store like Elasticsearch or BigQuery. Logs might require further buffering in a broker like Kafka before they can be processed by Logstash. Logs can also be the source for all analytics data, which has a tremendous utility from a business intelligence perspective. Complex queries can be made thanks to storing events. Events are structured key-value paris. Most analytics pipelines use Kafka as an event bus. Sending enriched event data to Kafka allows one to search in real time over streams with KSQL, an SQL engine for Kafka. This data can be expired from the Kafka log regularly. There are alternatives like Humio, Honeycomb and Facebook's Scuba. ### Metrics _Metrics_ are a numeric representation of data measured over intervals of time. Since they are numbers, metrics enable longer retention of data as well as easier querying. They are perfectly suited to building dashboards that reflect historical trends stored in time-series databases. Modern monitoring systems like Prometheus and newer versions of Graphite represent every time series using a metric name as well as additional key-value pairs called _labels_. **Pros:** - Metrics transfer and storage has a constant overhead. The cost doesn't increase in lockstep with user traffic. Storage increases with more permutations of label values tho. - They are more malleable to mathematical, probabilistic, and statistical transformations such as sampling, aggregation, summarisation, and correlation. They are better suited to report the overall health of a system. - Metrics are also better suited to trigger alerts. **Cons:** - They are _system_ scoped, making it hard to understand anything else other than what's happening inside a particular system. - A single line doesn't give you much information about what happened to a request across all components of a system. Distributed tracing is a technique that addresses the problem of brining visibility into the lifetime of a request across several systems. ### Tracing A _trace_ is a representation of a series of casually related distributed events that encode end-to-end request flow through a distributed system. Traces identify specific points in an application, proxy, framework, library, runtime, middleware and anything else in the path of the request that represents: - Forks in execution path - A hop or a fan out across network or process boundaries. A trace is a directed acyclic graph of _spans_, where the edges between spans are called _references_. When a request begins, it's assigned a globally unique ID, which is then propagated through the request path, so that each point of instrumentation is able to insert or enrich metadata before passing the ID around to the next hop. Each hop along the flow is represented by a span. Each record emitted on each service are usually asynchronously logged to disk before being submitted out of band to a collector, which then can reconstruct the flow of execution. Understanding request lifecycles makes it possible to debug requests spanning multiple services to pinpoint the source of increased latency or resource utilisation. Zipkin and Jaeger are the two most popular OpenTracing-compliant open source distributed tracing solutions. **Cons:** - Traces are the hardest to retrofit into an existing infrastructure. Every component in the path of a request needs to be modified to propagate tracing information. - It's not sufficient for developers to instrument just their own code alone. - Challenging at places with polyglot architectures. > #### Service mesh, a new hope > Service meshes make integrating tracing functionality almost effortless as they implement tracing and stats collections at the proxy level. Applications will still need to forward headers to the next hop in the mesh, but no additional instrumentation is necessary. ",
    "url": "/learning-notes/books/distributed-systems-observability/",
    "relUrl": "/books/distributed-systems-observability/"
  },"11": {
    "doc": "Effective Java",
    "title": "Effective Java",
    "content": "# [Effective Java](https://www.goodreads.com/book/show/105099.Effective_Java_Programming_Language_Guide) - [Creating and destroying objects](#creating-and-destroying-objects) - [Methods common to all objects](#methods-common-to-all-objects) - [Classes and interfaces](#classes-and-interfaces) - [Generics](#generics) - [Enums and annotations](#enums-and-annotations) - [Lambdas and streams](#lambdas-and-streams) - [Methods](#methods) - [General programming](#general-programming) - [Exceptions](#exceptions) - [Concurrency](#concurrency) - [Serialisation](#serialisation) ## Creating and destroying objects ### Consider static factory methods instead of constructors ```java public static Boolean valueOf(boolean b) { return b ? Boolean.TRUE : Boolean.FALSE; } ``` Advantages: * They have names. * They are not required to create a new object each time they are invoked. * They can return an object of any subtype of their return type. * The class of the returned object can vary from call to call as a function of the input parameters. * The class of the returned object need to exist when the class containing the method is written. Disadvantages: * Classes that provide only static factory methods without public or protected constructors cannot be subclassed. This is preferable when using composition instead of inheritance. * They are hard for programmers to find. Some conventions - `from`, as type-conversion method `Date d = Date.from(instant)` - `of`, an aggregation method `Set faceCards = EnumSet.of(JACK, QUEEN, KING)` - `valueOf`, more verbose alternative than `from` and `of`, `BigInteger prime = BigInteger.valueOf(Integer.MAX_VALUE)` - `instance` or `getInstance`, returns and instance that cannot be said to have the same values, `StackWalker luke = StackWalker.getInstance(options)` - `create` or `newInstance` the method guarantees each call returns a new instance, `Object newArray = Array.newInstance(classObject, arrayLength)` - `get`**Type**, like `getInstance`but when factory method is in a different class `FileStore fs = Files.getFileStore(path)` - `new`**Type**, like `newInstance` but when factory method is in a different class `BufferedReader br = Files.newBufferedReader(path)` - **type**, a concise alternative to `get`_Type_ and `new`_Type_, `List litany = Collections.list(legacyLitany)` ### Consider builder when faced with many constructor parameters ```java public class NutritionFacts { private final int servingSize; // (mL) required private final int servings; // (per container) required private final int calories; // (per serving) optional private final int fat; // (g/serving) optional private final int sodium; // (mg/serving) optional private final int carbohydrate; // (g/serving) optional public NutritionFacts(int servingSize, int servings) { this(servingSize, servings, 0); } public NutritionFacts(int servingSize, int servings, int calories) { this(servingSize, servings, calories, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat) { this(servingSize, servings, calories, fat, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium) { this(servingSize, servings, calories, fat, sodium, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium, int carbohydrate) { this.servingSize = servingSize; this.servings = servings; this.calories = calories; this.fat = fat; this.sodium = sodium; this.carbohydrate = carbohydrate; } } ``` The telescoping constructor pattern works, but it is hard to write client code when there are many parameters, and harder still to read it. Avoid using JavaBeans pattern as it might leave the object in an inconsistent state partway through its construction. It does also precludes the possibility of making the class immutable. ```java public class NutritionFacts { private final int servingSize; private final int servings; private final int calories; private final int fat; private final int sodium; private final int carbohydrate; public static class Builder { private final int servingSize; private final int servings; private int calories = 0; private int fat = 0; private int sodium = 0; private int carbohydrate = 0; public Builder(int servingSize, int servings) { this.servingSize = servingSize; this.servings = servings; } public Builder calories(int val) { calories = val; return this; } public Builder fat(int val) { fat = val; return this; } public Builder sodium(int val) { sodium = val; return this; } public Builder carbohydrate(int val) { carbohydrate = val; return this; } public NutritionFacts build() { return new NutritionFacts(this); } } private NutritionFacts(Builder builder) { servingSize = builder.servingSize; servings = builder.servings; calories = builder.calories; fat = builder.fat; sodium = builder.sodium; carbohydrate = builder.carbohydrate; } } ``` ```java NutritionFacts cocaCola = new NutritionFacts.Builder(240, 8) .calories(100).sodium(35).carbohydrate(27).build(); ``` The Builder pattern simulates named optional parameters and it is well suited to class hierarchies. **It is good choice when designing classes whose constructors or static factories would have more than a handful of parameters.** ### Enforce the singleton property with a private constructor or an enum type Beware that making a class a singleton can make it difficult to test its clients because it's impossible to substitute a mock implementation / **Singleton with public final field**, this approach is simpler and it makes it clear that the class is a singleton. ```java public class Elvis { public static final Elvis INSTANCE = new Elvis(); private Elvis() { ... } public void leaveTheBuilding() { ... } } ``` **Singleton with static factory** ```java public class Elvis { private static final Elvis INSTANCE = new Elvis(); private Elvis() { ... } public static Elvis getInstance() { return INSTANCE; } public void leaveTheBuilding() { ... } } ``` **Enum singleton**, similar to public field approach but more concise. It provides serialisation machinery for free and provides guarantee against multiple instantiation even on serialisation and reflection attacks. **This is the best way of implementing a singleton.** ```java public enum Elvis { INSTANCE; public void leaveTheBuilding() { ... } } ``` ### Enforce noninstantiability with a private constructor Ocassionally you'll want to write a class that is a grouping of static methods. People abuse them in order to avoid thinking in terms of objects, but they have valud use cases. Such _utility clases_ are not designed to be instantiated. Attempting to enforce noninstantiability by making the class abstract does not work. **A class can be made noninstantiable by including a private constructor.** ```java public class UtilityClass { // Suppress default constructor for noninstantiability private UtilityClass() { throw new AssertionError(); } // Remainder omitted } ``` ### Prefer dependency injection to hardwiring resources Innapropiate use of static utility, inflexible and untestable. ```java public class SpellChecker { private static final Lexicon dictionary = ...; private SpellChecker() {} // Noninstantiable public static boolean isValid(String word) { ... } public static List suggestions(String typo) { ... } } ``` Dependency injection provides flexibility and testability ```java public class SpellChecker { private final Lexicon dictionary; public SpellChecker(Lexicon dictionary) { this.dictionary = Objects.requireNonNull(dictionary); } public boolean isValid(String word) { ... } public List suggestions(String typo) { ... } } ``` A useful variant of the pattern is to pass a resource _factory_ to the constructor so the object can be called repeatedly to create instance of a type. ```java Mosaic create(Supplier tileFactory) { ... } ``` ### Avoid creating unnecessary objects ```java String s = new String(\"bikini\"); ``` Performance can be greatly improved ```java static boolean isRomanNumeral(String s) { return s.matches(\"^(?=.)M*(C[MD]|D?C{0,3})\" + \"(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\"); } ``` Reusing expensive object for improved performance ```java public class RomanNumerals { private static final Pattern ROMAN = Pattern.compile( \"^(?=.)M*(C[MD]|D?C{0,3})\" + \"(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})$\"); static boolean isRomanNumeral(String s) { return ROMAN.matcher(s).matches(); } } ``` Be careful with autoboxing, as it blurs the distinction betwen primitive and boxed primitive types. The following code is hideously slow, can you spot object creation? ```java private static long sum() { Long sum = 0L; for (long i = 0; i = 0) out.write(buf, 0, n); } finally { out.close(); } } finally { in.close(); } } ``` `try`-with-resources is the best way to close resources. ``` static String firstLineOfFile(String path) throws IOException { try (BufferedReader br = new BufferedReader(new FileReader(path))) { return br.readLine(); } } ``` `try`-with-resources on multiple resources, short and sweet. ``` static void copy(String src, String dst) throws IOException { try (InputStream in = new FileInputStream(src); OutputStream out = new FileOutputStream(dst)) { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) >= 0) out.write(buf, 0, n); } } ``` ## Methods common to all objects ### Obey the general contract when overriding `equals` * Each instance of the class is inherently unique * there is no need for the class to provide a \"logical equality\" test * A superclass has overriden `equals`, and the superclass behaviour is appropiate for this class * The class is private or package-private, and you are certain that its `equals` method will never be invoked. `equals` method implements _equivalence relation_ for any non-null reference values: * **Reflexive**: `x.equals(x)` must return `true` * **Symmetric**: `x.equals(y)` returns `true` if `y.equals(x)` returns `true` * **Transitive**: `x.equals(y)` returns `true` and `y.equals(z)` returns `true`, then `x.equals(z)` must return `true` * **Consistent**: `x.equals(y)` should consistently return `true` or consistently return `false` * `x.equals(null)` must return `false` Do not write an `equals` method that depends on unreliable resources. All objects objects must be unequal to `null`. A few more caveats: * Always override `hashCode` when you override `equals`. * Don't try to be too clever * Don't substitute another type for `Object` in the `equals` declaration. ### Always override `hashCode` when you override equals Equal objects must have equal hash codes. Do not be tempted to exclude significant fields from the hash code computation to improve performance. Don't provide a detailed specification for the value returned by `hashCode`, so clients can't reasonably depend on it; this gives you the flexibility to change it. ### Always override `toString` Providing a good `toString` implementation makes your class much more pleasant to use and makes systems using the class easier to debug. When practical, the `toString` method should return all the interesting information in the object. Whether or not you decide to specify the format, you should clearly document your intentions. ### Override `clone` judiciously In practice, a class implementing `Cloneable` is expected to provide a properly functioning public `clone` method. Immutable classes should never provide a `clone` method, because that would only encourage wasteful copying. The `clone` method functions as a constructor; you must ensure that it does no harm to the original object and that it properly establishes invariants on the clone. The `Cloneable` architecture is incompatible with normal use of final fields referring to mutable objects. Public `clone` methods should omit the `throws` clause, as methods that don't throw checked exceptions are easier to use. A better approach to object copying is to provide a copy constructor or copy factory. ```java public Yum(Yum yum) { ... } ``` ```java public static Yum newInstance(Yum yum) { ... }; ``` Advantages of using copy constructor or copy factory: * They don't rely on a risk-prone extralinguistic object creation mechanism. * They don't demand unenforceable adherence to thinly documented conventions. * They don't conflict with the proper use of final fields. * They don't throw unnecessary checked exceptions. * They don't require casts. * They can take an argument whose type is an interface implemented by the class. * Interface-based copy constructors and factories (_conversion_ constructors and _conversion_ factories) allow the client to accept the implementation type of the original. ### Consider implementing `Comparable` The notation `sgn`(expression) designates the mathematical _signum_ function, which is defined to return -1, 0, or 1 * The implementor must ensure that `sgn(x.compareTo(y)) == -sgn(y. compareTo(x))` for all `x` and `y`. Implies that `x.compareTo(y)` must throw an exception if and only if `y.compareTo(x)` throws an exception. * Transitive: ``(x. compareTo(y) > 0 && y.compareTo(z) > 0)`` implies `x.compareTo(z) > 0`. * `x.compareTo(y) == 0` implies that `sgn(x.compareTo(z)) == sgn(y.compareTo(z))`, for all `z`. * It is strongly recommended, but not required, that `(x.compareTo(y) == 0) == (x.equals(y))`. Use of the relational operators `` in compareTo methods is verbose and error-prone and no longer recommended. If a class has multiple significant fields, the order in which you compare them is critical. Start with the most significant field and work your way down. The `Comparator` interface is outfitted with a set of comparator construction methods, which enable fluent construction of comparators. Many programmers prefer the conciseness of this approach, though it does come at a modest performance cost. ```java private static final Comparator COMPARATOR = comparingInt((PhoneNumber pn) -> pn.areaCode) .thenComparingInt(pn -> pn.prefix) .thenComparingInt(pn -> pn.lineNum); public int compareTo(PhoneNumber pn) { return COMPARATOR.compare(this, pn); } ``` **Avoid comparators based on differences, they violate transitivity** ```java static Comparator hashCodeOrder = new Comparator() { public int compare(Object o1, Object o2) { return o1.hashCode() - o2.hashCode(); } }; ``` ## Classes and interfaces ### Minimise the accessibility of classes and members A well-designed component hides all its implementation details, cleanly separating their API from its implementation. Information hiding or _encapsulation_ is important because: * _Decouples_ the component that comprise the system, allowing them to be developed, tested, optimised, used, understood and modified in isolation. * Speeds up development because components can be developed in parallel. * Eases the burden of maintenance because components can be understood more quickly and debugged or replaced with little fear or harming other components. * Components can be optimised without affecting correctness of others. * Increases software reuse because components that aren't tightly coupled often prove useful in other contexts. * Decreases the risk in building large systems because individual components may prove successful even if the system does not. Make each class or member as inaccessible as possible. Instance fields of public classes should rarely be public. Classes with public mutable fields are generally thread-safe. Nonzero-length array is always mutable, so it is wrong for a class to have a public static final array field, or accessor that returns such a field. It is a potential security hole. ```java public static final Thing[] VALUES = { ... }; ``` Alternatively, return a copy of a private array ```java private static final Thing[] PRIVATE_VALUES = { ... }; public static final List VALUES = Collections.unmodifiableList(Arrays.asList(PRIVATE_VALUES)); ``` ### In public classes, use accessor methods, not public fields Degenerate classes like this should not be public ```java class Point { public double x; public double y; } ``` Because the data fields of such classes are accessed directly, these classes do not offer the benefits of _encapsulation_. You can't change the representation without changing the API, you can't enforce invariants, and you can't take auxiliary action when a field is accessed. If a class is accessed outside its package, provide accessor methods. If a class is package-private or is a private nested class, there is nothing inherently wrong with exposing its data fields. ### Minimise mutability An immutable class is simply a class whose instances cannot be modified. **Immutable classes are easier to design, implement, and use than mutable classes. They are less prone to error and are more secure.** 1. Don't provide methods that modify the object's state (mutators). 2. Ensure that the class can't be extended. 3. Make all fields final. 4. Make all fields private. 5. Ensure exclusive access to any mutable components. Returning new objects on operations is known as _functional approach_ because methods return the result of applying a function to their operand, without modifying it. Contrast it to the _procedural_ or _imperative_ approach in which methods apply a procedure to their operand, causing its state to change. **Method names in immutable objects are prepositions (such as `plus`) rather than names (such as `add`).** Advantages of immutable objects: * Simple. * Inherently thread-safe; they require no synchronisation. * Can be shared freely. * Not only you can share immutable objects, but they can share their internals. * Make great building blocks for other objects. * Provide failure atomicity for free. There is no possibility of temporary inconsistency. The main disadvantage immutable objects have is that they require a separate object for each distinct value. Classes should be immutable unless there's a very good reason to make them mutable. If a class cannot be made immutable, limit its mutability as much as possible. Declare every field `private final` unless there's a good reason to do otherwise. Constructors should create fully initialised objects with all of their invariants established. ### Favour composition over inheritance Using inheritance inappropriately lead to fragile software. It is safe to use inheritance within a package where programmers have classes and subclasses under control. It is safe to use inheritance when extending classes specifically designed and documented for extension. Unlike method invocation, implementation inheritance violates encapsulation. Subclasses depend on the implementation details of its superclass for its proper function. To avoid fragility use composition and forwarding instead of inheritance, especially if an appropriate interface to implement a wrapper exists. Wrapper classes are not only more robust than subclasses, they are also more powerful. ### Design and document for inheritance or else prohibit it The class must document its self-use of overridable methods. A class may have to provide hooks into its internal workings in the form of judiciously chosen protected methods. **The only way to test a class designed for inheritance is to write subclasses.** You must test your class by writing subclasses before you release it. Constructors must not invoke overridable methods. Superclass constructor runs before the subclass constructor. If the overriding method depends on any initialisation performed by the subclass constructor, the method will not behave as expected. ```java public class Super { // Broken - constructor invokes an overridable method public Super() { overrideMe(); } public void overrideMe() { } } public final class Sub extends Super { // Blank final, set by constructor private final Instant instant; Sub() { instant = Instant.now(); } // Overriding method invoked by superclass constructor @Override public void overrideMe() { System.out.println(instant); } public static void main(String[] args) { Sub sub = new Sub();//null sub.overrideMe();//instant } } ``` Same restriction applies to `clone` and `readObject`, they should not invoke overridable methods, directly or indirectly. Designing a class for inheritance requires great effort and places substantial limitations on the class. The best solution to this problem is to prohibit subclassing in classes that are not designed and documented to be safely subclassed. There are two ways of doing this. Declare the class final or make all constructors private or package-private and to add public static factories instead of constructors. ### Prefer interfaces to abstract classes Existing classes can easily be retrofitted to implement a new interface. Interfaces are ideal for defining _mixings_, a type that a class can implement in addition to its \"primary type\". For example Comparable is a mixing that allows a class can be ordered. Interfaces allow for the construction of nonhierarchical type frameworks. Interfaces enable safe, powerful functionality enhancements via the wrapper class idiom. If you use abstract classes, you leave the programmer who wants to add functionality with no alternative than to use inheritance. You can combine the advantages of interfaces and abstract classes by providing an abstract _skeletal implementation class_ to go with an interface. The interface defines the type, while the skeletal implementation class implements the primitive interface methods (_Template Method_ pattern). Concrete implementation built on top of an skeletal implementation. ```java static List intArrayAsList(int[] a) { Objects.requireNonNull(a); // The diamond operator is only legal here in Java 9 and later // If you're using an earlier release, specify return new AbstractList() { @Override public Integer get(int i) { return a[i]; // Autoboxing (Item 6) } @Override public Integer set(int i, Integer val) { int oldVal = a[i]; a[i] = val; // Auto-unboxing return oldVal; // Autoboxing } @Override public int size() { return a.length; } }; } ``` Skeletal implementation classes provide implementation assistance of abstract classes without imposing constraints as type definitions. Good documentation is essential in a skeletal implementation. ### Design interfaces for posterity It is not always possible to write a default method that maintains all the invariants of every conceivable implementation. In the presence of default methods, existing implementations of an interface may compile without error or warning but fail at runtime. Is still of the utmost importance to design interfaces with great care. While it may be possible to correct some interface flaws after an interface is released, you cannot count on it. ### Use interfaces only to define types Constant interface anti-pattern (only constants) are a poor use of interfaces. Implementing a constant interface causes this implementation detail to leak into the class's exported API. ```java public interface PhysicalConstants { static final double AVOGADROS_NUMBER = 6.022_140_857e23; static final double BOLTZMANN_CONSTANT = 1.380_648_52e-23; static final double ELECTRON_MASS = 9.109_383_56e-31; } ``` If the constants are strongly tied to a class or interface, you should add them there. If not, a utility class are a good choice ```java public class PhysicalConstants { private PhysicalConstants() { } // Prevents instantiation public static final double AVOGADROS_NUMBER = 6.022_140_857e23; public static final double BOLTZMANN_CONST = 1.380_648_52e-23; public static final double ELECTRON_MASS = 9.109_383_56e-31; } ``` ### Prefer class hierarchies to tagged classes Tagged class, vastly inferior to a class hierarchy. ```java class Figure { enum Shape { RECTANGLE, CIRCLE }; // Tag field - the shape of this figure final Shape shape; // These fields are used only if shape is RECTANGLE double length; double width; // This field is used only if shape is CIRCLE double radius; // Constructor for circle Figure(double radius) { shape = Shape.CIRCLE; this.radius = radius; } // Constructor for rectangle Figure(double length, double width) { shape = Shape.RECTANGLE; this.length = length; this.width = width; } double area() { switch(shape) { case RECTANGLE: return length * width; case CIRCLE: return Math.PI * (radius * radius); default: throw new AssertionError(shape); } } } ``` Tagged classes are verbose, error-prone and inefficient. A tagged class is just a pallid imitation of a class hierarchy. With class hierarchy ```java abstract class Figure { abstract double area(); } class Circle extends Figure { final double radius; Circle(double radius) { this.radius = radius; } @Override double area() { return Math.PI * (radius * radius); } } class Rectangle extends Figure { final double length; final double width; Rectangle(double length, double width) { this.length = length; this.width = width; } @Override double area() { return length * width; } } ``` ### Favour static member classes over nonstatic Typical use of a nonstatic member class ```java public class MySet extends AbstractSet { ... // Bulk of the class omitted @Override public Iterator iterator() { return new MyIterator(); } private class MyIterator implements Iterator { ... } } ``` If you declare a member class that does not require access to an enclosing instance, always put the `static` modifier in its declaration. If you omit this modifier, each instance will have a hidden extraneous reference to its enclosing instance and it will take time and space. ### Limit source files to a single top-level class Use static member classes instead of multiple top-level classes. If you do otherwise, your program might not be able to compile or run properly. ```java public class Test { public static void main(String[] args) { System.out.println(Utensil.NAME + Dessert.NAME); } private static class Utensil { static final String NAME = \"pan\"; } private static class Dessert { static final String NAME = \"cake\"; } } ``` Never put multiple top-level classes or interfaces in a single source file. ## Generics ### Don't use raw types ```java private final Collection stamps = ... ; ``` If you use raw types, you lose all the safety and expressiveness benefits of generics. ```java private final Collection stamps = ... ; ``` You lose type safety if you use a raw type such as `List`, but not if you use a parameterised type such as `List`. This method works but it uses raw types, which is dangerous. ```java static int numElementsInCommon(Set s1, Set s2) { int result = 0; for (Object o1 : s1) if (s2.contains(o1)) result++; return result; } ``` Better to use wildcard type. ```java static int numElementsInCommon(Set s1, Set s2) { ... } ``` You can put _any_ element into a collection with a raw type, easily corrupting the collection type invariant. **You can't put any element (other than null) into a `Collection`** There are a couple of exceptions to the use of raw types * You must use raw types in class literals, as `List.class` is legal, but `List.class` is not. * Raw types is the preferred way to use the `instanceof` operator with generic types ```java if (o instanceof Set) { // Raw type Set s = (Set) o; // Wildcard type ... } ``` ### Eliminate unchecked warnings Eliminate every unchecked warning that you can. That will ensure your code is typesafe. If you can't eliminate a warning, but you can prove that the code that provoked the warning is typesafe, then (and only then) suppress the warning with an `@SupressWarnings(\"unchecked\")` annotation. Always use the `SuppressWarnings` annotation on the smallest scope possible. Every time you use a `@SuppressWarnings(\"unchecked\")` annotation, add a comment saying why it is safe to do so. ### Prefer lists to arrays Arrays are _covariant_, which means that if a `Sub` is a subtype of `Super`, then array type `Sub[]` is a subtype of the array type `Super[]`. Generics, by contrast, are _invariant_, for any two distinct types `Type1` and `Type2`, `List` is neither a subtype or a supertype of `List`. Arrays are deficient, this code fragment is legal and it fails at runtime ```java Object[] objectArray = new Long[1]; objectArray[0] = \"I don't fit in\"; // Throws ArrayStoreException ``` This one won't compile ```java List ol = new ArrayList(); // Incompatible types ol.add(\"I don't fit in\"); ``` ### Favour generic types Generic types are safer and easier to use than types that require casts in client code.When you design new types, make sure that they can be used without such casts. ### Favour generic methods Generic methods, like generic types, are safer and easier to use than methods requiring their clients to put explicit casts on input parameters and return types. ### Use bounded wildcards to increase API flexibility Wildcard type for a parameter that serves as an E producer ```java public void pushAll(Iterable src) { for (E e : src) push(e); } ``` This would make `Stack` work also on `stack.pushAll(intVal)`, as `Integer` is a subtype of `Number`. Sometimes we would want to do the opposite, have a wildcard type for parameter that serves as an `E` consumer. ```java public void popAll(Collection dst) { while (!isEmpty()) dst.add(pop()); } ``` So we can do ```java Collection objects = ...; numberStack.popAll(objects); ``` For maximum flexibility, use wildcard types on input parameters that represent producers or consumers. _PECS_ stands for producer-`extends` and consumer-`super`. Do not use bounded wildcard types as return types. It would force wildcard types on client code. **If the user of a class has to think about wildcard types, there is probably something wrong with its API.** If a type parameter appears only once in a method declaration, replace it with a wildcard. ### Combine generics and varargs judiciously Mixing generics and varargs can violate type safety. ```java static void dangerous(List... stringLists) { List intList = List.of(42); Object[] objects = stringLists; objects[0] = intList; // Heap pollution String s = stringLists[0].get(0); // ClassCastException } ``` It is unsafe to store a value in a generic varargs array parameter. The `SafeVarargs` annotation constitutes a promise by the author of a method that it is typesafe. This is unsafe, it exposes a reference to its generic parameter array. The compiler might not have enough information to make an accurate determination of the type of the argument and it can propagate heap pollution up the stack. ```java static T[] toArray(T... args) { return args; } ``` It is unsafe to give another method access to a generic varargs parameter array. Safe method with a generic varargs parameter ```java @SafeVarargs static List flatten(List... lists) { List result = new ArrayList(); for (List list : lists) result.addAll(list); return result; } ``` Use `@SafeVarargs` on every method with a varargs parameter of a generic or parameterised type. A generic varargs method is safe if: 1. Does not store anything in the varargs parameter array 2. Does not make the array (or a clone) visible to untrusted code. Use `List` as a typesafe alternative to a generic varargs parameter ```java static List flatten(List> lists) { List result = new ArrayList(); for (List list : lists) result.addAll(list); return result; } ``` ### Consider typesafe heterogeneous containers Typesafe heterogeneous container pattern API ```java public class Favorites { public void putFavorite(Class type, T instance); public T getFavorite(Class type); } ``` Typesafe heterogeneous container pattern client ```java public static void main(String[] args) { Favorites f = new Favorites(); f.putFavorite(String.class, \"Java\"); f.putFavorite(Integer.class, 0xcafebabe); f.putFavorite(Class.class, Favorites.class); String favoriteString = f.getFavorite(String.class); int favoriteInteger = f.getFavorite(Integer.class); Class favoriteClass = f.getFavorite(Class.class); System.out.printf(\"%s %x %s%n\", favoriteString, favoriteInteger, favoriteClass.getName()); } ``` A `Favorites` instance is _typesafe_: it will never return an `Integer` when you ask it for a `String`. It is also _heterogeneous_: unlike an ordinary map, all the keys are of different types. Therefore, we call `Favorites` a _typesafe heterogeneous container_. Typesafe heterogeneous container pattern - implementation ```java public class Favorites { private Map, Object> favorites = new HashMap(); public void putFavorite(Class type, T instance) { favorites.put(Objects.requireNonNull(type), instance); } public T getFavorite(Class type) { return type.cast(favorites.get(type)); } } ``` Achieving runtime type safety with a dynamic cast ```java public void putFavorite(Class type, T instance) { favorites.put(type, type.cast(instance)); } ``` ## Enums and annotations ### Use enums instead of int constants The _`int` enum pattern_ provides nothing in the way of type safety and little in the way of expressive power. ```java public static final int APPLE_FUJI = 0; public static final int APPLE_PIPPIN = 1; public static final int APPLE_GRANNY_SMITH = 2; public static final int ORANGE_NAVEL = 0; public static final int ORANGE_TEMPLE = 1; public static final int ORANGE_BLOOD = 2; ``` ```java public enum Apple { FUJI, PIPPIN, GRANNY_SMITH } public enum Orange { NAVEL, TEMPLE, BLOOD } ``` Java's enum types are classes that export one instance for each enumeration constant via a public static field num. Enum are effectively final because they have no accessible constructors. They are type safe, if you declare a parameter to be of type `Apple`, you are guaranteed that any non-null object reference passed to the parameter is one of the three valid `Apple` values. Enum types let you add arbitrary methods, provide high-quality implementations of all the `Object` methods, they implement `Comparable` and `Serializable`. To associate data with enum constants, declare instance fields and write constructor that takes the data and stores it in the fields. Enum type that switches on its own value, questionable. ```java public enum Operation { PLUS, MINUS, TIMES, DIVIDE; // Do the arithmetic operation represented by this constant public double apply(double x, double y) { switch(this) { case PLUS: return x + y; case MINUS: return x - y; case TIMES: return x * y; case DIVIDE: return x / y; } throw new AssertionError(\"Unknown op: \" + this); } } ``` Enum type with constant-specific method implementations ```java public enum Operation { PLUS {public double apply(double x, double y){return x + y;}}, MINUS {public double apply(double x, double y){return x - y;}}, TIMES {public double apply(double x, double y){return x * y;}}, DIVIDE{public double apply(double x, double y){return x / y;}}; public abstract double apply(double x, double y); } ``` Enum type with constant-specific class bodies and data ```java public enum Operation { PLUS(\"+\") { public double apply(double x, double y) { return x + y; } }, MINUS(\"-\") { public double apply(double x, double y) { return x - y; } }, TIMES(\"*\") { public double apply(double x, double y) { return x * y; } }, DIVIDE(\"/\") { public double apply(double x, double y) { return x / y; } }; private final String symbol; Operation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } public abstract double apply(double x, double y); } ``` **Use enums any time you need a set of constants whose members are known at compile time.** It is not necessary that the set of constants in an enum type stay fixed for all time, the enum was specifically designed to allow evolution of enum types. ### Use instance fields instead of ordinals Abuse of ordinal to derive an associated value, **don't do this**. ```java public enum Ensemble { SOLO, DUET, TRIO, QUARTET, QUINTET, SEXTET, SEPTET, OCTET, NONET, DECTET; public int numberOfMusicians() { return ordinal() + 1; } } ``` Never derive a value associated with an enum from its ordinal; store it in an instance field instead. ```java public enum Ensemble { SOLO(1), DUET(2), TRIO(3), QUARTET(4), QUINTET(5), SEXTET(6), SEPTET(7), OCTET(8), DOUBLE_QUARTET(8), NONET(9), DECTET(10), TRIPLE_QUARTET(12); private final int numberOfMusicians; Ensemble(int size) { this.numberOfMusicians = size; } public int numberOfMusicians() { return numberOfMusicians; } } ``` ### Use `EnumSet` instead of bit fields Bit field enumeration constants, obsolete. ```java public class Text { public static final int STYLE_BOLD = 1 styles) { ... } } ``` So you could use it like ```java text.applyStyles(EnumSet.of(Style.BOLD, Style.ITALIC)); ``` **Just because an enumerated type will be used in sets, there is no reason to represent it with bit fields.** ### Use `EnumMap` instead of ordinal indexing ```java class Plant { enum LifeCycle { ANNUAL, PERENNIAL, BIENNIAL } final String name; final LifeCycle lifeCycle; Plant(String name, LifeCycle lifeCycle) { this.name = name; this.lifeCycle = lifeCycle; } @Override public String toString() { return name; } } ``` Using `ordinal()` to index into an array, **don't do this**. ```java Set[] plantsByLifeCycle = (Set[]) new Set[Plant.LifeCycle.values().length]; for (int i = 0; i (); for (Plant p : garden) plantsByLifeCycle[p.lifeCycle.ordinal()].add(p); // Print the results for (int i = 0; i > plantsByLifeCycle = new EnumMap(Plant.LifeCycle.class); for (Plant.LifeCycle lc : Plant.LifeCycle.values()) plantsByLifeCycle.put(lc, new HashSet()); for (Plant p : garden) plantsByLifeCycle.get(p.lifeCycle).add(p); System.out.println(plantsByLifeCycle); ``` Or even a shorter version, a bit naive as it doesn't produce an `EnumMap` ```java System.out.println(Arrays.stream(garden) .collect(groupingBy(p -> p.lifeCycle))); ``` Fixed version ```java System.out.println(Arrays.stream(garden) .collect(groupingBy(p -> p.lifeCycle, () -> new EnumMap(LifeCycle.class), toSet()))); ``` **It is rarely appropriate to use ordinals to index into arrays: use EnumMap instead**. ### Emulate extensible enums with interfaces In the case for _opcodes_, _operation codes_ that represent operations in some machine, is desirable to let the user of an API provide their own operations, effectively extending the set of operations provided by the API. You can do this by implementing interfaces. ```java public interface Operation { double apply(double x, double y); } public enum BasicOperation implements Operation { PLUS(\"+\") { public double apply(double x, double y) { return x + y; } }, MINUS(\"-\") { public double apply(double x, double y) { return x - y; } }, TIMES(\"*\") { public double apply(double x, double y) { return x * y; } }, DIVIDE(\"/\") { public double apply(double x, double y) { return x / y; } }; private final String symbol; BasicOperation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } } ``` And an emulated extension enum ```java public enum ExtendedOperation implements Operation { EXP(\"^\") { public double apply(double x, double y) { return Math.pow(x, y); } }, REMAINDER(\"%\") { public double apply(double x, double y) { return x % y; } }; private final String symbol; ExtendedOperation(String symbol) { this.symbol = symbol; } @Override public String toString() { return symbol; } } ``` Now you can use your new operations anywhere you could use the basic operations, provided that API's are written to take the interface type `Operation`, not the implementation `BasicOperation`. ```java private void test(Collection opSet); ``` While you cannot write an extensible enum type, you can emulate it by writing an interface to accompany a basic enum type that implements the interface. ### Prefer annotations to naming patterns It's been common to use _naming patterns_ to indicate that some program elements demanded special treatment by a tool or framework. Like prefixing your tests with `test` word, so JUnit 3 would pick it up. These have many disadvantages: 1. Typographical errors result in silent failures. Imagine you accidentally named a test method `tsetSomething` instead of `testSomething`. JUnit 3 wouldn't complain, but it wouldn't run the test either. 2. There is no way to ensure that they are used only on appropriate program elements. Imagine creating a class `TestSafetyMechanism` in the hopes that JUnit 3 would automatically test its methods. 3. They provide no good way to associate parameter values with program elements. Suppose you want to support a category of test that succeeds only if it throws a particular exception, the exception type is the parameter of the test. You could encode the exception type in the test name but the compiler would have no way of knowing to check the string that was supposed to name the exception. Annotations solve all these problems nicely, and JUnit adopted them starting with release 4. Marker annotation type declaration ```java import java.lang.annotation.*; /** * Indicates that the annotated method is a test method. * Use only on parameterless static methods. */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface Test { } ``` Program to process marker annotations ```java import java.lang.reflect.*; public class RunTests { public static void main(String[] args) throws Exception { int tests = 0; int passed = 0; Class testClass = Class.forName(args[0]); for (Method m : testClass.getDeclaredMethods()) { if (m.isAnnotationPresent(Test.class)) { tests++; try { m.invoke(null); passed++; } catch (InvocationTargetException wrappedExc) { Throwable exc = wrappedExc.getCause(); System.out.println(m + \" failed: \" + exc); } catch (Exception exc) { System.out.println(\"Invalid @Test: \" + m); } } } System.out.printf(\"Passed: %d, Failed: %d%n\", passed, tests - passed); } } ``` There is no reason to use naming patterns when you can use annotations instead. With the exception of toolshmiths, most programmers will have no need to define annotation types. All programmers should use the predefined annotations types that Java provides. ### Consistently use the override annotation Buggy code, spot the error. ```java public boolean equals(Bigram b) { return b.first == first && b.second == second; } ``` With an `Override`, the program won't compile ```java @Override public boolean equals(Bigram b) { return b.first == first && b.second == second; } ``` Override will check overriding method at compile time. `equals` requires the parameter to be type `Object`. The fixed version ```java @Override public boolean equals(Object o) { if (!(o instanceof Bigram)) return false; Bigram b = (Bigram) o; return b.first == first && b.second == second; } ``` Use the `Override` annotation on every method declaration that you believe to override a superclass declaration. ### Use marker interfaces to define types A _marker interface_ is an interface that contains no method declarations but merely designates (or \"marks\") a class that implements the interface as having some property. An example of this is the `Serializable` interface. Marker interfaces define a type that is implemented by instances of the marked class; marker annotations do not. **The existence of a marker interface type allows you to catch errors at compile time that you couldn't catch until runtime if you used a marker annotation.** Another advantage of marker interfaces over marker annotations is that they can be targeted more precisely. If an annotation is declared with target `ElementType.TYPE`, it can be applied to _any_ class or interface. The chief advantage of a marker annotation over marker interfaces is that they are part of the larger annotation facility. Marker annotations allow for consistency in annotation-based frameworks. If you find yourself writing a marker annotation type whose target is `ElementType.TYPE`, take the time to figure out whether it is really should be an annotation type or whether a marker interface would be more appropriate. ## Lambdas and streams ### Prefer lambdas to anonymous classes Obsolete ```java Collections.sort(words, new Comparator() { public int compare(String s1, String s2) { return Integer.compare(s1.length(), s2.length()); } }); ``` Lambda expression as function object ```java Collections.sort(words, (s1, s2) -> Integer.compare(s1.length(), s2.length())); ``` Omit the types of all lambda parameters unless their presence makes your program clearer. Enum with function object fields and constant-specific behaviour ```java public enum Operation { PLUS (\"+\", (x, y) -> x + y), MINUS (\"-\", (x, y) -> x - y), TIMES (\"*\", (x, y) -> x * y), DIVIDE(\"/\", (x, y) -> x / y); private final String symbol; private final DoubleBinaryOperator op; Operation(String symbol, DoubleBinaryOperator op) { this.symbol = symbol; this.op = op; } @Override public String toString() { return symbol; } public double apply(double x, double y) { return op.applyAsDouble(x, y); } } ``` Lambdas lack names and documentation; if a computation isn't self-explanatory, or exceeds a few lines (more than three), don't put it in a lambda. You should rarely, if eve, serialise a lambda. Don't use anonymous classes for function objects unless you have to create instances of types that aren't functional interfaces. ### Prefer method references to lambdas Where method references are shorter and clearer, use them; where they aren't, stick with lambdas. ```java map.merge(key, 1, (count, incr) -> count + incr); ``` to ```java map.merge(key, 1, Integer::sum); ``` All kinds of method references are summarised below Method Ref Type | Example | Lambda equivalent ------------------|--------------------------|------------------------------------------------------ Static | `Integer::parseInt` | `str -> Integer.parseInt(str)` Bound | `Instant.now()::isAfter` | `Instant then = Instant.now(); t -> then.isAfter(t)` Unbound | `String::toLowerCase` | `str -> str.toLowerCase()` Class Constructor | `TreeMap::new` | `() -> new TreeMap` Array Constructor | `int[]::new` | `len -> new int[len]` ### Favour the use of standard functional interfaces Unnecessary functional interface; use a standard one instead. ```java @FunctionalInterface interface EldestEntryRemovalFunction{ boolean remove(Map map, Map.Entry eldest); } ``` If one of the standard functional interfaces does the job, you should generally use it in preference to a purpose-built functional interface. The six basic functional interfaces are summarised below Interface | Function Signature | Example --------------------|-----------------------|----------------------- `UnaryOperator` | `T apply(T t)` | `String::toLowerCase` `BinaryOperator` | `T apply(T t1, T t2)` | `BigInteger::add` `Predicate` | `boolean test(T t)` | `Collection::isEmpty` `Function` | `R apply(T t)` | `Arrays::asList` `Supplier` | `T get()` | `Instant::now` `Consumer` | `void accept(T t)` | `System.out.println` Don't be tempted to use basic functional interfaces with boxed primitives instead of primitive functional interfaces (prefer primitive types to boxed primitives, performance will suffer otherwise). **Always annotate your functional interfaces with the `@FunctionalInterface` annotation.** Its behaviour is similar to `@Override`, it tells the readers that the class was designed to enable lambdas; it won't compile unless it has exactly one abstract method; it prevents maintainers from accidentally adding abstract methods to the interface as it evolves. ### Use streams judiciously ```java words.collect(groupingBy(word -> alphabetize(word))) .values().stream() .filter(group -> group.size() >= minGroupSize) .forEach(g -> System.out.println(g.size() + \": \" + g)); ``` Stream pipelines are evaluated `lazily`, evaluation doesn't start until the terminal operation is invoked. Overusing streams makes programs hard to read and maintain. In the absence of explicit types, careful naming of lambda parameters is essential to the readability of stream pipelines. Using helper methods is even more important for readability in stream pipelines than in iterative code because pipelines lack explicit type information. If you are not sure whether a task is better served by streams or iteration, try both and see which works better. ### Prefer side-effect-free functions in streams The following code uses the streams API but not the paradigm, don't do this. ``` Map freq = new HashMap(); try (Stream words = new Scanner(file).tokens()) { words.forEach(word -> { freq.merge(word.toLowerCase(), 1L, Long::sum); }); } ``` Proper use of streams ``` Map freq; try (Stream words = new Scanner(file).tokens()) { freq = words .collect(groupingBy(String::toLowerCase, counting())); } ``` **The `forEach` operation should be used only to report the result of a stream computation, not to perform the computation.** Pipeline to get a top-ten list of words from a frequency table ```java List topTen = freq.keySet().stream() .sorted(comparing(freq::get).reversed()) .limit(10) .collect(toList()); ``` **It is customary and wise to statically import all members of `Collectors` because it makes stream pipelines more readable.** ### Prefer `Collection` to `Stream` as a return type Streams do not make iteration obsolete, writing good code requires combining streams and iteration judiciously. The `Collection` interface is a subtype of `Iterable` and has a `stream` method, so it provides both iteration and stream access. **`Collection` or an appropriate subtype is generally the best return type for a public, sequence-returning method.** But do not store large sequence in memory just to return it as a collection. If the sequence you're returning is large but can be represented concisely, consider implementing a special-purpose collection. ### Use caution when making streams parallel Parallelising a pipeline is unlikely to increase its performance if the source is from `Stream.iterate`, or the intermediate operation `limit` is used. Do not parallelise stream pipelines indiscriminately. As a rule, performance gains from parallelism are best on streams over `ArrayList`, `HashMap`, `HashSet`, and `ConcurrentHashMap` instances; arrays; `int` ranges; and `long` ranges. What all these data structures have in common is that they can be cheaply split into subranges of any desired sizes, which makes them easy to divide work among parallel threads. **Not only can parallelising a stream lead to poor performance, including liveness failures; it can lead to incorrect results and unpredictable behaviour.** Under the right circumstances, it is possible to achieve near-linear speedup in the number of processor cores simply by adding a `parallel` call to a stream pipeline. ## Methods ### Check parameters for validity Each time you write a method or constructor, you should think about what restrictions exist on its parameters. You should document these restrictions and enforce them with explicit checks at the beginning of the method body. If an invalid parameter value is passed to a method and the method checks its parameters before execution, it will fail quickly and cleanly with an appropriate exception. The `Objets.requireNonNull` method, added in Java 7, is flexible and convenient, so there's no reason to perform null checks manually anymore. For an unexported method, you control the circumstances under which the method is called, so you can and should ensure that only valid parameter values are passed in. Nonpublic methods can check their parameters using _assertions_. ### Make defensive copies when needed You must program defensively, with the assumption that clients of your class will do their best to destroy its invariants. Broken \"immutable\" time period class ```java public final class Period { private final Date start; private final Date end; /** * @param start the beginning of the period * @param end the end of the period; must not precede start * @throws IllegalArgumentException if start is after end * @throws NullPointerException if start or end is null */ public Period(Date start, Date end) { if (start.compareTo(end) > 0) throw new IllegalArgumentException( start + \" after \" + end); this.start = start; this.end = end; } public Date start() { return start; } public Date end() { return end; } } ``` Attack the internals of `Period` instance ```java Date start = new Date(); Date end = new Date(); Period p = new Period(start, end); end.setYear(78); // Modifies internals of p! ``` It is essential to make a defensive copy of each mutable parameter to the constructor. Repaired constructor, makes defensive copies of parameters. ```java public Period(Date start, Date end) { this.start = new Date(start.getTime()); this.end = new Date(end.getTime()); if (this.start.compareTo(this.end) > 0) throw new IllegalArgumentException( this.start + \" after \" + this.end); } ``` Defensive copies are made before checking the validity of the parameters, and the validity check is performed on the copies rather than the originals. Do not use the `clone` method to make a defensive copy of a parameter whose type is subclassable by untrusted parties. Second attack on the internals of a `Period` instance ```java Date start = new Date(); Date end = new Date(); Period p = new Period(start, end); p.end().setYear(78); // Modifies internals of p! ``` Return defensive copies of mutable internal fields. Repaired accessors, make defensive copies of internal fields ```java public Date start() { return new Date(start.getTime()); } public Date end() { return new Date(end.getTime()); } ``` You should, where possible, use immutable objects as components of your objects so that you don't have to worry about defensive copying. ### Design method signature carefully * Choose method names carefully. * Don't go overboard in providing convenience methods. When in doubt, leave it out. * Avoid long parameter lists. Aim for four parameters or fewer. Long sequences of identically typed parameters are especially harmful. * For parameter types, favour interfaces over classes. * Prefer two-element enum types to `boolean` parameters. ```java public enum TemperatureScale { FAHRENHEIT, CELSIUS } ``` ### Use overloading judiciously What does this program print? ```java public class CollectionClassifier { public static String classify(Set s) { return \"Set\"; } public static String classify(List lst) { return \"List\"; } public static String classify(Collection c) { return \"Unknown Collection\"; } public static void main(String[] args) { Collection[] collections = { new HashSet(), new ArrayList(), new HashMap().values() }; for (Collection c : collections) System.out.println(classify(c)); } } ``` This program prints `Unknown Collection` three times, because **the choice of which overloading to invoke is made at compile time**. The selection among overloaded methods is static, while selection among overriden methods is dynamic. ```java class Wine { String name() { return \"wine\"; } } class SparklingWine extends Wine { @Override String name() { return \"sparkling wine\"; } } class Champagne extends SparklingWine { @Override String name() { return \"champagne\"; } } public class Overriding { public static void main(String[] args) { List wineList = List.of( new Wine(), new SparklingWine(), new Champagne()); for (Wine wine : wineList) System.out.println(wine.name()); } } ``` Avoid confusing choices of overloading. A safe, conservative policy is never to export two overloading with the same number of parameters. You can always give methods different names instead of overloading them. Do not overload methods to take different functional interfaces in the same argument position. ### Use _varargs_ judiciously Simple use of varargs ```java static int sum(int... args) { int sum = 0; for (int arg : args) sum += arg; return sum; } ``` The **wrong** way to use varargs to pass one or more arguments. This program will fail at runtime instead of failing at compile time. ```java static int min(int... args) { if (args.length == 0) throw new IllegalArgumentException(\"Too few arguments\"); int min = args[0]; for (int i = 1; i cheesesInStock = ...; /** * @return a list containing all of the cheeses in the shop, * or null if no cheeses are available for purchase. */ public List getCheeses() { return cheesesInStock.isEmpty() ? null : new ArrayList(cheesesInStock); } ``` There is no reason to special-case the situation where the no cheeses are available for purchase. Doing so requires extra code in the client to handle the possibly null return value ```java List cheeses = shop.getCheeses(); if (cheeses != null && cheeses.contains(Cheese.STILTON)) System.out.println(\"Jolly good, just the thing.\"); ``` The right way to return a possibly empty collection ```java public List getCheeses() { return new ArrayList(cheesesInStock); } ``` Optimisation, avoid allocating empty collections ```java public List getCheeses() { return cheesesInStock.isEmpty() ? Collections.emptyList() : new ArrayList(cheesesInStock); } ``` The right way to return a possibly empty array ```java public Cheese[] getCheeses() { return cheesesInStock.toArray(new Cheese[0]); } ``` Optimisation, avoids allocating empty arrays ```java private static final Cheese[] EMPTY_CHEESE_ARRAY = new Cheese[0]; public Cheese[] getCheeses() { return cheesesInStock.toArray(EMPTY_CHEESE_ARRAY); } ``` **Never return `null` in place for an empty array or collection.** ### Return optionals judiciously Optionals are similar in spirit to checked exceptions. You should declare a method to return `Optional` if it might not be able to return a result and clients will have to perform special processing if no result is returned. Container types, including collections, maps, streams, arrays and optionals should not be wrapped in optionals. Rather than returning an empty `Optional>`, you should simply return an empty `List`. Returning an optional that contains a boxed primitive is prohibitively expensive as it has two levels of boxing instead of zero. Library designers saw fit to provide analogues of Optional for primitive types `int`, `long` and `double` (`OptionalInt`, `OptionalLong` and `OptionalDouble`). **You should never return an optional of a boxed primitive type.** It is almost never appropriate to use an optional as a key, value, or element in a collection array. Often storing an optional in an instance field is a \"bad smell\", but sometimes may be justified. ### Write doc comments for all exposed API elements To document your API properly, you must precede every exported class, interface, constructor, method, and field declaration with a doc comments. The doc comment for a method should describe succinctly the contract between the method and its client. Doc comments should be readable both in the source code and in the generated documentation. No two members or constructors in a class or interface should have the same summary description. When documenting a generic type or method, be sure to document all type parameters. ```java /** * An object that maps keys to values. A map cannot contain * duplicate keys; each key can map to at most one value. * * (Remainder omitted) * * @param the type of keys maintained by this map * @param the type of mapped values */ public interface Map { ... } ``` When documenting an enum type, be sure to document the constants. ```java /** * An instrument section of a symphony orchestra. */ public enum OrchestraSection { /** Woodwinds, such as flute, clarinet, and oboe. */ WOODWIND, /** Brass instruments, such as french horn and trumpet. */ BRASS, /** Percussion instruments, such as timpani and cymbals. */ PERCUSSION, /** Stringed instruments, such as violin and cello. */ STRING; } ``` When documenting an annotation type, be sure to document any members as the type itself. ```java /** * Indicates that the annotated method is a test method that * must throw the designated exception to pass. */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface ExceptionTest { /** * The exception that the annotated test method must throw * in order to pass. (The test is permitted to throw any * subtype of the type described by this class object.) */ Class value(); } ``` Whether or not a class or static method is thread-safe, you should document its thread-safely level. The generated documentation should provide a clear description of your API. The only way to know for sure is to read the web pages generated by the Javadoc utility. ## General programming ### Minimise the scope of local variables By minimising the scope of local variables, you increase the readability and maintainability of your code and reduce the likelihood for error. **The most powerful technique for minimising the scope of a local variable is to declare it where it is first used.** Nearly every local variable declaration should contain an initialiser. Prefer `for` loops to `while` loops, as `for` loops limit the scope of the variables defined in their bodies. Idiom for iterating when you need the iterator ```java for (Iterator i = c.iterator(); i.hasNext(); ) { Element e = i.next(); ... // Do something with e and i } ``` A final technique to minimise the scope of local variables is to keep methods small and focused. ### Prefer `for-each` loops to traditional `for` loops Not the best way to iterate over a collection ```java for (Iterator i = c.iterator(); i.hasNext(); ) { Element e = i.next(); ... // Do something with e } ``` Not the best way to iterate over an array ```java for (int i = 0; i sonSet = new LinkedHashSet(); ``` Bad, it uses class as type ```java LinkedHashSet sonSet = new LinkedHashSet(); ``` **If you get into the habit of using interfaces as types, your program will be much more flexible.** If you decide to switch implementations, all you have to do is change the class name in the constructor. It is entirely appropriate to refer to an object by a class rather than an interface if no appropriate interface exists. When there is no appropriate interface, just use the least specific class int he class hierarchy that provides the required functionality. ### Prefer interfaces to reflection Reflection allows one class to use another, even if the latter class did not exist when the former was compiled. This comes at a price: * You lose all the benefits of compile-type type checking. * The code required to perform reflective access is clumsy and verbose. * Performance suffers. You can obtain many of the benefits of reflection while incurring few of its costs by using it in a very limited form. Create instances reflectively and access them normally via their interface or superclass. Reflective instantiation with interface access. ```java public static void main(String[] args) { // Translate the class name into a Class object Class> cl = null; try { cl = (Class>) // Unchecked cast! Class.forName(args[0]); } catch (ClassNotFoundException e) { fatalError(\"Class not found.\"); } // Get the constructor Constructor> cons = null; try { cons = cl.getDeclaredConstructor(); } catch (NoSuchMethodException e) { fatalError(\"No parameterless constructor\"); } // Instantiate the set Set s = null; try { s = cons.newInstance(); } catch (IllegalAccessException e) { fatalError(\"Constructor not accessible\"); } catch (InstantiationException e) { fatalError(\"Class not instantiable.\"); } catch (InvocationTargetException e) { fatalError(\"Constructor threw \" + e.getCause()); } catch (ClassCastException e) { fatalError(\"Class doesn't implement Set\"); } // Exercise the set s.addAll(Arrays.asList(args).subList(1, args.length)); System.out.println(s); } private static void fatalError(String msg) { System.err.println(msg); System.exit(1); } ``` ### Use native methods judiciously The Java Native Interface (JNI) allows Java programs to call _native methods_ written in _native programming languages_ such as C or C++. It is rarely advisable to use native methods for improved performance. ### Optimise judiciously Strive to write good programs rather than fast ones. Strive to avoid design decisions that limit performance. Consider the performance consequences of your API design decisions. It is a very bad idea to warp an API to achieve good performance. The performance issue that caused you to warp the API may go away in a future. Measure performance before and after each attempted optimisation. ### Adhere to generally accepted naming conventions Package and module names should be hierarchical, with components separated by periods. Alphabetic characters, rarely digits. Components should be short, meaningful abbreviations or acronyms. Class and interface names, including enum and annotation type names, should consist of one or more words, with the first letter of each word capitalised. Abbreviations should be avoided. If you use acronyms, capitalise just the first letter. Better to have `HttpUrl` than `HTTPURL`. Method and field names follow the same conventions as before, except that first letter should be lowercase. Constant fields should consist of one or more upper case words separated by the underscore character. Local variable names have similar conventions to member names, except that abbreviations are permitted. Type parameters consist of a single letter. `T` for arbitrary type `E` for element type, `K` and `V` for key and value types of a map, and `X` for exception. Return type is usually `R`. A sequence of arbitrary types can be `T`, `U`, `V` or `T1`, `T2`, `T3`. Instantiable classes, including enum types are generally named with a singular noun or noun phrase. Non instantiable utility classes are often named with a plural noun or with an adjective ending in `able` or `ible`. In case of annotations, nouns, verbs, prepositions and adjectives are all common. Methods that perform some action are generally named with a verb or verb phrase. Methods that return a `boolean` value usually have names that begin with the word `is` or `has` followed by a noun, noun phrase that functions as an adjective. Method that return non-`boolean` attribute of the object are usually named with a noun, a noun phrase or a verb phrase beginning with the verb `get`, although nowadays might not be necessary. Instance methods that convert the type of a n object, are often called `to`_`Type`_. Methods that return a view whose type differs from that of receiving object are often called as `as`_Type_. Methods that return a primitive with the same value as the object on which they're invoked are often called _type_`Value`. Fields of type `boolean` are often named like `boolean` accessor methods with the initial `is` omitted. Fields of other types are usually named with nouns or noun phrases. ## Exceptions ### Use exceptions for exceptional conditions Horrible abuse of exceptions ```java try { int i = 0; while(true) range[i++].climb(); } catch (ArrayIndexOutOfBoundsException e) { } ``` * Exception processing is slow * Placing code into a `try-catch` block inhibits some optimisations the JVM perform Exceptions should never be used for ordinary control flow. A well-designed API must not force its clients to use exceptions for ordinary control flow. ### Use checked exceptions for recoverable conditions and runtime exceptions for programming errors Use checked exceptions for conditions from which the caller can reasonably be expected to recover. Use runtime exceptions to indicate programming errors. _Precondition violations_ or the failure by the client of an API to adhere to the contract established by the API specification. `Error` exceptions are usually intended for the JVM to indicate conditions that make it impossible to continue execution. Therefore **all of the unchecked throwables you implement should subclass `RuntimeException`.** You shouldn't throw `Error` exceptions either. Exceptions are full-fledged objects, define methods to provide additional information concerning the condition that caused the exception. This is crucial for checked exceptions. ### Avoid unnecessary use of checked exceptions Checked exceptions _force_ programmers to deal with problems. Overuse of checked exceptions can make them far less pleasant to use. ### Favour the use of standard exceptions Do not reuse `Exception`, `RuntimeException`, `Throwable` or `Error` directly. Most commonly reused exceptions Exception | Occasion for use ----------------------------------|----------------- `IllegalArgumentException` | Non-null parameter value is inappropriate `IllegalStateException` | Object state is inappropriate for method invocation `NullPointerException` | Parameter value is null where prohibited `IndexOutOfBoundsException` | Index parameter value is out of range `ConcurrentModificationException` | Concurrent modification of an object has been detected where it is prohibited `UnsupportedOperationException` | Object does not support method Throw `IllegalStateException` if no argument values would have worked, otherwise throw `IllegalArgumentException`. ### Throw exceptions appropriate to the abstraction Higher layers should catch lower-level exceptions and, in their place, throw exceptions that can be explained in terms of the higher-level abstraction. Exception translations ```java try { ... // Use lower-level abstraction to do our bidding } catch (LowerLevelException e) { throw new HigherLevelException(...); } ``` Sometimes the low-level exception might be helpful to someone debugging the higher-level. _Exception chaining_ ```java try { ... // Use lower-level abstraction to do our bidding } catch (LowerLevelException cause) { throw new HigherLevelException(cause); } ``` While exception translation is superior to mindless propagation of exceptions from lower layers, it should not be overused. The best way to deal with exceptions from lower layers is to avoid them, by ensuring that lower-level methods succeed. Sometimes you can do this by checking the validity of parameters to be passed to lower level abstractions. ### Document all exceptions thrown by each method Always declare checked exceptions individually, and document precisely the conditions under which each one is thrown. Use the Javadoc `@throws` tag to document each exception that a method can throw, but do not use the `throws` keyword on unchecked exceptions. If an exception is thrown by many methods in a class for the same reason, you can document the exception in the class's documentation comment. ### Include failure-capture information in detail messages **To capture a failure, the detail message of an exception should contain the values of all parameters and fields that contributed to the exception.** For example `IndexOutOfBoundsException` should contain the lower bound, the upper bound, and the index value that failed. Do not include passwords, encryption keys, and the like in detail messages as stack traces may be seen by many people diagnosing and fixing software issues. One way to ensure that exceptions contain adequate failure-capture information in their detail messages is to require this information in their constructors. ### Strive for failure atomicity Generally speaking, a failed method invocation should leave the object in the state that it was in prior the invocation. The simplest way is to design immutable objects, where failure atomicity is free. For methods that operate on mutable objects, the most common way to achieve failure atomicity is to check parameters for validity before performing the operation. ### Don't ignore exceptions An empty `catch` block defeats the purpose of exceptions. ```java try { ... } catch (SomeException e) { } ``` If you choose to ignore an exception, the `catch` block should contain a comment explaining why it is appropriate to do so and the variable should be named `ignored`. ```java Future f = exec.submit(planarMap::chromaticNumber); int numColors = 4; // Default; guaranteed sufficient for any map try { numColors = f.get(1L, TimeUnit.SECONDS); } catch (TimeoutException | ExecutionException ignored) { // Use default: minimal coloring is desirable, not required } ``` ## Concurrency ### Synchronise access to shared mutable data Synchronisation is required for reliable communication between threads as well as for mutual exclusion. Do not use `Thread.stop` as it is inherently _unsafe_, its use can result in data corruption. Broken ```java public class StopThread { private static boolean stopRequested; public static void main(String[] args) throws InterruptedException { Thread backgroundThread = new Thread(() -> { int i = 0; while (!stopRequested) i++; }); backgroundThread.start(); TimeUnit.SECONDS.sleep(1); stopRequested = true; } } ``` Properly synchronised cooperative thread termination ```java public class StopThread { private static boolean stopRequested; private static synchronized void requestStop() { stopRequested = true; } private static synchronized boolean stopRequested() { return stopRequested; } public static void main(String[] args) throws InterruptedException { Thread backgroundThread = new Thread(() -> { int i = 0; while (!stopRequested()) i++; }); backgroundThread.start(); TimeUnit.SECONDS.sleep(1); requestStop(); } } ``` Synchronisation is not guaranteed to work unless both read and write operations are synchronised. Either share immutable data, or don't share at all. Confine mutable data into a single thread. When multiple threads share mutable data, each thread that reads or writes the data must perform synchronisation. ### Avoid excessive synchronisation To avoid liveness and safety failures, never cede control to the client within a synchronised method or block. Inside a synchronised region, do not invoke a method that is designed to be overridden, or one provided by a client in the form a function object. As a rule, you should do as little work as possible inside synchronised regions. ### Prefer executors, tasks, and streams to threads ```java ExecutorService exec = Executors.newSingleThreadExecutor(); ``` ```java exec.execute(runnable); ``` ```java exec.shutdown(); ``` You should refrain from working directly with threads, a `Thread` serves as both a unit of work and the mechanism for executing it. In the executor framework, the unit of work and the execution are separate. ### Prefer concurrency utilities to `wait` and `notify` Given the difficulty of using `wait` and `notify` correctly, you should use the higher-level concurrency utilities instead. It is impossible to exclude concurrent activity from a concurrent collection; locking it will only slow the program. Use `ConcurrentHashMap` in preference to `Collections.synchronizedMap`. Doing so can dramatically increase the performance of concurrent applications. For interval timing, always use `System.nanoTime` rather than `System.currentTimeMillis`. `System.nanoTime` is more accurate and is unaffected by adjustments to the system's real-time clock. The standard idiom for using `wait` method ```java synchronized (obj) { while () obj.wait(); // (Releases lock, and reacquires on wakeup) ... // Perform action appropriate to condition } ``` Always use the wait loop idiom to invoke the `wait` method; never invoke it outside of a loop. The loop serves to test the condition before and after waiting. There is seldom, if ever, a reason to use `wait` and `notify` in new code. ### Document thread safety The presence of `synchronized` modifier in a method declaration is an implementation detail, not a part of its API. To enable safe concurrent use, a class must clearly document what level of thread safety it supports: * **Immutable.** No extra synchronisation is necessary. * **Unconditionally thread-safe.** Instances are mutable, but clients don't need to worry about synchronisation. * **Conditionally thread-safe.** Instances are mutable. Some methods require external synchronisation for safe concurrent use. * **Not thread-safe.** Instances are mutable, clients must surround each method invocation with external synchronisation. * **Thread-hostile.** The class is unsafe for concurrent use even if every method invocation is surrounded by external synchronisation. Lock fields should always be declared `final`. ### Use lazy evaluation judiciously Under most circumstances, normal initialisation is preferable to lazy initialisation. Normal initialisation of an instance field ```java private final FieldType field = computeFieldValue(); ``` If you use lazy initialisation to break an initialisation circularity, use a synchronised accessor. ```java private FieldType field; private synchronized FieldType getField() { if (field == null) field = computeFieldValue(); return field; } ``` If you need to use lazy initialisation for performance on a static field, use the lazy initialisation holder class idiom. ```java private static class FieldHolder { static final FieldType field = computeFieldValue(); } private static FieldType getField() { return FieldHolder.field; } ``` If you need to use lazy initialisation for performance on an instance field, use the double-check idiom. ```java private volatile FieldType field; private FieldType getField() { FieldType result = field; if (result == null) { // First check (no locking) synchronized(this) { if (field == null) // Second check (with locking) field = result = computeFieldValue(); } } return result; } ``` ### Don't depend on thread scheduler Any program that relies on the thread scheduler for correctness or performance is likely to be nonportable. Threads should not run if they aren't doing useful work. Awful `CountDownLatch` implementation, busy-waits incessantly. ```java public class SlowCountDownLatch { private int count; public SlowCountDownLatch(int count) { if (count 0) throw new InvalidObjectException(start +\" after \"+ end); } ``` * For classes with object reference fields that must remain private, defensively copy each object in such a field. Mutable components of immutable classes fall into this category. * Check any invariants and throw an `InvalidObjectException` if a check fails. The checks should follow any defensive copying. * If an entire object graph must be validated after it is deserialised, use the `ObjectInputValidation` interface. * Do not invoke any overridable methods in the class, directly or indirectly. ### For instance control, prefer enum types to `readResolve` ```java public class Elvis { public static final Elvis INSTANCE = new Elvis(); private Elvis() { ... } public void leaveTheBuilding() { ... } } ``` `readResolve` for instance control, you can do better! ```java private Object readResolve() { // Return the one true Elvis and let the garbage collector // take care of the Elvis impersonator. return INSTANCE; } ``` If you depend on `readResolve` for instance control, all instance field with object reference types myst be declared `transient`. An attacker might secure a reference to the deserialised object before its `readResolve` method is run. Broken singleton, has nontransient object reference field. ```java public class Elvis implements Serializable { public static final Elvis INSTANCE = new Elvis(); private Elvis() { } private String[] favoriteSongs = { \"Hound Dog\", \"Heartbreak Hotel\" }; public void printFavorites() { System.out.println(Arrays.toString(favoriteSongs)); } private Object readResolve() { return INSTANCE; } } ``` Enum singleton, the preferred approach. ```java public enum Elvis { INSTANCE; private String[] favoriteSongs = { \"Hound Dog\", \"Heartbreak Hotel\" }; public void printFavorites() { System.out.println(Arrays.toString(favoriteSongs)); } } ``` The accessibility of `readResolve` is significant. On final classes, it should be private. On nonfinal class, you must carefully consider its accessibility. ### Consider serialisation proxies instead of serialised instances Making classes implement `Serializable` increases the likelihood of bugs and security problems as it allows instances to be created using extralinguistic mechanism in place of ordinary constructors. There is a technique that greatly reduces the risks called _serialisation proxy pattern_. Serialisation proxy for `Period` class ```java private static class SerializationProxy implements Serializable { private final Date start; private final Date end; SerializationProxy(Period p) { this.start = p.start; this.end = p.end; } private static final long serialVersionUID = 234098243823485285L; // Any number will do (Item 87) } ``` `writeReplace` method for the serialisation proxy pattern ```java private Object writeReplace() { return new SerializationProxy(this); } ``` `readObject` method for the serialisation proxy pattern ```java private void readObject(ObjectInputStream stream) throws InvalidObjectException { throw new InvalidObjectException(\"Proxy required\"); } ``` `readResolve` method for `Period.SerializationProxy` ```java private Object readResolve() { return new Period(start, end); // Uses public constructor } ``` Consider the serialisation proxy pattern whenever you find yourself having to write a `readObject` or `writeObject` method on a class not extendable by its clients. This pattern is perhaps the easiest way to robustly serialise object with nontrivial invariants. ",
    "url": "/learning-notes/books/effective-java/",
    "relUrl": "/books/effective-java/"
  },"12": {
    "doc": "The Elements of Programming Style",
    "title": "The Elements of Programming Style",
    "content": "# [The Elements of Programming Style](https://www.goodreads.com/book/show/454039.The_Elements_of_Programming_Style) - [Introduction](#introduction) - [Expression](#expression) - [Control structure](#control-structure) - [Program structure](#program-structure) - [Input and output](#input-and-output) - [Common blunders](#common-blunders) - [Efficiency and instrumentation](#efficiency-and-instrumentation) - [Documentation](#documentation) - [Epilogue](#epilogue) ## Introduction * ### Write clearly, don't be too clever It is more important to make the purpose of the code unmistakable than to display virtuosity. The problem with obscure code is that debugging and modification become much more difficult, and these are already the hardest aspects of computer programming. Real programs are like prose. Although details vary from language to language, _the principles of style_ are the same. Branching around branches is confusing in any language. The job of critical reading doesn't end when you find a typo or even a poor coding practice. Don't treat computer output as gospel. If you learn to be wary of everyone else's programs, you will be better able to check your own. ## Expression * ### Say what you mean, simply and directly No amount of commenting, formatting, or supplementary documentation can entirely replace well expressed statements. After all, they determine what the program actually does. * ### Use library functions Library functions help to keep program size manageable, and they let you build on the work of others, instead of starting from scratch each time. Debugging is twice as hard as writing a program in the first place. So if you're as clever as you can be when you write it, how will you ever debug it? * ### Avoid temporary variables The fewer temporary variables in a program, the less chance there is that one will not be properly initialized, or that one will be altered unexpectedly before it is used. * ### Write clearly, don't sacrifice clarity for \"efficiency\" There is little justification for using the more obscure mode of expression. The harder it is for _people_ to grasp the intent of any given section, the longer it will be before the program becomes operational. Trying to outsmart a compiler defeats much of the purpose of using one. * ### Let the machine do the dirty work Repeated patterns of code catch the eye when scanning listings, why didn't the programmer let the computer do the repeating? * ### Replace repetitive expressions by calls to a common junction * ### Parenthesize to avoid ambiguity * ### Choose variable names that won't be confused * ### Avoid unnecessary branches One of the most productive ways to make a program easier to understand is to reduce the degree of interdependence between statements, so that each part can be studied and understood in relative isolation. * ### Use the good features of a language, avoid the bad ones * ### Don't use conditional branches as a substitute for a logical expression --- The inversion and double parentheses slow comprehension. Example ```java if (!(foo == false || bar == false)) { //... } ``` Apply Morgan's rules ```java !(A || B) == !A && !B !(A && B) == !A || !B ``` --- * ### Use the \"telephone test\" for readability If someone could understand your code when read aloud over the telephone, it's clear enough. If not, then it needs rewriting. ## Control structure A computer program is shaped by its data representation and the statements that determine its flow of control. We will concentrate on matters of style that affect the program as a whole. * ### Use `IF-ELSE` to emphasize that only one of two actions is to be performed Be careful when tests might overlap. ```java if (hoursWorked = 40) return otherPay(); ``` An `IF-ELSE` ensures that someone reading the code can see that only one thing is done. ```java if (hoursWorked 50) { if (amountOfSales 100) { commission = 0.03 * amountOfSales; } ``` Better to write ```java if (amountOfSales How can I organize the data so the computation becomes as easy as possible? Clarity is certainly not worth sacrificing just to save three tests per access. * ### Don't stop with your first draft No program is ever perfect; there is always room for improvement. It is foolish to polish a program beyond the point of diminishing returns, but most programmers do too little revision; they are satisfied too early. ## Program structure Most programs are too big to be comprehended as a single chunk. They must be divided into smaller pieces that can be conquered separately. Subroutines, functions, and procedures are the \"modules,\" or building blocks, of large programs usable in other applications, contributing to a library of labor-saving routines. When a program is not broken up into small enough pieces, the larger modules often fail to deliver on. * ### Modularize. Use subroutines It must be possible to describe the function performed by a module in the briefest of terms; and it is necessary to minimize whatever relationships exist with other modules, and display those that remain as explicitly as possible. * ### Make the coupling between modules visible If we wish to keep this as a separate module, the remaining shared data, should be explicitly passed as arguments. * ### Each module should do one thing well The print operation has nothing to do with the calculation; it merely happens to use the result. Combining too many functions in one module is a sure way to limit its usefulness, while at the same time making it more complex and harder to maintain. * ### Make sure every module hides something It is best to hide the details. inside a function that has a simple interface to the outside world. Exposing internal details increments general confusion and may well have to be changed later on. One good test of the worth of a module, in fact, is how good a job it does of hiding some aspect of the problem from the rest of the code. > A module should hide from its fellows the details of how it performs its task, for otherwise one module cannot be changed independently of others. * ### Let the data structure the program A big program should be a collection of manageable pieces, each of which must obey the rules of good style. * ### Don't patch bad code, rewrite it The fact that a four line comment is needed to explain what is going on should be reason enough to rewrite the code. Patching only serves to emphasize the shortcomings of this organization. --- In a top-down design, we start with a very general pseudo-code statement of the program like ``` solve mazes ``` and then elaborate this statement in stages, filling in details until we ultimately reach executable code. --- A powerful tool for reducing apparent complexity is recursion. In a recursive procedure, the method of solution is defined in terms of itself. The trick is to reduce each hard case to one that is handled simply elsewhere. --- * ### Write and test a big program in small pieces Once a program works, we need no longer concern ourselves with how it does something, only with the fact that it does. We thus have some assurance that we can deal with the program a small section at a time without much concern for the rest of the code. * ### Use recursive procedures for recursively-defined data structures ## Input and output * ### Test input for validity and plausibility * ### Make sure input cannot violate the limits of the program * ### Identify bad input, recover if possible * ### Make input easy to prepare and output self-explanatory Use of numeric codes is bad practice in a program that people use directly. The use of mnemonics like `RED` and `BLUE` instead of numeric codes like `1` and `2` is commendable, for it makes the program easier to use correctly. Use alphabetic names instead of numeric codes. * ### Use uniform input formats * ### Make input easy to proofread * ### Use free-form input when possible When there are many (i.e., more than one or two) arguments or parameters to be provided to a program, let the users specify their parameters by name. ```groovy method(feed: 27, diameter: 3.5, rpm: 3600) ``` If input parameters are supplied by name, you can use default values in a graceful way. * ### Use self-identifying input. Allow defaults. Echo both on output --- The hard part of programming is controlling complexity - keeping the pieces decoupled so they can be dealt with separately instead of all at once. And the need to separate into pieces is not some academically interesting point, but a practical necessity, to keep things from interacting with each other in unexpected ways. Writing a separate input function is a prime example of decoupling. --- * ### Localize input and output in subroutines Input/output is the interface between a program and its environment. **Never trust any data, and remember the user.** Localize I/0 instead of spreading it all over the program. Hide the details of end of file, buffering, etc., in functions. ## Common blunders A major concern of programming is making sure that a program can defend against bad data. * ### Make sure all variables are initialized before use You should still distinguish between true constants and initialized variables. * ### Don't stop at one bug * ### Watch out/or off-by-one errors A common cause of off-by-one errors is an incorrect test, for example using \"greater than\" when \"greater than or equal to\" is actually needed. * ### A void multiple exits from loops * ### Make sure your code \"does nothing\" gracefully Degenerate cases frequently arise where a piece of code has nothing to do - in this instance, when N is one or two, no search is necessary. In such cases it is important to \"do nothing\" gracefully; the `DO-WHILE` has this useful property. * ### Test programs at their boundary values Extra tests and branches add no safety factor. Quite the contrary, their existence makes the program that much harder to read and understand. The code will perform identically if the redundant tests are eliminated. One good strategy, both for writing and for testing, is to concentrate on the boundaries inherent in the program. The most obvious boundary in any program, and often the easiest to test, is what it does when presented with **no data at all**. The next boundary, also easy, is for **one data item**. And this time we see what is wrong. * ### Program defensively \"Defensive programming\" means anticipating problems in advance, and cod- ing to avoid errors before they arise. Another way to head off potential disasters is to \"program defensively.\" Anticipate that in spite of good intentions and careful checking, things will sometimes go awry, and take some steps to catch errors before they propagate too far. At the cost of an occasional extra test and a little extra code, the program limits the spread of nonsense should anything damage the board. An important aspect of defensive programming is to be alert for these \"impossible\" conditions and to steer them in the safer direction. * ### 10.0 times 0.1 is hardly ever 1.0 Floating point arithmetic adds a new spectrum of errors. * ### Don't compare floating point numbers just for equality ## Efficiency and instrumentation Machines have become increasingly cheap compared to people. \"Efficiency\" involves the reduction of overall cost - not just machine time over the life of the program, but also time spent by the programmer and by the users of the program. Efficiency does not have to be sacrificed in the interest of writing readable code - rather, writing readable code is often the only way to ensure efficient programs that are also easy to maintain and modify. **Premature optimization is the root of all evil.** * ### Make it right before you make it faster * ### Keep it right when you make it faster Concern for efficiency should be tempered with some concern for the probable benefits, and the probable costs. * ### Make it clear before you make it faster Simplicity and clarity are often of more value than the microseconds possibly saved by clever coding. * ### Don't sacrifice clarity for small gains in \"efficiency\" * ### Let your compiler do the simple optimizations * ### Don't strain to re-use code, reorganize instead * ### Make sure special cases are truly special What did concern with \"efficiency\" in the original version produce, besides a bigger, slower, and more obscure program? * ### Keep it simple to make it faster Complexity loses out to simplicity. Fundamental improvements in performance are most often made by algorithm changes, not by tuning. * ### Don't diddle code to make it faster, find a better algorithm Time spent selecting a good algorithm is certain to pay larger dividends than time spent polishing an implementation of a poor method. For any given algorithm, polishing is not likely to significantly improve a fundamentally sound, clean implementation. * ### Instrument your programs. Measure before making \"efficiency\" changes The cost of computing hardware has steadily decreased; software cost has steadily increased. \"Efficiency\" should concentrate on reducing the expensive parts of computing. ## Documentation The best documentation for a computer program is a clean structure. The only reliable documentation of a computer program is the code itself. Whenever there are multiple representations of a program, the chance for discrepancy exists. If the code is in error, artistic flowcharts and detailed comments are to no avail. Only by read- ing the code can the programmer know for sure what the program does. This is not to say that programmers should never write documentation. It is vital to maintain readable descriptions of what each program is supposed to do, how it is used, how it interacts with other parts of the system, and on what principles it is based. Anything that contributes no new information, but merely echoes the code, is superfluous. A comment is of zero (or negative) value if it is wrong. Code must largely document itself. If it cannot, rewrite the code rather than increase the supplementary documentation. Good code needs fewer comments than bad code does. * ### Make sure comments and code agree The trouble with comments that do not accurately reflect the code is that they may well be believed subconsciously, so the code itself is not examined critically. * ### Don't just echo the code with comments, make every comment count * ### Don't comment bad code, rewrite it A bad practice well commented remains bad. Variable names, labels can aid or hinder documentation. * ### Use variable names that mean something * ### Format a program to help the reader understand it The single most important formatting convention that you can follow is to indent your programs properly. * ### Indent to show the logical structure of a program If we eliminate the unnecessary grouping and nesting, things clarify remarkably. Unless we are consistent, you will not be able to count on what our formatting is trying to tell you about the programs. Good formatting is a part of good programming. * ### Document your data layouts One of the most effective ways to document a program is sim- ply to describe the data layout in detail. If you can specify for each important variable what values it can assume and how it gets changed, you have gone a long way to describing the program. * ### Don't over-comment We use few comments in our programs - most of the pro- grams are short enough to speak for themselves. ## Epilogue Programmers have a strong tendency to underrate the importance of good style. Eternally optimistic, we all like to think that once we throw a piece of code together, however haphazardly, it will work properly the first time and ever after. One excuse for writing an unintelligible program is that it is _a private matter_. It is the same justification you use for writing \"qt milk, fish, big box\" for a grocery list instead of composing a proper sentence. If the list is intended for someone else, of course, you had better specify what kind of fish you want and what should be inside that big box. But even if only you personally want to understand the message, if it is to be readable a year from now you must write a complete sentence. So in your diary you might write, \"Today I went to the supermarket and bought a quart of milk, a pound of halibut, and a big box of raisins.\" You learn to write as if to someone else because next year you will be \"someone else.\" **\"Style\" is not a list of rules so much as an approach and an attitude. \"Good programmers\" are those who already have learned a set of rules that ensures good style.** ",
    "url": "/learning-notes/books/elements-of-programming-style/",
    "relUrl": "/books/elements-of-programming-style/"
  },"13": {
    "doc": "Escaping the Build Trap",
    "title": "Escaping the Build Trap",
    "content": "# Escaping the Build Trap - [The value exchange system](#the-value-exchange-system) - [Constraints on the value exchange system](#constraints-on-the-value-exchange-system) - [Project vs Products vs Services](#project-vs-products-vs-services) - [The product-led organisation](#the-product-led-organisation) - [Sales-led](#sales-led) - [Visionary-led](#visionary-led) - [Technology-led](#technology-led) - [Product-led](#product-led) - [What we know and what we don't](#what-we-know-and-what-we-dont) - [The role of a product manager](#the-role-of-a-product-manager) - [Bad product manager archetypes](#bad-product-manager-archetypes) - [The mini-CEO](#the-mini-ceo) - [The waiter](#the-waiter) - [The former project manager](#the-former-project-manager) - [A great product manager](#a-great-product-manager) - [Tech expert versus market expert](#tech-expert-versus-market-expert) - [A great product manager](#a-great-product-manager-1) - [Start with the why](#start-with-the-why) - [One role, many responsibilities](#one-role-many-responsibilities) - [The product manager career path](#the-product-manager-career-path) - [Organising your teams](#organising-your-teams) - [Strategy](#strategy) - [What is strategy?](#what-is-strategy) - [Strategic gaps](#strategic-gaps) - [Creating a good strategic framework](#creating-a-good-strategic-framework) - [Company-level vision and strategic intents](#company-level-vision-and-strategic-intents) - [Product vision and portfolio](#product-vision-and-portfolio) - [Product management process](#product-management-process) - [The product kata](#the-product-kata) - [Understanding the direction and setting success metrics](#understanding-the-direction-and-setting-success-metrics) - [Problem exploration](#problem-exploration) - [Solution exploration](#solution-exploration) - [Building and optimising your solution](#building-and-optimising-your-solution) - [The product-led organisation](#the-product-led-organisation-1) - [Outcome-focused communication](#outcome-focused-communication) - [Rewards and incentives](#rewards-and-incentives) - [Safety and learning](#safety-and-learning) - [Budgeting](#budgeting) - [Customer centricity](#customer-centricity) - [Escaping the build trap to become product-led](#escaping-the-build-trap-to-become-product-led) - [Six questions to determine wheter a company is product-led](#six-questions-to-determine-wheter-a-company-is-product-led) The build trap is when organizations become stuck measuring their success by outputs rather than outcomes, when they focus more on shipping and developing features rather thant he actual value those things produce. When companies stop producing real value for the users, they begin to lose market share. You need to look at the entire company, not just at the development team. Are you optimising your organisation to continually produce value? Are you set up to grow and sustain products as a company? ## The value exchange system Companies end up in the build trap when they misunderstand value. Instead of associating value with outcomes, they measure value by the number of things they produce. Customers realises _value_ when their problems are solved. Then and only then do they provide value back to the business. Every feature you build should result in some outcome that is tied back to that business value. **When companies do not understand their customers' or users' problems, they cannot possibility define value for them.** Sometimes companies fall under the catch-up game, trying to fast-follow its competitors. This is what happened with Google+ and Facebook, never differentiating enough, just copying. Companies do also overpromise during their sales process, giving customers whatever they take to get the contract signed. The result is a ton of one-off features that satisfied the needs of only one client,r ather than a strategic choice to build what would scale for many clients. Organisations get stuck in reactive mode, without building with intent. You have to get to know your customers and users, deeply understanding their needs. Companies need to get their employees closer to their customers and users so that they can learn from them. ## Constraints on the value exchange system Your customers and users don't exist in a vacuum, their wants and needs change according to what's around them. The only thing we can do is understand them better to know how to act. Even thought businesses cannot control the customer side, they have full control over their own constraints. > Raise your hand if you went back and iterated on the last thing you shipped > How do you know that what you shipped was successful? If your answer is around deadlines and finishing with bug-free code, you're likely in a company optimised for _outputs_ instead of _outcomes_. **_Outputs_ are easily quantified things that we produce, like the number of products, features, releases, velocity, etc. _Outcomes_ are the things that result when we finally deliver those features ant the customer problems are solved.** For most companies, their entire structure is optimised to increase the output. To be strategic, we should define and measure value and then celebrate them for delivering on outcomes for our business and users. ## Project vs Products vs Services * **Project** Scope out work to be done, create deadlines and milestones, and then have the team get to work. * **Products** Deliver value repeatedly to customers and users, without requiring to build something new every time. * **Services** Unlike products, they use human labor to primarily deliver value to the user. Many companies use a combination of products and services to deliver value. **A _project_ is a discrete scope of work that has a particular aim. Projects are an essential part of product development, but the mentality of thinking only in projects will cause damage.** A feature enhancement is a project, but your work may not be done when you are finished. You need to keep iterating by scoping out new projects to reach the overall outcome to be successful. ## The product-led organisation **_Product-led_ companies understand that the success of their products is the primary driver of growth and value for their company.** Many companies are instead lead by sales, visionaries or technology, which all land you in the build trap. ### Sales-led Companies let their contracts define their product strategy. Like having 30 features that no-one uses. Many small companies start of as sales-led, and that can be okay. This way of working does not scale for long. When you have 50 to 100 customers or more, you need to change your strategy to building features that apply to everyone, without customisation. ### Visionary-led Consider Apple and Steve Jobs. These type of companies can be very powerful, when you have the right visionary. However, is not sustainable. Innovation needs to be baked in to the system so that one person is not the weakness link. ### Technology-led Companies that are driven by the latest and coolest technology. They often suffer from the lak of a market-facing, value-led strategy. Technology is critical to a software company success, but it cannot drive the product strategy. Product strategy connects the business, market, and technology together. You need to be able to lead with a value proposition for your users, or you will not be able to make money. ### Product-led Companies that optimise for their business outcomes. You don't need to hire an entire new team, but changing the mindset is a challenge. **You need to begin focusing on outcomes and to adopt an experimental mindset to eliminate uncertainty that what you are building will reach your goals.** ## What we know and what we don't Product development is full of uncertainty. It's important to separate out the facts from the things that we need to learn. When kicking off a project, it's best to begin by identifying what you know to be true about the situation, your _known knowns_. Facts that you gather from data or critical requirements from customers. You need to separate these items out as facts and to label those that you are unsure about as our _known unknowns_. Assumptions that you want to test, turn them into facts, and build to satisfy those facts. **Although we should all listen to our intuition, you should be cautious because this is often where bias thrives.** The _unknowns unknowns_ are the things that you don't know you don't know. They pop up during research. **Product management is the domain of recognising and investigating the known unknowns and of reducing the universe around the unknown unknowns.** Product managers identify features and products that will solve customer problems while achieving business goals. They optimise the Value Exchange System. **Product managers are the key to becoming product-led.** ### The role of a product manager A Product manager deeply understands both the business and the customer. They are responsible for synthesising multiple pieces of data, and then determining in which direction the team should move. They keep the team focused on the _why_ are we building this product, and what outcome will it produce? The Chief Product Officer ties together the business outocmes to the roadmpap and represents its impact back to the board. ## Bad product manager archetypes Under a _Waterfall_ process, the first step for a product manager is to talk to _internal stakeholders_ and ask them for their input and requests. This encourages product managers to always satisfy their stakeholders. After requirements are detailed out, they are usually handled to the designers to create an attractive-looking interface. After the product managers approve the designers' work, the software engineers can begin coding. Coding typically takes months, and for large projects, it can even take years. Many companies have adopted Agile as it was a silver bullet. Agiles does indeed promote a better way of collaboration and faster method of building software, but it largely ignore how to do effective product management. When you go through the motions without active thinking, you end up with a lot of useless features. **We rarely teach product managers how to think, and even if we do, we don't measure this thinking process.** Some common archetypes of bad product managers ### The mini-CEO Product managers are not the mini-CEOs of a product. Product managers can't change many things a CEO can in an organisation. They don't have the authority over people – because they are not people managers and they need to rely on influencing them. This is a very arrogant product manager archetype who thinks they rule the world. **The job of a product manager is to produce value, not to develop your own ideas.** > Start listening to your team. Involve them. Listen to your customers and focus on their problems instead of your own solutions. Fall in love with those problems. Validate your dideas. Turn to concrete evidence, rather than opinions. Listening to everyone's opinion is important, but it doesn't mean a product manager should implement every suggestion. ### The waiter This is a product manager who is an order taker. They go to their stakeholders, customers, or managers, ask for what they want, and turn those wants into a list of items to be developed. There is no goal. There is no vision. There is no decision making involved. This is usually the role of a _product owner_. So, how these people prioritise? More often than not, the most important person gets their features prioritised. Instead of discovering problems, waiters ask \"What do you want?\" They implement ideas without validating them. **It's not the customer's job to come up with their own solutions. That is the product manager job.** Waiters are reactive thinkers, not strategic thinkers. Pushing back is essential to building a successful product. **Project managers who are put into product management roles often become waiters waving a calendar.** ### The former project manager Product managers are not project managers. Project managers are responsible for the _when_. Product managers are responsible for the _why_, why are we building this? How does it deliver value to our customers? How does it help meet the goals of the business? **Agile methodologies distribute the responsibilities of the project manager across the team. These cross-functional teams have all the key players dedicated to ship a feature, so less coordination is needed across departments. Thus, project management is not needed.** Answering _why_ is very different than answering _when_. It requires a strategic mindset. ## A great product manager The role of a product manager is to work with a team to create the right product that balances business needs with solving user problems. They understand many sides of the company. They need to understand the market and how the business works. They need to truly understand the vision and the goal of the company. They must have deep empathy for the users. **The \"product manager\" title is misleading, as an effective product manager is not really a manager. To be effective, product managers need to recognise team members' strengths and work with them to achieve the common goal. They need to convince their team and the company on the things they are working are the right things to be building. Influencing skills are essential.** Product managers really own the _why_, of what they are building. **They work with the team to develop ideas and jump in once requirements get validated to make sure that the product being created achieves the goals of the customer, user, and business. It's the team, collectively, that really owns the product, the _what_.** Product managers should be at the helm of experimentation. Product managers connect the dots. They must be humble, they recognise they don't know all the answers. **The ultimate goal for a product manager is to reduce the risk by focusing on learning.** ### Tech expert versus market expert Great product managers will get further by taking advantage of the skills and expertise of their team. **Product management is about looking at the entire system and figuring out how it can product revenue for the company.** Product managers need to know just enough to talk with an engineer or a business person. **A product manager must be tech literate, not tech fluent in order to make good trade-off decisions.** Although it's valuable for a product manager to know the market well, this is something they can learn. ### A great product manager Always focus on the problem. If you anchor yourself with the _why_, you will be more likely to build the right thing. ### Start with the _why_ * Why are we making everything digital in the mortgage space? * Why even do this project? * What's the desired result that we hope to achieve here? * What does success look like? * What happens if we make it all digital and nobody applies for mortgages? * How are we mitigating that risk? Too often, product managers dive into creating solutions without thinking through the associated risks. When organisations hand down solutions, they skip setting success metrics and goals. The biggest issue reported by leaders is that product managers won't step up and \"own the product\". Product managers can and should question solutions and push back on things handed down. However, the work required to gather data and prove the solution takes time. There is some confusion between a _product owner_ and a _product manager_. In Scrum literature, a product owner: * Define the product backlog and create actionable user stories for the development teams. * Groom and prioritise the work in the backlog. * Accept the completed user stories to make sure the work fulfills the criteria. **Product owner is a _role_ you play on a Scrum team. Product manager is a _career_.** Product management and Scrum can work well together, but product management is not dependent on Scrum. **Most organisations do not give their people the necessary time to do product vision and research work. They would rather hold them responsible for a steady stream of outputs and measure success based on stacking backlogs and writing stories.** Product managers play some key roles, but one of the most important ones is being able to marry the business goals with the customer goals to achieve value. Good product managers figure out how to achieve goals for the business by creating or optimising products, all with a view toward solving actual customer problems. ### One role, many responsibilities Without a Scrum team or with a smaller team, you might be doing more strategy and validation work for a product that has not been defined yet. With a Scrum team, you might be more focused on the execution of solutions. As a manager of product managers, you might be leading strategy for a larger part of the product and coaching your teams to discover and execute well. **With a good strategy framework in place and ruthless prioritisation around a few key goals, one person can effectively talk to customers, understand their problems, and help to define the solutions with the team.** ### The product manager career path _Tactical work_ for a product manager focuses on the shorter-term actions of building features and getting out of the door, scoping out work and crunching the data to determine what to do next. _Strategic work_ is about positioning the product and the company to win in the market and achieve goals, it looks like the future state of the product and company. _Operational work_ is about tying the strategy back to the tactical work. Here is were Product Managers create a roadmap that connects the current and the future state of the product. #### Associate product manager Open up this role to people making the switch into product management. Pair them with a senior product manager to teach them the ropes. #### Product manager Works with a development team and UX designers to ideate and build the right solutions for the customers, talking to users, synthesising the data, making the decisions from a feature perspective. Product managers are usually responsible for a feature or a set of features part of a larger product. The product manager needs to be strategic enough to help craft the vision of the features and how they fit into the overall product but tactical enough to ensure a smooth execution of the solution. They tend to skew more operational than strategic at this level due to their shorter-term impact the delivery of the roadmap. The danger is when a product manager is 100% operational, focusing only on shipping products and not on optimising the feature. When they optimise for the day-to-day execution of the team they usually fall behind in the necessary strategy and vision for the features to succeed. **It's imperative to push back as much project management effort as possible to the team and trust them to deliver.** Product managers are part of a larger product team, feeding data about the success of features to product people. This helps inform the strategy and direction of the product portfolio and organisation. #### Senior product manager Same as product managers but they also oversee more scope or a more complex product. It is as high in the product management field as you can go as an individual contributor. They want to focus on building products instead of growing a team. You must balance being highly strategic and highly operational. This role is for people who like difficult product problems. They are usually entrepreneurial, and that's a great trait because they will be the ones to start new product lines for businesses. #### Director of product At a certain point, the company will grow enough that there are too many people reporting into the head of product. A director of product becomes necessary to help promote strategic alignment and operational efficiency. They oversee a group of product managers who are aligned around a product in a portfolio or a product line. They ar responsible for the strategic roadmap of the product, making sure all product managers are aligned by the appropriate goals and working in the most important items to move the product forward. #### VP of product This is someone who oversees the strategy and operations for an entire product line. They set the vision and goals for the overall product. In large enterprises they are also directly responsible for financial success of their product line, not just the delivery of product features. A VP of product is usually the highest level in smaller companies because there is only one product and not multiple product lines. A successful VP of product needs to fundamentally be more of a strategic person and they know that in order to scale their organisation, they need to hire in people who take over the tactical and operational components. #### Chief product officer A CPO oversees a company's entire product portfolio and they ensure it works together to achieve the company goals. Although a VP of product needs to understand how their product roadmap affects the economics of the company, a CPO needs to do that across all products. They work with VPs of product to ensure that every product is strategically aligned to the company's objectives and each product has what it needs, from a resource and people perspective, to reach the established goals. A CPO needs to be able to interface at the board level. A successful CPO needs to be able to translate their actions into terms the board will understand. > They inspire confidence, empathise, and are relentless and resilient To inspire confidence, **CPOs work across many functions to gain buy-in and alignment, they get through things through influence versus direct authority**. By empathising with the other members of their peer group, their customers and their teams, CPOs can find a way forward that aligns all the goals. Finally, a CPO must be relentless and resilient. They need to desire to dig in and find out what is working and what is not, assessing and analysing, trying to prove their hypothesis right and wrong, and holding themselves accountable to data. Having a strong product leader in the C-Suite is a critical step to becoming product-led. ### Organising your teams Companies tend to organise in three main ways: value streams, features, and technical components. Teams organised around features usually do so in order to get ownership over every part of the product, although this is good if you are starting from scratch, it promotes a very output-oriented mindset. We tend to look for ways to develop more things related to our little slice of the product. When features are stable, we should monitor them and then move on to more important work needed to support our strategy. When companies are small, you can organise effectively around goals you are trying to reach. This is the case for TransferWise. One team is focused on retention, another on implementing new currencies, and another on acquiring customers. Each of these teams has ownership of their goal and is judged for success based on their outcomes. It takes a huge amount of coordination across the product teams, so everyone is responsible for collaborating intensely with one another. This structure creates a nice redundancy across the company, so important information about a single product is not stuck in the head of one person. As companies scale, this may not be a viable option. A _value stream_ is all activities needed to deliver value to the customer. That includes process from discovering the problem, setting goals, and conceiving of the idea, to delivering the actual product or service. Every organisation should strive to optimise this flow in order to get value out the door faster to customers, and in order to do so, it makes sense to organise your teams around the value stream. First you begin with the customer or user, whomever is consuming your product at the end of the day. What is the value that you are providing to them? Then work backward. FInd the touchpoints they have with your company to receive that value. How do you organise to optimise and streamline that journey for them? How do you optimise to provide more value, faster? Many companies are confused by the word _product_. If your app, interface, or feature is not inherently adding value on its own, it's just a piece of the entire product. You have to look beyond just that piece to understand how to manage for value delivery and creation. For example, car insurance provides peace of mind in case you get into an accident, that's value. An iPhone app that allows you to manage your car insurance is only a piece of that product's value stream; the app on its own is not enough value. You can still have a product manager owning that iPhone app experience, but you must make sure that they are part of a larger division that holds the true value, the car insurance division. This structure makes it possible to set strategy at the division level. with the product manager able to execute on product initiatives that tie to their product. Keeping the strategy and the value execution together is key. By minimising the number of layers and by giving product managers more scope over their product areas, you can effectively create a product organisation with a structure that supports the product strategy. #### Marquetly's product team > The company had 20 product teams organised around components. \"How should we build this organisation?\" \"We need to restructure around value streams\". We needed to start by hiring an experienced chief product officer. We have a great VP of product, good at the tactical and strategic work for a single product vision, but unfortunately she doesn't understand how to manage a portfolio of products. We also need more senior people. > > You can't build an organisational structure without a product vision, because the value streams are not apparent. > > To make considerable impact, you need to have everyone going in the same direction, working toward the same goals. ## Strategy A good strategy is not a plan; it's a framework that helps you make decisions. Product strategy connects the vision and economic outcomes of the company back to product portfolio, individual product initiatives, and solution options for the teams. Strategy creation is the process of determining the direction of the company and developing the framework in which people make decisions. Netflix's vision in 2005 was \"to provide movies and TV shows in the most convenient and easy way for customers\", they didn't see DVDs as the end point. They knew that if they truly wanted to become the most convenient vehicle by which people would watch movies, it had to figure out a way to get entertainment into the hands of its users faster. Because they started early on, their streaming technology wasn't fast enough to download movies, but once the internet got faster they company expected to see more people downloading videos. But it didn't happen. Netflix realised that only internet-enabled devices at the time were laptops and home computers, and that they weren't the most convenient and delightful way to watch movies. Netflix decided to build its own internet-connected device plugged into TVs for years but a few days before lunch, Reed Hastings shut it down. Hastings realised if they launched their own devide, they couldn't partner with anyone else. He would be in the business of hardware, not software or entertainment, and that wasn't part of Netflix core vision; it didn't align with the overall strategy. Netflix is an example of a company that got out of the build trap, they focused the entire company around a solid vision \"becoming the best global entertainment distribution service, licensing entertainment content around the world, creating markets that are accesible to film makers, and helping content creators around the world to find a global audience\". This vision states not only why the company exists but also the plan for getting there. Netflix self-organised around key outcomes and strategies to help reach its goals. Gibson Biddle, who was VP of product at Netflix aligned his team around a common guideline for evaluating its product strategy \"delight customers, in margin-enhancing, hard-to-copy ways\". Key strategies | Tactics | Metrics -----------------|------------------------------------------------------|-------- Personalised | Ratings Wizard, Netflix Prize | % of customers who rate >= 50 titles at 6 weeks; RMSE Instant | Hub expansion, streaming | % of disks delivered in one day; % of customers who watch >= 15 min/month Margin-enhancing | Previously viewed, advertising, price & plan testing | Gross margin, LTV Easy | Simplify and kill; progressive disclosure | % of customers with >= titles in queue on day one **The powerful thing about a strategic framework is that it forces you to think about the whole before zooming in on the details.** When a company thinks only about the feature-level model, it loses track of the outcomes those features should produce. ### What is strategy? You can't figure out the right product until you know what problem are you solving. Often people don't want a strategy, they want a plan. A good strategy isn't a detailed plan. It's a framework that helps you make decisions. Many companies spend months in \"strategic planning\" for the following year, creating comprehensive and detailed outlines of the tasks they will accomplish, the cost of those actions, and the revenue they will generate. **Thinking of strategy as a plan is what gets us into the build trap.** Stephan Bungay, one of the most respected leaders in strategy deployment and creation, in his book \"The Art of Action\" he writes: > Strategy is a deployable decision-making framework, enabling action to achieve desired outcomes, constrained by current capabilities, coherently aligned to the existing context. ### Strategic gaps While studying strategy in many organisations, Stephen Bungay discovered that when companies approach strategy as a plan, they often fail. #### The knowledge gap The Knowledge Gap is the difference between what management would like to know and what the company actually knows. Instead of seeking more detailed information, upper management should be limiting its direction to defining and communicating the _strategic intent_, or the goals of the business. **The strategic intent communicates where the company is heading and what it desires to achieve when it gets there, it points the team toward the outcomes the business wants to achieve. The company needs to provide room for experimentation and to understand the _why_ before it could suggest the _how_ to solve the problem.** > We know there is a problem, the next step is to discover that problem, tackle it with a solution, and then try to optimise the solution so we can increase acquisition. #### The alignment gap **This is the difference between what people do and what management wants them to do, which is to achieve business goals. Organisations try to fill this gap by providing more detailed instruction; whereas, instead, they should allow each level within the company to define how it will achieve the intent of the next level up.** > At one company, I walked around asking all of the product managers on the hundred or so teams why they were working on their current project. I then asked their leaders the same question. I got two different answers. They could not connect the activities of the teams back to the outcomes of the companies because leadership had passed down feature requests rather than expected outcomes and goals. As soon as those feature requests were committed, it was nearly impossible to change them because the company expected them to be delivered. > **When teams realise customers don't want their solution, they should have the freedom to explore alternative solutions. This is how a product-led organisation should operate.** Product teams need the freedom to explore solutions and to adjust their actions according to the data they receive. Management should feel comfortable granting the necessary autonomy to capable teams. **Instead of sending down mandates, organisations should, instead, turn to aligning every level of the company around the _why_ and should give the next layer down the opportunity to figure out the _how_ and report back.** When leadership is not aligned at the top, the issues trickle all the way down to teams. #### The effects gap This is the difference between what we expect our actions to achieve and what actually happens. When organisations do not see the results they want, they try to fill the gap by putting more controls in place. However, that is the worst thing you can do in this scenario. Giving individuals and teams the freedom to adjust their actions so that they are in line with their goals is what truly allows them to achieve results. We should strive to align teams with a framework of goals and direction and then stepping back to give that team the room to explore. Prescribing fully though-out solutions restricts product teams to only those parameters instead of being able to focus on learning and adjusting their decisions as they go. You need to view strategy in a different way. You should enable action to achieve results. #### Autonomous teams At Marketly, Product Managers often said > I keep having leaders tell me to own the vision of my product, but I'm not allowed. My manager keeps handling me solutions. Every time I try to suggest something different, I'm shut down. In contrast to what leaders said > Our product managers won't step up and own the product. I have to keep prescribing things for them, but it's because they don't take initiative. When teams are not aligned with a clear direction and goals, they cannot effectively make decisions. If they dare to try, much of the time, the leader steps in and says \"No, that's not right\". **Autonomy is what allows organisations to scale. The alternative is hiring hundreds of thousands of middle managers that lead by authority, telling people what to do. Which is not only inefficient but also causes unnecessary layers in management and a lot of frustration.** Leading by authority is a relic of industrial-age methodologies, when low-skilled workers were supervised closely so that their output was maximised. **In the world of software, we don't work this way in.** When you have that sort of talent, you need to give people room to make decisions so that you can get the full benefit of their knowledge and skill. That's what a strategic framework promotes. ### Creating a good strategic framework Marketly CEO originally thought the issues were with the development teams \"they aren't going fast enough, they are slacking off\", and although the company had OKRs, they were very output-oriented instead of outcome-oriented \"Ship the first version of the new teacher platform\" or \"Delivery by June 2018\". Key results weren't tied to any outcome, either business or user-oriented. Everyone would emerge with a list of features to build and then dole them out to the product managers. Product managers then were responsible for estimating. After reporting estimates back, they would then plan the budget to organise the roadmap. Goals were set on the leadership level, as well. Every part of the company was measuring something and yet, the company was not meeting its goals for the past years. The issues detected were a few. The leadership team was prioritising the work itself, based on what it thought it was right to build rather than on feedback by customers. It was reacting to the customers that screamed the loudest instead of evaluating whether those requests matched the strategic objectives. The morale was low. **A good company strategy should be made up of two parts: the operational framework, or how to keep the day-to-day activities of a company moving; and the strategic framework, or how the company realises the vision through product and service deployment in the market.** Trying budgeting, strategy, and product development to an artificial yearly time cycle only creates a lack of focus and follow-through. Think of the major pieces of work you do that are actually _bets_. Spotify operates using something called DIBBs (Data, Insights, Beliefs, and Bets). The concept of thinking of initiatives as bets is powerful because it sets up a different type of expectation. Spotify sets up an environment in which it's safe to try new things and fail. #### Strategy deployment Strategies are interconnecting stories told through the organisation. > Agile teams are really good at telling two-to-four-week stories. As you go up in the organisation, you tell stories with longer timespans. Executives are really goodat telling five-year stories, but a team cannot act on a five-year story when they're used to thinking in two-to-four weeks. – Jade Bloom While executives might be looking at a five-year strategy, middle management is thinking in smaller strategies – yearly or quarterly – bounding teams in a direction that allows them to make decisions on a monthly to weekly basis. When teams are not sufficiently constrained, they become stuck > They feel like they cannot make a decision because there are too many options – Jade Bloom Not having the right level of direction lands us in the build trap. OKRs is a type of strategy deployment used by Google. Hoshin Kanri is a strategy deployment method used by Toyota. Even the military uses strategy deployment with mission command. Setting the direction for each level of an organisation so they can act. In most product organisations there should be four major levels in strategy deployment: * Vision * Strategic intent * Product initiatives * Options #### Strategy creation Is the process of figuring out which direction the company should act. This takes time and focus to craft and maintain. You need to be identifying problems and determining how to organise around solving them at every level. If you are in the C-Suite this should be your top priority. You must first understand the vision, or where you want to go. Then we can identify problems or obstacles standing in our way. In Toyota, the continuous improvement framework is called the Improvement Kata. The Kata teaches people in the company how to strategically tackle problems to reach goals (explained in Mike Rother's book Toyota Kata). Through this act of exploring and identifying problems, you uncover data that is needed to help inform the strategy and vision. Vision is not set solely top-down by management. The entire organisation should be sharing information as they learn about. Bloom calls this information Physics. > The teams should be out there, analysing, testing, and learning and then communicating what they discover back to their peers and their management teams. This is how we set strategy. **This process of communicating data and direction up and down – and across – the organisation is how we maintain alignment.** ### Company-level vision and strategic intents #### Company vision The company vision sets the direction and provides meaning for everything that follows. Amazon is an example of a company with a great vision and strategy: > To be Earth's most customer-centric company where customers can find and discover anything they might want to buy online, and endeavors to offer its customers the lowest possible prices If you are a single-product company, your company's vision is very similar or even the same as your product vision. By keeping an eye on the overall vision, people can make effective decisions about the things that they should and shouldn't pursue. The strategy needs to start at the corporate level, moving through the business lines, and ultimately arriving at the products. At these types of companies, products are just details on how the company vision is manifested. They are the vehicles for value. **A good _mission_ explains why the company exists. A _vision_ explains where the company is going based on that purpose.** The best thing a company can do is to combine both the mission and vision in one statement to provide the value proposition of the company – what the company does, why it does it, and how it wins doing that. Examples of compelling vision statements: > At Bank of America, we are guided by a common purpose to help financial lives better by connecting clients and communities to the resources they need to be successful – Bank of America > Becoming the best global entertainment distribution service, licensing entertainment content around the world, creating markets that are accessible to film-makers, and helping content creators around the world to find a global audience – Netflix **They are short, memorable, and clearly articulated. They don't include fluffy terminology.** You do need to focus your company around where you want to concentrate. **It's okay to want to be the best on the market or the market leader, but you need to give some context on how.** Leaders need to spend time communicating their vision, you must tell a story. The difficult part is connecting the vision back to the company's operations. This is where company leaders must specify _strategic intents_. #### Strategic intents This is how you intent to reach that vision changes as your company matures and develops. **Strategic intents communicate the company's current areas of focus that help realise the vision.** When determining the intents, the C-Suite of the company should ask **\"what is the most important thing we can do to reach our vision, based on where we are now?\"** These should not be laundry list of desires or goals, **they just need to be a few key things that need to happen to make a big leap forward.** Instead of dictating these solutions down to the teams, leadership should focus on creating strategic intents. **Getting the right level and number of intents is very important. Too many higher-level goals, and you are back to peanut buttering.** One intent is usually good for a small company, and three are plenty for a large organisation. Strategic intents are about the whole company, not just the product solution. ### Product vision and portfolio **Product initiatives translate the business goals into the problems that we will solve with our product.** The product initiatives answer _how_? A product initiative could be something like the following: > As a Netflix subscriber, I want to be able to watch Netflix anywhere, with anyone, comfortably. There are many solutions (_options_) to solve this problem, and they have to be aligned to this product initiative. **Options are your bets, sometimes the solution will be readily apparent, but other times you need to experiment to find the solution.** **Product initiatives set the direction for the product teams to explore options.** #### Product vision Companies often have trouble aligning around a _product vision_. Often they end up with too many products and no coherent vision. Building one-off products to satisfy individual customer requests, failing to address a wider audience. Too many people, too little direction, and no holistic approach. Often revealing a bigger issue, the lack of an overall vision. Although delivering multiple features and delivering value is a good thing, **we need something to tie it all together at the top. The product vision communicates why you are building something and what the value proposition is for the customer.** The product vision emerges from experimentation, you need to be careful not to make the product vision too specific. **It cannot describe every little feature but should include more of the main capabilities it enables for the user**. An example for a vision at Marketly: > We help marketing professionals to advance their skills by allowing them to understand their current competencies, easily find the most relevant classes to get to the next level, and then learn the skills they need in the most engaging and digestible fashion, from world-class teaches in the marketing space. The VP of Product should make sure everyone is aligned with this holistic vision. #### Product portfolio Companies with more than one product wrap their products under what is called a _product portfolio_. The CPO is responsible for setting the direction and overseeing the product portfolio. The CPO answers these questions for their team: * How do all of our products work as a system to provide value to our customers? * What unique value does each of the product lines offer that makes this a compelling system? * What overall values and guidelines should we consider when deciding on new product solutions? * What should we stop doing or building because it does not serve this vision? Leaders often complain that they don't have time to innovate. Usually, this is due to poor capacity planning and strategy creation. **It's not that you don't have time to innovate; it's that you are not _making time_ to innovate. You are going to need to say no to some things.** ### Product management process > #### Marketly product initiatives > > We believe that by increasing the amount of content in our site in key areas of interest, we can acquire more individual users and retain existing users, resulting in a potential revenue increase of $2,6555,000 per month from individual users. > > _Options to explore_: > * Easier and faster ways for teachers to create courses > * Feedback loops for teachers on areas of interest for students > * Outreach to new teachers who can create courses in areas of interest **Usually, when we think about processes, we focus more on the act of developing software than we do about building the right software. This is the build trap.** ### The product kata The process by which we uncover the right solutions to build is called the [Product Kata](https://melissaperri.com/blog/2015/07/22/the-product-kata). The Product Kata is highly inspired by the Toyota Kata, which was a Continuous Improvement framework that creates the habit of improving by focusing on learning. It teaches you how to analyse problems and then create small experiments to solve them. Every team member is responsible for improving the company's processes. 1. Understand the **direction**. What challenge are you striving to meet? 2. Grasp the current **condition**. What is the process's current pattern? 3. Establish the next **target condition**. What pattern do you want to have next? 4. **Plan-do-check-act (cycle)** toward the target condition. The step-by-step discovery process between where you are and where you want to be next. Management will set the challenge while teams grasp the current condition and establish target conditions. Next, you follow the Coaching Kata to plan the steps to get to the next target condition. The coach is a team member who asks the following questions during every meeting. The whole team answers and plans the work. The questions are the following: 1. What is the **target condition**? 2. What is the **actual condition**? Reflect on the last step taken. You don't actually know what the result of a step will be! 1. What was your **last step**? 2. What did you **expect**? 3. What **actually happened**? 4. What did you **learn**? 3. What **obstacles** do you think are preventing you from reaching the target condition? Which **one* are you addressing now? 4. What is your **next step**? (next plan-do-check-act cycle / experiment) What do you expect? 5. When can we go and see what we **have learned** from taking that step? Reflect on the last step taken. You don't actually know what the result of a step will be! 1. What was your **last step**? 2. What did you **expect**? 3. What **actually happened**? 4. What did you **learn**? Now the process applied to Product is what is called Product kata and the continuous product improvement 1. Company goal, product KPI, future state. 2. What are the users doing now? (planning) 3. What is the first little goal? 4. User research, product experiments (experimenting) --- The first task is to get to the product initiative and determine what problems you can solve to further the strategic intent. We then need to break the success metrics into something we can measure on a shorter time scale. We call this the _team goal_, and it's how we measure the success of the option. The team goal should be something we can measure after every release. #### Context matters After we have set the goal, we begin walking the Product Kata: 1. What is the goal? 2. Where are we now in relation to that goal? 3. What is the biggest problem or obstacle standing in the way of me reaching that goal? 4. How do I try to solve that problem? 5. What do I expect to happen (hypothesis)? 6. What actually happened, and what did we learn? We answer questions one through four to figure out how to plan our next move as a team. Five and six, determine whether to go back to the beginning for the next round. These questions take us to problem exploration, solution exploration and solution optimisation. When considering whether to experiment around a particular solution > Don't spend your time overdesigning and creating unique, innovative solutions for things that are not core to your value proposition. If someone has already solved that problem with a best practice, learn from that, implement their solutions, gather data to determine if it's successful in your situation, and then iterate. Reserve your time and energy for the things that will make or break your value proposition. – Zappos former head of UZ, Brian Kalma When the problem that you are solving is core to your value proposition, take a step back and don't rush into the first solution. The best thing you can do at the experimentation stage is to kill the bad ideas! The fewer, the better. ### Understanding the direction and setting success metrics #### Product metrics Metrics tell you how healthy is your product, and ultimately, your business. Unfortunately, teams end up measuring **_vanity metrics_. Goals that look shiny and impressive because they always get bigger.** For example, people get excited by the number of users they have on their product, the daily page views, or how many logins their system has. These metrics do not cause you to change your behavior or priorities. You can easily turn a vanity metric into an actionable metric by adding a time component to it. For example, do you have more users this month than the last? Consider the meaning behind the metrics and how they help inform your decisions and understanding. Additionally, product teams often measure output-oriented metrics such as the number of features shipped, story points complete or user stories worked on. Although these are good productivity metrics, these are not product metrics. They cannot tie the results of product development back to the business. There are many product frameworks available to help you think about the appropriate product goals. #### Pirate metrics Pirate metrics were created by Dave McClure, founder of 500 Startups, to talk about the life cycle of users through your product. Thinking of it as a funnel. Users finding your product is _acquisition_; users having a great first experience is _activation_; keeping users returning to your product is called _retention_; users recommending others because they love your product is _referral_; and finally, users paying for your product because they see value in it is _revenue_. Put it all together AARRR – Pirate metrics. Acquisition is that users land on your site and sign up. Activation is when someone takes the first step with your product toward having a great experience. **This path works for consumer products with a _freemium_ attribute.** If you are a B2B product with a sales team, you generate revenue before users have activation. You can swap the order of these to match your product's flow. As this works as a funnel, you can easily calculate the conversion through each step. Understanding how many people are in each phase of the funnel also lets you target those cohorts and figure out how to move them into the next one. Although Pirate Metrics is popular, some people saw the flaw that it did not talk about user satisfaction. Kerry Rodden, a Googler, created the [HEART metrics](https://library.gv.com/how-to-choose-the-right-ux-metrics-for-your-product-5f46359ab5be) for this. #### The HEART framework HEART metrics measure _happiness_, _engagement_, _adoption_, _retention_, and _task success_. Adoption is similar to activation in Pirate Metrics because you are talking about someone using the product for the first time. Retention is the same as in Pirate Metrics. Happiness is a measure of how satisfied the user is with the product. Engagement is a measure of how often users interact with the product. Task success measures how easy it is for a user to accomplish what they were meant to with the product. #### Setting direction with data Retention is a lagging indicator. It will be months before you have solid data to show that people stayed with you. That's why we also need to measure leading indicators such as activation, happiness, and engagement. Leading indicators tell us whether we're on our way to achieving those lagging indicators like retention. In the case of retention, you can qualify what keeps people retained, for example, happiness and usage of the product. One of the first things every company should do is to implement a metrics platform. Amplitude, Pendo.ai, Mixpanel, Intercom, and Google Analytics are all data platforms, and they all enable product managers to make well-informed decisions. You won't be able to set success metrics without investigating the problem. This is why we first need problem exploration. ### Problem exploration #### Understanding the problem Although data analysis is important, it can't tell the entire story. So, **it's essential that we all go talk to actual humans and get to the heart of their problems.** Giff Constable created an entire book called _Talking to Humans_. User research, observations, surveys, and customer feedback are all tools that you can harness to better explore the problem from a user standpoint. User research, in this case, is not to be mistaken for _usability testing_, which involves showing a prototype or website and directing people to complete actions. There, you are learning whether they can use and navigate the solution easily, not whether the solution actually solves the problem. This type of research is called _evaluative_. Problem-based user research is _generative research_, meaning that its purpose is to find the problem you want to solve. Going to the source of the customer's problem and understanding the context around it. Trying to identify the pain point and the root cause of the problem. **It's easy to fall into the trap of solving problems before you find their root causes. We're all prone to problem solve, even if we don't know what the problem is.** It's also easy to become attached to solution ideas. \"Nobody wants to hear their baby is ugly\". The way around this is not to get too attached. Kill the bad ideas before they take up too much time and energy from the teams and before you get hooked on them. Instead, **fall in love with the problem you are solving.** By getting into the mindset of solving problems early, you allow much more time to build the right thing, because you're not wasting time chasing after the wrong things. #### Breaking down barriers and getting creative In many companies, it's difficult, or even impossible, to talk to the customer, usually due to corporate bureaucracy. **In a consumer industry, you can usually reach out to friends of friends who use the product or have the right background. In a B2B environment, you can work with the sales or account managers to have them be your research spies – asking the questions you might need to know during their sales calls or follow-up meetings.** You have to remember that is not the customer's job to solve their own problems. It's your job to ask them the right questions. ### Solution exploration #### Experimenting to learn **Companies often confuse the _building to learn_ and _building to earn_. Experimentation is all about building to learn.** Experiments should not be designed to last for a long time, they are meant to prove whether a hypothesis is true or false, and, in software, we want to do this as quickly as possible. When we use an MVP only to get a feature out quicker, we're usually cutting corners on a great experience in the process. We sacrifice the amount we can learn from. The most important piece of the MVP is the learning, \"the minimum amount of effort to learn\". Experiments are designed to help companies learn faster. We are not creating stable, robust, and scalable products. The Product Kata is great for this, it always asks the question \"What do you need to learn next?\". Concierge, Wizard of Oz, and concept testing are three examples of solution experiments. With any experiment, it is important to think of how you will end it. Setting expectations on experiments with your customers is key to keeping them happy. Explain to them why you are testing, when and how the experiement will end, and what you plan to do next. Communication is key to a successful experimentation process. #### Concierge Concierge experiments deliver the end result to your client manually, but they do not look like the final solution at all. Your customers will understand that you're doing it manually and that it's not automated. Concierge experiments are particularly interesting for B2B companies because this is how many of these companies got started, by taking on the work for the customers and then later automating it. These experiments don't scale, given that it's labor-intensive. You should conduct these experiments with just enough users so that you can stay in regular contact with them, get plenty of feedback. #### Wizard of Oz This is a method you can use for reaching a broader audience for feedback. It looks and feels like a real, finished product. Customers don't know that, on the backend, it's all manual. This is a great technique when you are looking for feedback at scale. Companies are tempted to leave Wizard of Oz experiments up for a long time because they look real to the customers. This is not wise because it is still manual on the backend. Wizard of Oz can be combined with techniques such as A/B testing. Although, you wouldn't want to use A/B testing in two instances: if you were very unsure about your solution direction or if you did not have enough traffic on those pages to have significance. #### Concept testing It focuses more on high-touch interaction with the customer. You try to demonstrate or show concepts to the user to gauge their feedback. From landing pages and low-fidelity wireframes to higher-fidelity prototypes or videos of how the service might play out. The idea is to pitch the solution idea in the fastest, lightest way possible to convey the message. In many early-stage companies, concept testing is the way they get early sales or capital. #### When you don't need to experiment robustly The mentioned tools are used for higher amounts of uncertainty and, thus, larger risk in your solution ideas. In a case where the team knows the problem and the solution, it's time to implement it. There is no need for up-front testing. You should still be building to learn instead of rushing into a complete solution, but there are other tools you can harness, such as prototypes. Prototypes are the most popular tool for testing. They don't require any code, and there are many software products out there that can help you link screens together to make the flow feel real. **Prototypes don't make sense when you need to validate the problem.** You'd be spinning your wheels creating shiny designs that look great but don't help you to learn what you need to learn. #### Experimenting in complex industries If we take too long to get feedback, we not only waste money but also waste time. The opportunity cost of building the wrong thing is too high. #### Experimenting on internal products You should do it. If people can't figure out how to use tools, that's on you, not them. Internal tools are often neglected, but they still matter to the company. They need to be treated the same way as any other product. ### Building and optimising your solution #### Evolving the product vision After the direction is set for the product vision, it's important to make sure everyone understands the context and work that needs to be done. Story mapping and North Star documents are two ways to help teams find alignment around the vision. **A North Star document explains the product in a way that can be visualised by the entire team and company. This includes the problem it is solving, the proposed solution, the solution factors that matter for success, and the outcomes the product will result in.** North Stars are great for providing context to a wide audience. They should be evolved over time, as you learn more about your product. This is not an action plan, it does not include how the team will be building the product. That is where _story mapping_ comes in. _Story mapping_ is a technique created by the product management veteran and consultant Jeff Patton, to make sure people understood the work and to prioritise the first release. It helps teams break down their work and align around goals. \"It's purpose is to help the team communicate about their work and what needs to get done to deliver value\". This includes breaking down each desired action from the user standpoint. #### Prioritising work Prioritisation is a top issue for most product managers. There are many frameworks out there that will help you prioritise, like _benefits mapping_, _Kano models_ or Cost of Delay. In the book _The Principles of Product Development Flow_, Don Reinersten talks about the importance of the Cost of Delay. Cost of Delay is a numeric value that describes the impact of time on the outcomes you hope to achieve. It combines urgency and value so that you can measure impact and prioritise what you should be doing first. Consider the trade-offs between the amount of value you can capture with the scope of the release and the time it takes to get it out the door. It's an optimisiation problem. You want to reduce scope enough so that you can capture the maximum value in the right time. If you wait too long because you overscoped the release, you lose the money you could have been making. Worse, a competitor could swoop in and steal your market. On the flip side, you don't want to release something that is terrible and provides minimal benefit to the user in order to get it out early. You should discuss each feature or feature component in terms of urgency and value. If it is high urgency, that means that ever moment you do not ship that feature to customers, you are losing out an opportunity to hit your goal. **High value is about solving the strongest problems or desires for the customer.**. Learn more, head to [Black Swan Farming](https://blackswanfarming.com/). #### The real definition of done When teams create their Definition of Done, it's usually around finishing building features required to ship a product. It sets the wrong expectations about what a finished feature is. **We are done developing or iterating on a feature only when it has reached its goals. We need to measure the outcomes.** Teams should set the success criteria before the launch while measuring and iterating until they reach it. Being able to talk to customers, oriented to outcomes, and with the required space provided by the leadership team to figure out how to achieve those outcomes. These are the marks of a product-led company. Culture, policies, and structure are the things that really set a company apart to thrive in product management. ## The product-led organisation > The product-led organisation is characterised by a culture that understands and organises around outcomes over outputs, including a company cadence that revolves around evaluating its strategy in accordance to meeting outcomes. In product-led organisations, people are rewarded for learning and achieving goals. Management encourages product teams to get close to their customers, and product management is seen as a critical function that furthers the business. Kodak made good strides in trying to innovate, but its organisation prevented it from doing so. The company was reactive than strategic, waiting too long to respond to a threat. **When people work in silos, innovation often happens in a separated place without the proper resources to fully execute on what is being discovered.** The fate faced by Kodak can be avoided by adopting a product-led mindset. You can be making an effort to understand customers and to conduct good research, but without the organisation to sustain it, the efforts are too little, too late. You need to become a product-led organisation, both in mentality and in practice. ### Outcome-focused communication Companies stuck in the build trap are usually not patient enough to see outcomes emerge, so they instead measure progress by the number of features shipped. Leaders will say that they want to achieve results, but at the end of the day, they still measure success by features shipped. People want to feel like they are accomplishing things. Checking off the boxes of finished work feels good, but we need to remember that this is not the only measure of success. It's important to have a cadence of communciation that shows progress at every level of the organisation, tailored to each specific audience. #### Cadences and communication **Visibility in organisations is absolutely key. The more leaders can understand where teams are, the more they will step back and let the team execute.** If you keep things transparent, you will have more freedom to become autonomous. Companies tend to fall into bad habits because they have not figured out how to consistently communicate progress in the form of outcomes. When leaders do not see progress toward goals, they quickly resort to their old ways. There are a few core meetings to evaluate progress: * **Quarterly business review**: The senior leadership team should discuss progress toward the strategic intents and outcomes of a financial nature. Reviewing revenue for the quarter, churn of customers, and costs associated with the development of operations. The CPO and their VPs of product are responsible for communicating how the outcomes of product initiatives have furthered strategic intents. **This is not the place to prioritise new product initiatives or to go into detail on them. That is what the product initiative review is for.** * **Product initiative review**: This is a quarterly meeting dedicated for the product development side of the house – CPO, CTO, design leaders, VPs of Product and product managers. Here you review the progress of the options against the product initiatives and adjust our strategy accordingly. This is the place for product managers to talk about the results of preliminary experimentation. New product initiatives can be introduced in this meeting for feedback and buy-in. * **Release reviews**: These provide the opportunity for teams to show off the hard work they have done and to talk about success metrics. These should happen monthly, before features go out, to showcase what is in the pipeline to be released. You should be communicating only what we know is going to ship, not the experiments or research being conducted. Executives can join to see what is being shipped out to customers. This can also be a place to communicate the roadmaps internally so other departments and the executive team are aware. #### Roadmaps and sales teams Instead of thinking of roadmaps as a Gantt chart, you should view them as an explanation of strategy and the current stage of your product. To do this, the product roadmap should be updated constantly (_Living Roadmaps_). You should design your communication to match your audience. A great book on how to set a roadmap is the book _Product Roadmaps Relaunched_. Roadmaps consist of a few key parts: * The theme * Hypothesis * Goals and success metrics * Stage development * Any important milestones You should align your company around certain terminology to describe stages of development. We can use these four phases: * Experiment: At this stage we want to understand the problem and to determine whether it's worth solving. Teams are conducting problem exploration and solution exploration activities. No production code is being created. * Alpha: Stage to find out if the proposed solution is desirable to the customers. Production code gets built and is live for a small set of users. These users understand that they are getting early access to a feature that might change or be killed. * Beta: Stage to find out if the solution is scalable, from a technical standpoint. * Generally Available (GA): The solution is widely available to all of our clients. Sales teams can talk openly about GA products. Poorly-constructed roadmaps are the source of much tension between product and sales. Although communicating status can be scary, given the variable nature of software development, it's also necessary. **Product management enables the sales strategy.** Sales still needs something to sell. Creating working agreements and roadmaps that can be communicated to customers is key to developing a good relationship between product and sales. #### Product operations As product teams scale to more than a few teams, keeping track of progress, goals, and processes becomes a challenge. Teams need to focus on growing their product, and operations work becomes too much of a distraction. At Marketly they ended up implementing a product operations team, run by a chief of staff who reported to the CPO. This was a very small team (two people) to help streamline operations and reporting. This allowed product people to focus on what they were good at, while product operations helped them to make ifnormed decisions, by surfacing up those reports. This team was responsible for: * Collecting data on progress toward goals and outcomes across teams. * Report on goals, outcomes, roadmaps, progress, capacity, and costs. * Set up and maintain a product analytics platform to report on product engagement metrics. * Standardise product progresses that go across teams, such as strategy cadences, experimentation tracking and feedback, documentaiton on product features, collecting data, setting goals, creating and maintaining roadmaps, and sales enablement. * Organise and run critical product meetings for strategy creation, strategy deployment and releases. * Conduct any coaching or training for the product teams. The point of this team is not to dictate how the members of a team work together to build the product, but instead to create the citeria for inputs and outputs of the work. They are not dictating whether a team can talk to users. They are creating systems that help teams figure out which users to target for their experimentation. The product operations team should be made up a combination of project managers and product people. \"Success for you would be automating away your team\". This is not a team that is meant to be large. It's an efficiency engine dedicated to automating, streamlining, and optimising. ### Rewards and incentives To often Product Managers say \"It doesn't matter what the goal is. We just have to deliver this feature\". They are being forced into the build trap by company policy. Shipping product instead of learning or solving problems for customers is what gets you into the build trap. People become afraid to try anything new. The best advice is to push back. If you don't have the seniority, you can still try to change the minds of the people who can bring those messages up the chain. Talk to your bosses about what success really means. Define your metrics for when you know you will be done. Always come with data. Most sales teams are held accountable to selling, signing the contracts and bringing in the revenue. Many teams overpromise in order to make their commission numbers. Imagine being in company where the Sales team oversold so much the development team has to run two years behind. Customers wold get angry and this would lead to high churn. Sales teams would target the wrong customers in order to make numbers. These customers would leave quickly. We want to incentivise sales teams to keep selling, but adjusting the components of their salary so that their livelihood does not depend too mucho n the commision percentages can help to mitigate this risk. Trying retention numbers to their success metrics. ### Safety and learning There may not be enough safety in the organisation to fail and learn. **Product managers need a certain amount of trust from the organisation to have room to explore different options.** Teams are going to have tro try some perceivaly wild stuff. If they are not allowed to explore these weird paths, they will never push for the status quo. Remember, **it is not a success if you fail and o not learn.** Learning should be at the core of every product-led organisation. It should be what drives us. It is also better to fail in smaller ways earlier, rather than spending all the time and money failing a publickly large way. Many companies talk about how they want their people to be innovative and how they want to create crazy new products, but there has to be an understanding that it's safe to fail in order to get innovation. The irony is that experimentation is the ultimate risk-management strategy because when you experiment early, you can preent failure. Taking 10 years to fail, slowly burning through cash and never getting anyware is more problematic than allowing for smaller failures along the way. If you adopt a great product mindset and you give people the freedom to fail, what you're doing is allowing them to fail quickly, quietly, and at a lower cost because they're testing things early. That's the type of failure you want to encourage. That's the type of failure from which we can recover. Leaders who give people the room to do that see the best results and avoid the build trap. It's also the leader's job to give people boundaries within which to operate. There are many different ways to create boundaries. You can segment your user base into populations for Alpha and Beta testing, start with small representative population, learn from them and then expand. Allows you to contain the rollback if the product doesn't work. ### Budgeting At some corporations if you don't deliver what's on the roadmap you don't get as much funding next year. That means that if a team finds a way to build a product cheaper, or finds that the product shouldn't be built at all, the team will build it anyway because they will be penalised if they don't spend all their money. It's far wiser to look at funding product development like a venture capitalist (VC). **\"This is where we are. These are our next goals. We need this much money to get to those goals.\"** ### Customer centricity In addition to a culture that rewards and promotes learning, you need a culture that focuses on the customer. To put yourself into your customer's shoes ask \"what would make my customers happy and move our business forward?\" Product management is about value exchange. **Being customer-centric allows you to figure out what products and services will fulfill that value on the customer side.** Let your teams talk to and see your customers in action. **Being customer-centric means that you know the most important thing you can do to create great products is to deeply understand your customers.** This is also the core of what it means to be product-led. When companies try being customer-led, one of the biggest mistakes they make in the transition is having leadership think that it's everyone else's job to change instead of theirs. ## Escaping the build trap to become product-led > I needed to learn about humility. **I learned that my role was not that of the big idea generator but that the bad idea terminator.** > > As I moved into more senior roles, I learned that having a good strategic framework could make or break a company. People will get in the way of a good product every time. If it doesn't meet the agendas of senior stakeholders, good ideas can be squashed. To mitigate that risk, **you need to deeploy understand what motivates people and to know how you can address their personal motivations by introducing information and data that wins them over.** One of the quickest ways to kil the spirit of a great employee is to put them in an environment where they can't succeed. Even good product managers become tired of waking up and going to war every day. **The fundamental criterion for building a product is that the product solves a problem for the user.** ## Six questions to determine wheter a company is product-led * Who came up with the last feature or product idea you built? * What was the last product you decided to kill? * When was the last time you talked with your customers? * What is your goal? * What are you currently working on? * What are your product managers like? ",
    "url": "/learning-notes/books/escaping-the-build-trap/",
    "relUrl": "/books/escaping-the-build-trap/"
  },"14": {
    "doc": "How to Win Friends and Influence People",
    "title": "How to Win Friends and Influence People",
    "content": "# [How to Win Friends and Influence People](https://www.goodreads.com/book/show/4865.How_to_Win_Friends_and_Influence_People) - [Preamble](#preamble) - [Ways to make people like you](#ways-to-make-people-like-you) - [Win people to your way of thinking](#win-people-to-your-way-of-thinking) - [Be a leader](#be-a-leader) ## Preamble Don’t criticise, condemn or complain. There is nothing else that so kills the ambitions of a person as criticisms from superiors. > I never criticise anyone. I believe in giving a person incentive to work. So I am anxious to praise but loath to find fault. Give honest and sincere appreciation. ## Ways to make people like you 1. Become genuinely interested in other people. 2. Smile. 3. Remember that a person’s name is to that person the sweetest and most important sound in any language. 4. Be a good listener. Encourage others to talk about themselves. 5. Talk in terms of the other person’s interests. 6. Make the other person feel important – and do it sincerely. ## Win people to your way of thinking 1. The only way to get the best of an argument is to avoid it. 2. Show respect for the other person’s opinions. Never say, ‘You’re wrong.’ 3. If you are wrong, admit it quickly and emphatically. 4. Begin in a friendly way. 5. Get the other person saying ‘yes, yes’ immediately. _([Socratic method](http://www.wikihow.com/Argue-Using-the-Socratic-Method))_ 6. Let the other person do a great deal of the talking. 7. Let the other person feel that the idea is his or hers. 8. Try honestly to see things from the other person’s point of view. 9. Be sympathetic with the other person’s ideas and desires. 10. Appeal to the nobler motives. 11. Dramatise your ideas. _(Showmanship)_ 12. Throw down a challenge. > The one major factor that motivated people was the work itself. If the work was exciting and interesting, the worker looked forward to doing it ## Be a leader A leader’s job often includes changing your people’s attitudes and behaviour. Some suggestions to accomplish this: 1. Begin with praise and honest appreciation. 2. Call attention to people’s mistakes indirectly. 3. Talk about your own mistakes before criticising the other person. 4. Ask questions instead of giving direct orders. 5. Let the other person save face. 6. Praise the slightest improvement and praise every improvement. Be ‘hearty in your approbation and lavish in your praise.’ 7. Give the other person a fine reputation to live up to. > When she came to Tommy, she looked him straight in the eyes and said, ‘Tommy, I understand you are a natural leader. I’m going to depend on you to help me make this class the best class in the fourth grade this year.’ She reinforced this over the first few days by complimenting Tommy on everything he did and commenting on how this showed what a good student he was. With that reputation to live up to, even a nine-year-old couldn’t let her down – and he didn’t.” 8. Use encouragement. Make the fault seem easy to correct. 9. Make the other person happy about doing the thing you suggest. ",
    "url": "/learning-notes/books/how-to-win-friends-and-influence-people/",
    "relUrl": "/books/how-to-win-friends-and-influence-people/"
  },"15": {
    "doc": "Kanban: Successful Evolutionary Change for Your Technology Business",
    "title": "Kanban: Successful Evolutionary Change for Your Technology Business",
    "content": "# [Kanban: Successful Evolutionary Change for Your Technology Business](https://www.goodreads.com/book/show/8086552-Kanban) - [Solving an agile manager's dilemma](#solving-an-agile-managers-dilemma) - [Kanban](#kanban) - [Recipe for success](#recipe-for-success) - [From the worst to the best in five quarters](#from-the-worst-to-the-best-in-five-quarters) - [A continuous improvement culture](#a-continuous-improvement-culture) - [Mapping the value stream](#mapping-the-value-stream) - [Coordination with Kanban systems](#coordination-with-kanban-systems) - [Establishing a delivery cadence](#establishing-a-delivery-cadence) - [Establishing an input cadence](#establishing-an-input-cadence) - [Setting work-in-progress limits](#setting-work-in-progress-limits) - [Establishing service level agreements](#establishing-service-level-agreements) - [Metrics and management](#metrics-and-management) - [Operations review](#operations-review) - [Starting a Kanban change initiative](#starting-a-kanban-change-initiative) - [Bottlenecks and non-instant availability](#bottlenecks-and-non-instant-availability) - [An economic model for Lean](#an-economic-model-for-lean) - [Sources of variability](#sources-of-variability) - [Issue management and escalation policies](#issue-management-and-escalation-policies) ## Solving an agile manager's dilemma > Our deadlines were set by managers without regard to engineering complexity, risk, or project size. > Changes suggested out of context would be rejected by the workers who lived and understood the project context. Drum-Buffer-Rope is an example of a class of solutions known as pull systems. The side effect of pull systems is that they limit work-in-progress (WIP) to some agreed-upon quantity, thus preventing workers from becoming overloaded. ## Kanban _Kan-ban_ is a Japanese word that literally means \"signal card\" in english. Cards are used as a signal to tell an upstream step in the process to produce more. Kanban is fundamental to the _kaizen_ (continuous improvement). Workflow visualisation, work item types, cadence, classes of service, specific management reporting, and operation reviews is what defines the Kanban Method. ### Kanban system A number of cards equivalent to the agreed capacity of a system are placed in circulation. One card attaches to one piece of work. Each card acts as a signalling mechanism. A new piece of work can be started only when a card is available. The free card is attached to a piece of work and follows it as it flows through the system. When there are no more free cards, no additional work can be started. Any new work must wait in a queue until a card becomes available. When some work is completed, its card is detached and recycled. With a card now free, a new piece of work in the queuing can be started. This is known as a pull system because new work is pulled into the system when there is capacity to handle it, rather being pushed into the system based on demand. A pull system cannot be overloaded if capacity, as determined by the number of signal cards in circulation, has been set appropriately. ### Kanban in software development In software development the cards do not actually function as signals to pull more work. They represent work items. Card walls have become a popular visual control mechanism. These are not inherently Kanban systems, they are merely visual control systems. **If there is no explicit limit to work-in-progress and no signalling to pull new work through the system, it is not a Kanban system.** ### Why use Kanban We use it in order to limit team's work-in-progress to a set capacity and to balance the demand on the team. By providing visibility onto quality and process problems, it makes obvious the impact of defects, bottlenecks, variability, and economic costs on flow and throughput. Kanban facilitates emergence of a highly collaborative, high-trust, highly empowered, continuously improving organisation. ### Kanban and Lean Kanban uses five core properties to create an emergent set of Lean behaviours in organisations: * Visualise workflow * Limit work-in-progress * Measure and manage flow * Make process policies explicit * Use models to recognise improvement opportunities (like Theory of Constraints, Systems Thinking, _muda_) --- Kanban is not a software development lifecycle methodology or an approach to project management. It requires that some process is already in place so that Kanban can be applied to incrementally change the underlying process. Your situation is unique and you deserve to develop a unique process definition tailored and optimised to your domain. ## Recipe for success Asking people to change their behaviour creates fear and lowers self-esteem, as it communicates that existing skills are clearly no longer valued. The recipe for success presents guidelines for a new manager adopting a new existing team. * **Focus on quality**: Excessive defects are the biggest waste in software development. - Both agile and traditional approaches to quality have merit - Code inspections improve quality - Collaborative analysis and design improve quality - The use of design patterns improves quality - The use of modern development tools improves quality - Reducing the quality of design-in-progress boosts software quality * Reduce work-in-progress * Deliver often * Balance demand against throughput * Prioritise * Attack sources of variability to improve predictability Working down the list, there is gradually less control and more collaboration required with other downstream and upstream groups. The final step it's the one that separates the truly great technical leaders from the merely competent managers. ### WIP, lead time, and defects Longer lead times (from starting to done) seem to be associated with significantly poorer quality. In fact, an approximately 6.5x increase in average lead time resulted in greater than 30x in initial defects. Longer average lead times result from greater amounts of work-in-progress. Hence, the management leverage point for improving quality is to reduce the quantity of work-in-progress. **Shorter iterations will drive higher quality.** ### Frequent releases build trust Reducing WIP shortens lead time and frequent releases build trust with external teams (social capital). Trust is event driven and that small, **frequent gestures or events enhance trust more than larger gestures made only occasionally.** Small gestures often cost nothing but build more trust than large, expensive gestures bestowed occasionally. ### Tacit knowledge Information discovery in software development is tacit in nature and is created during collaborative working sessions, face-to-face. Our minds have a limited capacity to store all this tacit knowledge. Knowledge depreciates with time, so shorter lead times are essential. ### Balance demand against throughput By setting the rate at which we accept new requirements, we are effectively fixing our work-in-progress to a given size. As work is delivered, we will pull new work from the people creating demand. ### Create slack Slack capacity created by the act of limiting work-in-progress enables improvement. **You need slack to enable continuous improvement.** You need to balance demand against throughput and limit the quantity of work-in-progress to enable slack. In order to have slack, you must have an unbalanced value stream with a bottleneck resource. Optimising for utilisation is not desirable. ### Prioritise At this point, management attention can turn to optimising the value delivered rather than merely the quantity of code delivered. **Prioritisation should not be controlled by engineering.** Improving prioritisation requires the product owner, business sponsor, or marketing department. Engineering can seek only to influence how prioritisation is done. ### Building maturity First, learn to build high-quality code. Then reduce the work-in-progress, shorten lead times, and release often. Next, balance demand against throughput, limit work-in-progress, and create slack to free up bandwidth, which enable improvements. Then, with a smoothly functioning and optimising software development capability, improve prioritisation to optimise value delivery. ### Attack sources of variability to improve predictability Reducing variability in software development requires knowledge workers to change the way they work, to learn new techniques and change their personal behaviour. All of this is hard. Variability creates a greater need for slack. ## From the worst to the best in five quarters ### Adjusting policies > So how prioritisation was facilitated? If something was important and valuable, it was selected for the input queue from the backlog; if it wasn't then it wasn't selected. Any item older than six months was purged from the backlog. > The weekly meeting with product owners also disappeared, when a slot became available in the input queue. It would alert the product owners to pick again. ## A continuous improvement culture Continually improving quality, productivity, and customer satisfaction is known as \"kaizen culture\". Workforce is empowered. Individuals feel free to take action; free to do the right thing without fear. Management is tolerant with failure and individuals are free to self-organise around the work they do and how they do it. Everyone looks out for the performance of the team and the business above themselves. A kaizen culture has a high level of social capital, a trusting culture where individuals respect each other. High-trust cultures tend to have flatter structures than lower-trust cultures. It is the degree of empowerment that enables a flatter structure to work effectively. Achieving kaizen culture may enable elimination of wasteful layers of management and reduce coordination costs as a result. **Introducing a radical change is harder than incrementally improving an existing one.** ### Unanticipated effects of introducing Kanban > The physical board had a huge psychological effect. By attending standup each day, team members were exposed to a sort of time-lapse photography of the flow work across the board. ### Sociological change Kanban method appear to achieve a kaizen culture faster and more effectively than typical Agile software development teams. Kanban enables every stakeholder to see the effects of his or her actions or inactions. ## Mapping the value stream Kanban drives change by optimising your existing process. The main change will be quantity of WIP and the interface to add interaction with upstream and downstream parts of your business. So you must work with your team to map the value stream as it exists. Map the process they actually use. ### Defining a start and end point for control Define interface points with upstream and downstream partners. ### Work item types Identify the types of work that arrive at that point in any others that exist within the workflow that will need to be limited. A few examples: * Requirement * Feature * User story * Use case * Change request * Production defect * Maintenance * Refactoring * Bug * Improvement suggestion * Blocking issue ### Drawing a card wall Once you have understood your workflow by sketching or modelling it, start to define a card wall by drawing columns on the board that represent the activities performed, in the order hey are performed. Where to put buffers: do not try to second-guess the location of bottleneck that will require a buffer. Rather, implement a system and wait for the bottleneck to reveal itself, then make changes to introduce a buffer. ### Demand For each type of work identified, you should make a study of the demand. Decide to allocate capacity within the Kanban system to cope with that demand. > The allocation is 60% change requests, 10% code refactorings, and 30% production text changes ### Anatomy of a work item card The information on the cards must facilitate the pull system and empower individuals to make their own pull decisions. Cards may include: * Electronic tracking number as a link to the electronic card * Title of the item * Date the ticket entered the system. Facilitates first-in, first-out queuing for standard class of service * Name of assigned person. Name tags, stickers or magnets are a good idea to represent members. Basically sufficient information to facilitate project-management decisions without the intervention or direction of a manager. ### Electronic tracking For teams that are distributed geographically, or those who have policies that allow team members to work from their homes, electronic tracking is essential. They allow you to visualise the work item tracking as if it were on a card wall. ### Coping with concurrency Two or more activities can happen concurrently. There are two ways for coping with this. * One is not to model it at all; just leave a single column where both activities can occur together. Using different colours or shapes of ticket to show the different activities. * The other option is to split the board vertically into two (or more) sections ``` ┌─────────────┬────────────────┬────────────────┬────────────┐ │ INPUT QUEUE │ ANALYSIS │ DEV & TEST DEV │ TEST READY │ ├─────────────┼─────────┬──────┼─────────┬──────┼────────────┤ │ │ IN PROG │ DONE │ IN PROG │ DONE │ │ · · · · · · · ``` ### Coping with unordered activities Usually happens in highly innovative and experimental work. First, simply have a single column as a bucket for the activities and do not explicitly track on the board which of them is complete. Second, tickets have to move vertically up and down the column they are pulled into each of the specific activities. When the activity is complete, the checkbox can be filled to visually signal that the item is ready to be pulled to another activity in the same column. If all the checkboxes are filled, the item is ready to be pulled to the next column on the board or it can be moved to \"done\". ┌──────────────────────────┐ │ DEV & TEST DEV │ ├───────────────────┬──────┤ │ IN PROGRESS │ DONE │ ├───────────────────┼──────┤ │ ┌───────────────┐ │ │ │ │ ☒ UI DESIGN │ │ │ │ │ ☒ SECURITY │ │ │ │ │ ☐ PERSISTENCE │ │ │ │ │ ☐ BIZ LOGIC │ │ │ · └───────────────┘ · · ## Coordination with Kanban systems ### Visual control and pull You are likely to want to visually capture the assigned staff member, the start date, the electronic tracking number, the work item type, the class of service, and some status information, such as whether the item is late. Visually communicate enough information to make the system self-organising and self-expediting ad the team level. It should enable team members to pull work without direction from their manager. ### Electronic tracking As an alternative or as a supplement ### Daily standup meetings In Agile a typical standup meeting is for a single team of up of twelve people, usually about six. Involving answering three questions: **What did you achieve yesterday? What will you do today? Are you blocked or do you need assistance?** In Kanban, the wall contains all the information so the focus will be on the flow of work. The facilitator will \"walk the board\". Work backward, from right to left (in direction of the pull), through the tickets on the board. Making emphasis on items that are blocked, delayed due to defects or stuck for a few days. There will also a call for any other blocking issues that are not on the board and for anyone who needs help to speak up. Mature teams do not need to walk through every card. They will tend to focus only on the tickets that are blocked or have defects. ### The after meeting Huddles of small groups of 2 or 3 people. Team members that want to discuss something on their minds. After meetings generate improvement ideas and result in process tailoring and innovation. ### Queue replenishment meetings These serve the purpose of prioritisation in Kanban. They happen at regular intervals providing cadence for queue replenishment. Ideally they will involve several product owners or business people from potentially competing groups. The tension becomes a positive influence. Mature teams evolve towards demand-driven prioritisation where stakeholders can be available on demand. ### Release planning meetings Basically plan downstream delivery. ### Triage Triage is used to classify bugs that will be fixed, and their priority, versus bugs that will not be fixed. The most useful application of triage is the backlog of items waiting to enter the system. The purpose of triaging the backlog is to reduce its size. A good rule of thumb is to trim the backlog for anything more than 3 months of work. There is a relationship between the size of the backlog, the volatility of the domain and the delivery velocity. ### Issue log review and escalation When work items are impeded, an issue work item will be created. The issue will remain open until the impediment is removed and the original work item can progress through the system. Issues that are not progressing should be escalated. ### Sticky buddies Each person not physically present in the office and able to move the sticky note on the card wall, make a peer-to-peer agreement with someone who would be present in the office to act as their delegate. ### Synchronising across geographic locations The key coordination across multiple site is to use an electronic system. It isn't enough to have only a card wall. It will be necessary to keep physical card walls synchronised on a daily basis. It is important to assign someone to take responsibility to this at each location. ## Establishing a delivery cadence Kanban avoids any dysfunction introduced by artificially forcing things into time-boxes. It decouples the time it takes to create a user story from the delivery rate. While some work is complete and ready for delivery, some other work will be in progress. It makes sense to question how often prioritisation (and perhaps planning estimation) should happen. ### Coordination costs of delivery Activities involved in successfully delivering software need to be accounted for, planned, scheduled, resourced and then actually performed. #### Efficiency of delivery To be more efficient, focus on reducing waste by reducing coordination and transaction costs in order to make the batch size efficient. #### Agreeing a delivery cadence You, the team, and the organisation are aware of the costs of making a delivery and are capable of making some form of ration assessment about the acceptable frequency of delivery. #### Improve efficiency to increase delivery cadence Choose conservatively initially. Let the organisation prove that it can achieve this level of consistency. After, improve configuration management. Reducing coordination and transaction costs is at the hart of Lean. Deliver more value to your customers more often. #### Making on-demand deliveries Regular delivery helps to build trust. Lack of predictability destroys trust. Choose a regular cadence except in circumstances in which trust already exists, where near-continuous deployment is desirable. Under special circumstances, it makes sense to plan a special, off-cycle release. It should be treated as exceptional. ## Establishing an input cadence ### Agreeing on a prioritisation cadence Weekly is a good schedule for prioritisation cadence. It provides frequent interaction with business owners, builds trust through the interaction involved and enables the players to move the pieces once a week. It's a collaborative experience, there is transparency to the work and to the work flow; progress can be reported every week; everyone gets to feel that they are contributing to something valuable. ### Efficiency of prioritisation Some teams sit together, so there is no need for a meeting, a quick discussion across the desk will be enough. Other teams may have people in multiple zones, so weekly meetings may not be so easily scheduled. As general advice, more frequent prioritisation is desirable. It allows the input queue to be smaller, and, as a result, there is less waste in the system. WIP is lower and therefore a lead time is shorter. ### Transaction costs of prioritisation Activities such as estimation, business plan preparation, and candidate selection from the backlog, are preparatory work for prioritisation. These are the transaction costs of prioritisation. It is desirable to keep these costs low. ### Improve efficiency to increase prioritisation cadence Because of the coordination-cost effect, Agile planning methods are efficient only for small teams focused on single systems and product lines. By choosing to eliminate estimation, transaction costs and coordination costs of prioritisation are reduced. This reduction facilitates frequent prioritisation or on-demand prioritisation. ### On-demand prioritisation Each week product managers would refill the empty slots in the input queue. Choose on-demand prioritisation when you have a relatively high level of organisation maturity, low transaction costs, and low coordination costs. Otherwise, use a regularly scheduled prioritisation meeting. ## Setting work-in-progress limits WIP limits should be agreed upon by consensus with up- and downstream stakeholders and senior management. ### Limits for work tasks One task at a time should be ideal. As items could be blocked and task switching should be allowed, some research suggest that two items in progress per knowledge worker is optimal. You may encounter resistance to the notion that one item per person, pair or small team is the correct choice. #### Limits for queues When work is completed and waiting to be pulled by the next stage it is said to be \"queuing\". #### Buffer bottlenecks The bottleneck in your workflow may require a buffer in front of it. Buffers and queues smooth flow and improve predictability of lead time. Buffers also ensure that people are kept working and provide for a greater utilisation. **Do not sacrifice predictability in order to achieve agility of quality.** #### Input queue size It can be directly determined from the prioritisation cadence and the throughput, or production rate in the system. Queue and buffer sizes should be adjusted empirically as required. #### Unlimited sections of workflow With Drum-Buffer-Rope, all work stations downstream from the bottleneck have unlimited WIP. BOTTLENECK ↓ ☺··☺··☺··☹·☺·☺ └────────┘ DRUM-BUFFER-ROPE ☺·☺····☺·☹·☺···☺ └──────────────┘ CONWIP ☺··☺··☺··☹·☺···☺ └────────┴─────┘ DBR + CONWIP ☺··☺··☺··☹··☺··☺ └──┴──┴──┴──┴──┘ KANBAN With a Kanban system, most or all the stations in the workflow have WIP limits. The local WIP limit with the Kanban system will stop the line quickly keeping the system from becoming overloaded. #### Don't stress your organisation In more mature organisation that suffer few unexpected issues you can be more aggressive with your WIP-limit policies. For more chaotic organisations, you will want to introduce looser limits initially with greater WIP and an intention to reduce it later. #### It's a mistake not to set a WIP limit The tension created by imposing a WIP limit is positive tension. It forces discussion about the organisation's issues and dysfunctions. Without WIP limits, progress and process improvement is slow. #### Capacity allocation ┌───────┬────────────────┬────────────────┬─· ALLOCATION │ INPUT │ ANALYSIS │ DEV & TEST DEV │ TOTAL = 20 │ QUEUE ├─────────┬──────┼─────────┬──────┼─· │ │ IN PROG │ DONE │ IN PROG │ DONE │ ├───────┼─────────┼──────┼─────────┼──────┼─· CHANGE REQ │ │ │ │ │ │ 12 │ │ │ │ │ │ ├───────┼─────────┼──────┼─────────┼──────┼· MAINTENANCE │ │ │ │ │ │ 2 │ │ │ │ │ │ ├───────┼─────────┼──────┼─────────┼──────┼· PROD DEFECT │ │ │ │ │ │ 6 │ │ │ │ │ │ · · · · · · Capacity allocation allows us to guarantee service for each type of work received by the Kanban system. It is important to complete some demand analysis to facilitate reasonable allocation of WIP limits on swim lanes for each type of work. ## Establishing service level agreements Some requests are needed more quickly than others, while some are more valuable than others. **By quickly identifying the class of service for an item, we are spared the need to make a detailed estimate or analysis.** Policies associated with a class of service affect how items are pulled through the system. Class of service determines priority. ### Typical class-of-service definitions Based on business impact, each class of service comes with its own set of policies. You might want to offer up to a maximum of six such classes. Common ones are: * **Expedite** (white): Offers a vendor the ability to say \"Yes!\" in difficult circumstances to meet a customer need. The business makes a choice to realise value on a specific sale at a cost of both delaying other orders and incurring additional carrying costs of higher inventory levels. * **Fixed delivery date** (purple): Requests of this nature carry a significant cost delay whether direct or indirect. There would be a date when a penalty will happen. * **Standard class** (yellow): One common Kanban system design scheme separates work types by size, such a small, medium, and large. A different service level agreement for standard class items of each size can be offered. Peg: small items to be processed in 4 days, medium size items 1 month, and large items 3 months. * **Intangible class** (green): There is no cost of delay within the timeframe that it might take to deliver the item, such as platform replacement. Platform replacement, although it has a low immediate cost of delay, gets displaced by other work with greater and more immediate cost of delay. ### Policies for class of service Different colours of tickets or different swim lanes on the card wall are the most common. Any staff member can use simple prioritisation policies without management intervention. Policies: * **Expedite policies** - Only one expedite request is permitted at any given time. - Other work will be put on hold. - WIP limit may be exceeded in order to accommodate the expedite. * **Fixed delivery date policies** - Receive some analysis an estimate of size, it may be broken up into smaller items. - They are pulled in preference over other, less risky items. - Must adhere to the WIP limit. - If a fixed delivery date item gets behind, it might be promoted to an expedite request. * **Standard class policies** - Prioritised based on an agreed-upon mechanism, such as democratic voting, and typically selected based on their cost of delay or business value. - First in, first out (FIFO) queuing as they are pulled through the system. - No estimation is performed. - May be analysed for order of magnitude in size. - Large items my be broken down into smaller items. Each item may be queued and flowed separately. * **Intangible class** - Prioritised based on an agreed-upon mechanism, such as democratic voting, and typically selected based on their cost of delay or business value. - Members may choose to pull an intangible class item regardless of its entry date. - No estimation is performed. - May be analysed for size. ### Determining a service delivery target Service-level agreements that offer target lead time with due-date performance metric allows us to avoid costly activities such as estimation. To determine the target lead time (from first selection to delivery), it helps to have some historical data. If you don't have any make reasonable guess. > 30% of all requests were late compared to the target lead time. Due date performance = 70% ### Assigning a class of service The class should be assigned when the item is selected into the input queue. 5 + 4 + 3 + 4 + 2 + 2 = 20 TOTAL ├───────┼────────────────┼───────┼────────────────┼───────┼──────┼─────────┼· EXPEDITE │ INPUT │ ANALYSIS │ DEV │ DEVELOPMENT │ BUILD │ TEST │ RELEASE │ █ +1 = 5% │ QUEUE ├─────────┬──────┤ READY ├─────────┬──────┤ READY │ │ READY │ │ │ IN PROG │ DONE │ │ IN PROG │ DONE │ │ │ │ FIXED ├───────┼─────────┼──────┼───────┼─────────┼──────┼───────┼──────┼─────────┼· ▒ 4 = 20% │ ░ │ ░ │ │ ▓ │ ░ │ │ ░ │ ▓* │ ░ │ │ │ │ ░ │ │ │ │ │ │ │ STANDARD │ □ │ ░ │ │ ▒* │ ░ │ │ ▓ │ ▒ │ ▒ │ ░ 10 = 50% │ │ │ │ │ │ │ │ │ │ │ □ │ ░ │ │ ▓ │ █ │ │ │ │ ░ │ INTANGIBLE │ │ │ │ │ │ ▒ │ │ │ │ ▓ 6 = 30% │ □ │ │ │ ░ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ □ SLOT │ □ │ │ │ │ │ │ │ │ │ * BLOCKED · · · · · · · · · · Now that we have allocated capacity to different classes of service, the input queue replenishment activity is complicated. If the costs associated are high, we may choose to leave the slot empty. It may make sense to effectively reserve capacity for a fixed delivery date item. If risks are low, we may choose to fill the slot with a standard class item. ## Metrics and management We are less interested in reporting on whether a project is \"on-time\" or a specific plan is being followed. **What's important is to show that the system is predictable and is operating as designed.** We want to show how well we perform against the class-of-service promises. What's the due-date performance? We want to track the trend over time. ### Tracking WIP The bands on the chart should be smooth and their height should be stable. ### Lead time How predictably our organisation delivers against the promises in the class-of-service definitions. How quickly did we get it from the order into production? ### Due-date performance The item was delivered on time. ### Throughput Number of items that were delivered in a given time period, such as one month. It should be reported as a trend over time. The goal is to continually increase it. You may be able to report the relative size. You may be able to report the value of the work delivered as a dollar amount. Throughput is used as an indicator of how well the system is performing an to demonstrate continuous improvement. ### Issues and blocked work items A cumulative-flow diagram of reported impediments overlaid with a graph of the number of WIP items that have become blocked. This chart can be used on a day-to-day basis alert of impediments and their impact. ### Flow efficiency Track assigned time (to an individual) against time spent blocked and queuing. Show how much potential there is for improvement. ### Initial quality It makes sense to report the number of escaped defects as a percentage against the total WIP and throughput. ### Failure load Items we process because of earlier poor quality. ## Operations review ### Main agenda * Each manager gets 8 minutes for a presentation on their department's performance. * Project specific updates from program-management office. * Team managers get 5 minutes to quickly present their metrics plus a few minutes for questions, comments and suggestions on the metrics presented. - Defect rates - Lead times - Throughput - Value-added efficiency - Maybe some extra metrics to review some aspect of the process ### Keystone of lean transition Ops Review is the keystone of Lean transition and Kanban method implementation. It provides the feedback loop that enables growth of organisation maturity and organisation-level continuous improvement. ### Appropriate cadence Operations review have to be monthly. Quarterly is too seldom to really drive an improvement program, review tends to be superficial. The loss of a feedback loop reduces the opportunities for reflection and adaptation that could lead to improvements. ### Demonstrating the value of managers Operations review trains the workforce to think like managers, to understand when to make interventions and when to stand back and leave the team to self-organise. ### Organisational focus fosters kaizen Organisation-wide ops review fosters institutionalisation of changes, improvements and processes. Teams want to demonstrate how they can help the organisation with better predictability, more thought, shorter lead times, lower costs, and higher quality. ## Starting a Kanban change initiative **The main reason for adopting Kanban is change management. Everything else is secondary.** ### Cultural change rather than a managed-change initiative It is a significant shift away from how a typical Agile transition is planned and managed. There is no planned initiative, no assessments. Ideally, there is no end. Leadership drives a continuous process, encouraging incremental changes. There is a gradual transformation toward a kaizen culture. ### Goals for our Kanban system **Change with minimal resistance must be our first goal.** * **Goal 1: Optimise existing processes**. Existing processes will be optimised through introduction of visualisation and limiting work-in-progress to catalyse changes. * **Goal 2: Deliver with high quality**. Limiting WIP and allowing us to define policies around what is acceptable before a work item can be pulled to the next step in the process. * **Goal 3: Improve lead time predictability**. The amount of WIP is directly related to lead time, so keep WIP small. * **Goal 4: Improve employee satisfaction**. There is a huge impact on performance that comes with a well motivated and experienced workforce. * **Goal 5: Provide slack to enable improvement**. Without slack (space between work items), team members cannot take time to reflect upon how they do their work and how they might do it better. * **Goal 6: Simplify prioritisation**. Maximise business value and minimise risk and cost. In order to respond to change in the market and evolving events, it is necessary to reprioritise. What is needed is a prioritisation scheme that delays commitments as long as possible and that asks a simple question that is easy to answer. Kanban provides this by asking the business owners to refill empty slots in the queue while providing them with a reliable lead-time and due-date performance metric. * **Goal 7: Provide transparency on the system design and operation**. Transparency into the performance of the team, provide customers with confidence. Transparency into the process, let everyone involved see the effects of their actions or inactions. People will be more reasonable. * **Goal 8: Design a process to enable emergence of \"high-maturity\" organisation**. Seek predictability above all else, coupled with business agility and good governance. Success at the senior-executive level depends a lot on trust, and trust requires reliability. ### Steps to get started 1. Map the value stream 2. Define some point where you want to control input. Define what is the upstream of that point and the upstream stakeholders. 2. Define some exit point beyond which you do not intend to control. Define what is downstream of that and who the downstream stakeholders are. 3. Define a set of work item types based on the types of work requests that come from the upstream stakeholders. 4. Analyse the demand for each work item type. Observe the arrival rate and variation. 5. Meet with the upstream and downstream stakeholders. 6. Discuss policies around capacity and get agreement on WIP limit. 7. Discuss and agree on an input-coordination mechanism, such as a regular prioritisation meeting, with the upstream partners. 8. Discuss and agree on a release/delivery-coordination mechanism, such as a regular software release, with downstream partners. 9. You might need to introduce the concept of different classes of service. 10. Agree on a lead-time target for each class of service of work items (SLA). 11. Create a board/card wall to track the value stream. 12. Optionally, create an electronic system to track and report the same. 12. Agree with the team to have a standup meeting every day in front of the board; invite up- and downstream stakeholders but don't mandate their involvement. 13. Agree to have a regular operations review meeting for retrospective analysis of the process; invite up- and downstream stakeholders but don't mandate their involvement. 14. Educate the team on the new board, WIP limits, and the pull system. ### Kanban strikes a different type of bargain Kanban does not seek to make a promise and commit based on something that is uncertain. The typical implementation involves agreement that there will be regular delivery of high-quality working software, with complete transparency, daily visibility of progress and offering frequent opportunities to select the most important new items for development. A commitment around scope, schedule, and budget is indicative of a one-off transaction. It implies that there is no ongoing relationship; it implies a low level of trust. Kanban is based on the notion that the team will stay together and engage in a supplier relationship over a long period of time. ### WIP limits Getting our partners to agree on a WIP limits is a vital element. There will be a day when our partners ask us to take some additional work and we will be able to respond that we have an agreed-upon WIP limit. We would be able to ask \"which one of the current items in progress would you prefer that we drop in order to start your new item?\" ### Prioritisation An agreement to have a regular replenishment meeting and mechanism for how new work will be selected. ### Lead time and classes of service It helps to have some historical data on past performance. The lead-time target you are agreeing to for each class of service should be presented as a target rather than a commitment. If you do need to agree that lead time in the SLA represent a commitment, you should add a margin for safety. A lower level of trust results in a direct economic cost. ## Bottlenecks and non-instant availability A bottleneck in a process flow is anywhere that a backlog of work builds up waiting to be processed. ### Capacity-constrained resources #### Elevation actions A natural reaction is to hire another person to help, adding capacity so that bottleneck is removed. However, **elevating a capacity-constrained resource ought to be the last resort**. Hiring more people slows a project down. #### Exploitation/protection actions Rather than jump immediately to elevation, is better to first find ways to fully exploit the capacity of the bottleneck resource. On a highway analogy, speed does not really affect throughput. It's the gap between vehicles that is most important. Variability in the system has a huge impact on throughput. Fortunately, in our office, our capacity-constrained resources are affected by variability that we _can_ do something about. We can seek to keep the individual busy working on value-added work by minimising the non-value-added (wasteful) activities required of that person. If you find yourself saying \"invest\", you are generally talking about an elevation action. Adding resources is not the only way to elevate capacity. **Automation is a good an natural strategy for elevation.** It does also reduce the variability: repeatable tasks and activities are repeated with digital accuracy. A strong organisational capability at issue identification, escalation, and resolution is essential for effective exploitation of capacity-constrained bottlenecks. If there are several issues blocking current work, they should get the highest priority. Kanban will help to raise awareness. One more technique is to ensure that the resource is never idle. **The most common way to avoid such idle time is to protect the bottleneck resource with a buffer** of work intended to absorb the variability in the arrival rate of new work queuing. You will get more work done despite the slightly longer lead time and the slightly greater total WIP. #### Subordination actions Something else will need to change in order to improve the exploitation of the bottleneck. **Changes required to improve performance in a bottleneck are usually not made at the bottleneck.** ### Non-instant availability resources These look and feel like bottlenecks but they are not bottlenecks. They tend to become a problem with shared resources or people who are asked to perform a lot of multi-tasking. When encountering non-instant availability problems, the ultimate is to turn them into an instantly available resources. #### Subordination actions Making policy changes across the value stream to maximise the exploitation of the bottleneck. Automation is usually the route to elevation. Hiring another engineer is not a good choice. ## An economic model for Lean ### Redefining \"waste\" It is probably better to refer to \"wasteful\" activities as costs. COST ↑ │ │░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ │░░░░░░░░░░░░░░░░COORDINATION COSTS░░░░░░░░░░░░░░░░ │░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ │▒▒▒▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒▒▒▒ │▒▒▒▒▒▒▒▒▒▒▒▒▒ VALUE-ADDED ▒▒▒▒▒▒▒▒▒▒▒▒▒ │▒▒▒▒▒▒▒▒▒▒▒▒▒ WORK ▒▒▒▒▒▒▒▒▒▒▒▒▒ │▒▒▒▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒▒▒▒ │▒TRANSACTION▒ ████▒TRANSACTION▒ │▒▒▒▒COSTS▒▒▒▒ ████████▒▒▒▒COSTS▒▒▒▒ │▒▒▒▒▒▒▒▒▒▒▒▒▒ ████████████▒▒▒▒▒▒▒▒▒▒▒▒▒ │▒▒▒▒▒▒▒▒▒▒▒▒▒ █████FAILURE████▒▒▒▒▒▒▒▒▒▒▒▒▒ │▒▒▒▒▒▒▒▒▒▒▒▒▒ ████████████LOAD████▒▒▒▒▒▒▒▒▒▒▒▒▒ │▒▒▒▒▒▒▒▒▒▒▒▒▒████████████████████████▒▒▒▒▒▒▒▒▒▒▒▒▒ └────────────────────────────────────────────────────→ TIME ### Transaction costs In economic terms, setup and cleanups activities are referred to as transaction costs. Every value-added activity has associated costs. Customer may not see, most likely does not value them. The customer may be forced to pay the costs of these activities but would prefer not to. Most project will have cleanup costs such us delivery to the customer. Iterations, too, have transaction costs. ### Coordination costs Coordination is necessary as soon as two or more people try to achieve a common goal together. These are any activities that involve communicating and scheduling. Any form of meeting is a coordination activity. If team members meet in order to create value-adding information, such as design, a test, a piece of analysis, a section of code, then that meeting is not a coordination cost, it is a value-added activity. If team members meet in order to discuss status, or task assignment, or scheduling that helps coordination, that meeting is a coordination cost and should be regarded as wasteful. You should seek to reduce or eliminate coordination meetings. Kanban provides coordination information that enables self-organisation and reduces coordination costs on a project. The more information can be made transparent to the knowledge workers on the team, the more self-organisation will be possible and the fewer coordination activities will be required. ### How do you know if an activity is a cost? You can ask yourself, \"if this activity is truly value-adding, would we do more of it?\" Planning is clearly not value-added. The customer would not pay for more planning if he could avoid doing so. Consider about minimising the time and energy spent or make the activity more effective. What is important is that you have identified an activity as non-value adding, and that you want to reduce or eliminate that activity as part of a program of continuous improvement. ### Failure load Demand generated by the customer that might have been avoided through higher quality delivered earlier. It adds value that should have been there already. Reducing failure load reduces opportunity cost of delay. ## Sources of variability ### Internal sources of variability Kanban is a change-management technique that requires making alterations to an existing process. ### Work item size One dimension of variability on itemising work for development is the size of work items. The average user story is 1.2 days of effort. ### Work item type mix By breaking our work by specific type, it is possible to treat different types differently and to provide greater predicability. Extreme Programming created different definitions for different sizes of stories: Epic, Story and Grain of Sand. By using techniques to identify different work item types, we can change the mean and spread of variability and improve the predictability in the system for any one type of work. An additional strategy to improve predictability is to allocate total WIP capacity by specific types. **Value predictability more than throughput. Predictability builds and holds trust**. ### Class-of-service mix Allocate a WIP limit to each class of service. This will enable the mean and spread of variability for each class to settle down and the system will be predictable. ### Irregular flow Every single item pulled through Kanban system will be different. Kanban copes with this as long as the WIP limits are enforced. It requires sustainable buffering to absorb the ebbs and surges in flow through the system. Additional buffers may be required, and WIP limits will beed to be larger, when there is more variability in the system. Increasing WIP limit to smooth flow will increase the mean lead time and reduce the range of lead-time variability. Managers, owners, and usually customers value predictability over the random chance of a shorter lead time or greater throughput. ### Rework Doing rework affects variability. The best strategy for reduction of variability due to defects is to relentlessly pursue high quality with very low defect counts. ### External sources of variability They usually come from places that are not directly controlled by the software development process, like elements of the physical world that can't be easily anticipated, predicted, or controller, as suppliers or customers. ### Requirements ambiguity Work items become blocked due to inability to make a decision. This can be addressed by directly influencing the analysis processes used to develop requirements, and by improving the capability and skill level of those defining them. ### Expedite requests Expediting is known to be bad. It affects predictability of other requests. It increases mean lead time and the spread of variability and it reduces throughput. Expediting is undesirable even if it is being done to generate value. One solution is to limit the number of expedite requests. By denying the business the ability to expedite anything they feel like, you force upstream people to explore opportunities early and asses them effectively. ### Irregular flow There are two approaches to dealing with the blocked work items issue: * You can have a larger overall WIP limit. It will ease the flow but at the expense of lead time and possibly quality. The biggest drawback is that there is less tension to provoke discussion and implementation of improvements. * Leave work to be block, raising awareness of the blocking issue. Encouraging those idle team members to think about the root causes and possible process changes that will reduce or eliminate the possibility of recurrence. ### Environment availability The idle time incurred by enforcing a WIP limit has been seen to encourage collaboration on resolving problems. ### Difficulty scheduling coordination activity There is a challenge of coordinating external teams, stakeholders and resources. A frequent reaction is to schedule meetings with a regular cadence. By marking items as blocked and raising visibility on to the source of the blockage, you'll lead some behavioural changes to improve the situation. ## Issue management and escalation policies When an item becomes blocked, the convention is to attach a pink sticky note to the card, indicating the reason for the blockage or a red border in electronic systems. Blocked work items require an organisation to develop a capability for issue management. ### Managing issues Knowing something is blocked does not lead to developing a strong capability for getting it unblocked. It is essential to track the reason. The daily standup meeting should focus on discussing blockages and progress toward the resolution of the issues. Issues should be tracked. ### Escalating issues By taking the time to define escalation paths and write policy around it, the team knows where to send issues for resolution. This saves time. ### Tracking and reporting issues A start date, and end date, an assigned team member, a description of the issue, and links to the blocked customer-valued items are the minimum requirements for an issue tracking system. A history of efforts made at resolution, a history of assigned individuals, an indication of escalation path, an estimated time to resolution, an impact assessment, and suggested root-cause fixes for future prevention may also be useful fields to track. ",
    "url": "/learning-notes/books/kanban/",
    "relUrl": "/books/kanban/"
  },"16": {
    "doc": "Peopleware: Productive Projects and Teams",
    "title": "Peopleware: Productive Projects and Teams",
    "content": "# [Peopleware: Productive Projects and Teams](https://www.goodreads.com/book/show/67825.Peopleware) - [Preface](#preface) - [Managing the human resource](#managing-the-human-resource) - [The office environment](#the-office-environment) - [The right people](#the-right-people) - [Growing productive teams](#growing-productive-teams) - [Fertile soil](#fertile-soil) - [It's supposed to be fun to work here](#its-supposed-to-be-fun-to-work-here) ## Preface The major problems of systems work are not much technological as sociological. If a group of people who have to work together don't trust each other, no nifty software package or gizmo is going to make a difference. ## Managing the human resource Most of us as managers are prone to one particular failing: a tendency to manage people as though they were modular components. ### Failing projects The cause of failure most frequently cited by our survey participants was \"politics\", a.k.a. the project's _sociology_. By noting the true nature of a problem as sociological rather than political, you make it more tractable. > The major problems of our work are not so much technological as sociological in nature. #### The high-tech illusion Our successes stem from good human interactions by all participants in the effort, and our failures stem from poor human interactions. The main reason we tend to focus on the technical rather than the human side of the work is because it's easier to do. ### Make a cheeseburger, sell a cheeseburger The \"make a cheeseburger, sell a cheeseburger\" mentality only serves to damp your people's spirits and focus their attention away from the real problems at hand. To manage thinking workers effectively you need to take measures. #### A quota of errors Making an occasional mistake is natural and healthy. Fostering an atmosphere that doesn't allow for error simply makes people defensive. Encourage people to make some errors. Ask your folks on occasion what dead-ends roads they've been down. #### Management: The bozo definition \"Management is kicking ass\" mentality might work for cheeseburger production, but won't for any effort which people do work with their heads rather than their hands. #### The people store Many managers convince themselves that no one is irreplaceable. That there is no such thing as a key person. The _people_ manager realizes that uniqueness is what makes project chemistry vital and effective. #### A project in steady state is dead The only steady state of a project is rigor mortis. The focus of project management ought to be the _dynamics_ of the development effort. A _catalyst_ is someone who can help a project to jell, is worth two people who just do work. #### We haven't got time to think about this job, only to do it We have to learn to do work less of the time and think about the work more. The more heroic the effort required, the more important is that the team members learn to interact well and enjoy it. ### Vienna waits for you There is a widespread sense that what real-world management is all about is getting people to work harder and longer, largely at the expense of their personal lives. #### Spanish theory management The Spanish Theory is that only a fixed amount of value existed on earth, and therefore the path to accumulation of wealth was to learn to extract it more efficiently from soil or from people's back. The English Theory is that value can be created by ingenuity and technology. The English had the Industrial Revolution, while the Spanish spun their wheels trying to exploit the land and the Indians in the New World. The Spanish Theory of Value is alive among managers. Productivity means achieving more in an hour of work, extracting more for an hour of pay. Attaining new productivity levels through the simple mechanism of unpaid overtime. #### The ain't no such thing as overtime There will be more or less an hour of undertime for every hour of overtime. For the long term it will cancel out. Nobody can work more than forty hours with the level of intensity required for creative intellectual work. Overtime is like sprinting. #### Workaholics Workaholics will put in uncompensated overtime. The realization that one has sacrificed a more important value (family, love, home, youth) for a less important value (work) is devastating. These profiles will eventually realise this and they will be gone. You can't let them do so at the expense of their personal lives. The loss of a good person isn't worth it. #### Productivity: Winning battles and losing wars Productivity has to be defined as benefit divided by cost. The cost has to include the replacement of any workers used up by the effort. #### Reprise In the Spanish Theory managers think that is better to have hopelessly impossible schedule to extract more labor from the workers. > **People under time pressure don't work better, they just work faster** In order to work faster, they may have to sacrifice the quality of the product and their own work experience. ### Quality, if the time permits Man's character is dominated by a small number of basic instincts. When you feel them, there is always passion involved. The slightest challenge to one of these built-in values can be upsetting. We tend to tie our self-esteem strongly to the quality of the product we produce. Not the _quantity_ of product, but the _quality_. **Any step you take that may jeopardize the quality of the product is likely to set the emotions of your staff directly against you.** #### The flight from excellence Workers kept under extreme time pressure will begin to sacrifice quality. They will hate what they're doing. The decision to pressure people into delivering a product that doesn't measure up to their own quality standards is almost a mistake. We have to assume that people who pay for our work are of sound enough mind to make sensible trade-off between quality and cost. Allowing the standard of quality to be set by the buyer, rather than the builder, is what we call _the flight from excellence_. In the long run, market-based quality costs more. > Quality, far beyond that required by the end user, is a means to higher productivity. How is it possible that higher quality coexists with higher productivity? > **The trade-off between price and quality does not exist in japan. Rather, the idea that high quality brings on cost reduction is widely accepted.** #### Quality is free, but... Letting the builder set a satisfying quality standard of his own will result in a productivity gain sufficient to offset the cost of improved quality. > Quality is free, but only to those who are willing to pay heavily to it. Hewlett-Packard has long been an example of quality standards. Quality identification works for increased job satisfaction and some of the lowest turnover figures. The team can insist that delivery wait until its own standards are achieved. ### Parkinson's law revisited > Work expands to fill the time allocated for it Some managers have a strong conviction that the only way to get work done at all is to set an impossibly optimistic delivery date. #### Parkinson's law and Newton's law Parkinson's law is not a law in the same sense that Newton's law is a law. Newton was a scientist. Parkison was a humorist. His \"law\" caught on because it was funny. His law comes after observing a fictitious government bureaucracy. > Parkinson's law almost certainly doesn't apply to your people #### You wouldn't be saying this if you'd ever met our herb **The reasons that some people don't perform are lack of competence, lack of confidence, and lack of affiliation with others on the project and the project goals.** Treating your people as Parkisonian workers doesn't work. I can only demand and demotivate them. #### Some data from the University of New South Wales Some data proving that Parkison's law doesn't apply to most workers: Programers seem to be a bit more productive after they've done the estimate themselves. Programmers work harder to meet the analyst's estimate, bad estimates are always a demotivating factor. The systems analysts tends to be a better estimator than either the programmer or the supervisor. Tight estimates sap the builders' energy. The surprising part of the 1985 Jeffrey-Lawrence study is that no estimates outperformed all others. Projects on which the boss applied no schedule pressure whatsoever (\"Just wake me up when you're done\") had the highest productivity of all. **Schedule pressure needs to be made in much the same way you decide whether or not to punish your child.** #### Variation on a theme by Parkison > Organizational busy work tends to expand to fill the working day Becoming worse every year, is frightening true in many organizations. It's the company that exhibits Parkinsonian behaviour rather than its employees. ### Laetrile Laetrile is a liquid from pressed apricot pits. You can buy the stuff in the grocery store for about the price of almond extract. In Mexico, you can buy it for fifty dollars a drop to \"cure\" your fatal cancer. **People who are desperate enough don't look very hard at the evidence.** Managers are \"desperate enough\", and their desperation makes them easy victims of a kind of technical laetrile that purports to improve productivity. Something to bear in mind is that problems are not usually susceptible to easy solutions because all easy solutions were though of and applied long ago. Easy nonsolutions are often more attractive than hard solutions. False hopes engendered by easy technological nonsolutions are like Sirens. An attractive fallacty that leads nowhere. The seven false hopes of software management: * There is some new trick you've missed that could send productivity soaring. > The line that there is some magical innovation out there that you've missed a pure fear tactic, employed by those with a vested interest in selling it. * Other managers are getting gains of 100% or 200% more! * Technology is moving so swiftly that you're being passed by. > Productivity within the software industry has improved by 3 to 5 percent a year * Changing languages will give you huge gains. > It might give you a 5% gain * Because of the backlog, you need to double productivity immediately. * Isn't about time you automate your software development staff? > The belief that software developers do easily automatable work is a fallacy. Principal work is communication to organize users's expressions of needs into formal procedure. * Your people will work better if you put them under a lot of pressure. > They won't, they'll just enjoy it less. #### This is management **The manager's function is not to make people work, but to make it possible for people to work.** ## The office environment Everybody's workday is plagued with frustration and interruption. ### The furniture police For each of the observed kinds of disturbance, you'd look for an easy, mechanical way to protect your workers. You'd investigate the advantages of closed space and the trade-off of cost against privacy and quiet. Finally you'd take into account people's social needs and provide some areas where a conversation could take place without disturbing others. #### The police mentality People who promulgate rules about leaving each desk clean at night and prohibiting anything to be hung on the partitions except perhaps a company calendar. #### The uniform plastic basement Basement space is really preferable from the point of view of the Furniture Police. But people work better in natural light. It translates into higher quality of work. People want to shape their space to their own convenience and taste. Typical _inconveniences_ that come from dealing with human workers. **For most organizations with productivity problems, there is no more fruitful area for improvement than the workplace.** ### \"You never get anything done around here between 9 and 5\" \"Overtime is a fact of life\" Staying late or arriving early or stating home to work in peace is a damning indictment of the office environment. #### A policy default Management had decided after due reflection that they couldn't really do much about it. This is a policy of total default. #### Productivity nonfactors From studies run in _Coding War Games_ in many organizations, the following factors had little or no correlation to performance: * **Language**, with exceptions on assembly language obviously * **Years of experience**, with the exception of people with less than 6 months of experience. * **Number of defects**, zero-defect workers paid no performance penalty for doing more precise work. In fact, they took slightly less time. * **Salary**, very weak relationship between salary and performance. #### You may want to hide this from your boss What did correlate positively to good performance was this rather unexpected one: _It mattered a lot who your pair mate was_. **Two people from the same organization tend to perform alike.** Best performers are clustering in some organizations while the worst performers are clustering in others. Something about their environment and corporate culture is failing to attract and keep good people or is making it impossible for even good people to work effectively. #### Effects in the workplace People who perform better tend to gravitate toward organizations that provide a better workplace. If you participate in or manage a team of people who need to use their brains during the workday, then the workplace environment is your business. ### Saving money on space A penny saved on the work space is a penny earned on the bottom line. Savings have to be compared to the risk of lost effectiveness. #### Few facts IBM studied the work habits of those who would occupy the space. They concluded: * 100 square feet of dedicated space per worker. * 30 square feet of work surface per person. * Noise protection in form of enclosed offices or 6-foot-high partitions. #### Workplace quality and product quality > Workers who reported before the exercise that their workplace was acceptably quiet were one-third more likely to deliver zero-defect work. > Zero-defect workers: 66% reported noise level okay > One-or-more-defect workers: 8% reported noise level okay When a worker complains about noise, he's telling you that he is likely to be defect-prone. #### A discovery of Nobel Prize significance Worker density is inversely proportional to dedicated space per person. Noise is directly proportional to density, so halving the allotment of space per person can be expected to double the noise. #### Hiding out When the office environment is frustrating enough, people look for a place to hide out. #### Gilb's law > Anything you need to quantify can be measured in some way that is superior to not measuring it at all. **Measurement is better than nothing.** An organisation that can't make some assessment of its own productivity rate just hasn't tried hard enough. #### Measuring with your eyes closed Measurement schemes tend to become threatening and burdensome. Management has to perceptive and secure enough to cut itself out of the loop. **The data on individuals is not passed up to management.** Data collected on the individual's performance has to be used only to benefit that individual. Is an exercise in self-assessment. Individuals are inclined to do exactly the same things with the data that the manager would do. ### Brain time versus body time Work mode | Percent of time --------------------------------|---------------- Working alone | 30% Working with one other person | 50% Working with two or more people | 20% Thirty percent of the time, people are noise sensitive, and the rest of the time, they are noise generators. #### Flow Flow is a condition of deep, nearly mediative involvement. Unfortunately, you can't turn on flow like a switch. It takes a slow descent into the subject, requiring ~15 minutes of immersion period. The state can be broken by an interruption that is focused on you, or by insistent noise. Each time you're interrupted, you require an additional immersion period. During this immersion, you are not really doing work. #### The endless state of no-flow > If the average incoming phone call takes five minutes, and your reimmersion period is fifteen minutes, the total cost of a that call in flow time (work time) los is twenty minutes. Just as important as the loss of effective time is the accompanying frustration. People who work for you need to get into flow. Anything that keeps them from it will reduce their effectiveness and the satisfaction they take in their work. #### The e-factor If your people spend 40% of their total work hours uninterrupted, then the environment is allowing people to get into flow when they need to. > E-factor = Uninterrupted hours/body-present hours. ### The telephone One of the principal causes of interruption is the incoming telephone call. Even if some of the calls were important, they may not have been worth interrupting your flow. #### Tales from the crypt Steps that can be taken to minimize the negative impact of interruptive calls. The most important of these is to realise how much we have allowed the telephone to dominate our time allocation. #### A modified telephone ethic People who are charged with getting work done must have some peace and quiet to do it in. That means periods of total freedom from interruptions. When they want to work in flow, they have to have some efficient way of ignoring incoming calls. **The big difference between a phone call and an electronic mail message is that the phone call interrupts and the e-mail does not.** Priority at \"receiver's convenience\" is acceptable for the great majority of business communications. The trick isn't in the technology; it is in the changing of habits. #### Incompatible multitasking Mixing flow and highly interruptive activities is a recipe for nothing but frustration. ### Bring back the door In creating a sensible workplace, the most obvious symbol of success is the door. Workers can control noise and interruptibility to suit their changing needs. The most obvious failure system is the paging system. #### The show isn't over till the fat lady signs If you believe that the environment is working against you, you've got to start saying so. As people begin to realise that they aren't alone in their feelings, environmental awareness increases. #### The issue of Glitz Depressing surroundings would be counterproductive, but as long as the office wasn't depressing, then you could happily ignore it. The money spent on high-fashion decor is a waste. The next time someone proudly shows you around a newly designed office, think hard about whether it's the functionality of the space that is being touted or its appearance. **Work-conductive office space is not a status symbol. it's a necessity. Either you pay for it by selling out what it costs, or you pay for it in lost of productivity.** #### Creative space You can either treat the symptom or treat the cause. Treating the cause means choosing isolation in the form of noise barriers. Treating the symptom is much cheaper. Pink noise, disruptive noise is drowned out at small expense. You can ignore the problem altogether so that people have to resort to iPods and headphones to protect themselves from the noise. You should expect to incur an invisible penalty: they will be less creative. In the 1960s, researchers at Cornell University conducted a series of tests on the effects of working with music. Many of the everyday tasks are done in the serial processing center of the left brain. Music will not interfere. The creative leap involves right-brain function. If the right brain is busy listening to \"1000 Strings\" on Muzak, the opportunity for creative leap is lost. Since creativity is a sometime thing anyway, we often don't notice when there is less of it. #### Vital space Enclosed offices sooner or later get around to the \"sterility\" of working alone. But enclosed offices need to be one-person offices. #### Breaking the corporate mold A proposal to allow people to reorganise into shared suites will be seen as a threat. Someone in the upper reaches of the organisation will hate the idea. By making everything uniform, the \"owner\" of a territory exercises and demonstrates control. Management should make sure there is enough space, enough quiet and enough ways to ensure privacy so people can create their own sensible work space. Uniformity has no place in this view. ### Taking umbrella steps Christopher Alexander in 1979 with \"The Timeless Way of Building\" book set out to codify the elements of good architectural design. #### Alexander's concept of organic order The master plan is an attempt to impose totalitarian order. A single and therefore uniform vision governs the whole. Alexander proposes a facility can grow in an evolutionary fashion. > Organic order emerges, every place is unique and the different places also cooperate, with no parts left over. #### Patterns > You feel more comfortable if there is a wall behind you. There should be no blank wall closer than eight feet in front of you. You should be sufficiently enclosed to cut noises. **Design sensible work space for people who make their living by thinking.** ##### Tailored work space from a kit Individual modules give poor-quality space to the person working alone and no space at all to the team. Each team needs identifiable public and semiprivate space. Each individual needs protected private space. ╔═════════════════════════════════════╗ ║ GEORGE ┌─┐ ┌─┐ CONFERENCE ║ ║ ┌──┐ ┌┴─┴──┴─┴┐ TABLE ║ ║ ┌┤ │ │ │ STEVE ║ ║ └┤ │ └┬─┬──┬─┬┘ ┌──┐ ║ ║ └──┘ └─┘ └─┘ ┌─┴──┴─┐ ║ ║ └──────┘ ║ ║ READING ║ ARIA ╔════════════╝ ║ AREA ║ ┌──┐ ║ ║ ┌─┐ ┌─┐ ║ ┌─┴──┴─┐ ║ ║ └─┘ └─┘ ║ └──────┘ ║ ╚═══════════╩════════════╝ ##### Windows > Rooms without a view are like prisons for the people who have to stay in them The problem of windowless space is a direct result of a square aspect ratio. A sensible limit for building width is thirty feet. ##### Indoor and outdoor space If you've ever had the opportunity to work in space that had an outdoor component, it's hard to imagine ever again limiting yourself to working entirely indoors. ##### Public space An old pattern for interior space is a smooth \"intimacy gradient\". At the entrance of the workplace should be some area that belongs to the whole group. Further along the intimacy gradient should be space for tightly knit work groups to interact and to socialize. Finally, there is the protected quiet thinking space for one person to work alone. Group interaction space needs tables and seating, writing surfaces, areas to post and space for members to prepare simple meals and eat together. ##### A pattern of patterns Reliance on non-replicable formulas. The space needs to be isomorphic. People at all levels need to leave their mark on the workplace. ##### Return to reality You don't have to solve the space problem for the whole institution. If you can solve it just for your own people, you're way ahead. ## The right people * Get the right people * Make them happy so they don't want to leave. * Turn them loose. ### The Hornblower factor The Hornblower books can be read as an elaborate management analogy. #### Born versus made Managers are supposed to use their leadership skills to bring out untapped qualities in each subordinate. But managers are unlikely to change their people in any meaningful way. People who work for you through whatever period will be more or less the same at the end as they were at the beginning. If they are not right for the job from the start, they never will be. Getting the right people in the first place is all-important. Your skill at these tasks will determine to a large extent your eventual success. #### The uniform plastic person You can't hire based on appearance. **Most hiring mistakes result from too much attention to appearances and not enough to capabilities.** Evolution has planted in each of us a certain uneasiness toward people who differ by very much from the norm. You probably don't feel that you have an uncontrollable tendency to hire attractive or \"normal\" looking people. The perceived norm encourages you to hire people that look like, sound like and thing like everybody else. **The need for uniformity is a sign of insecurity. Strong managers pride is tied only to their staff's accomplishments.** #### Standard dress Uniformity is so important to insecure authoritarian regimes that they even impose dress codes. Accomplishment matters only by people who don' look different. Valuable people begin to realize that they aren't appreciated for their real worth, that their contributions to the work are not as important as their haircuts and neckties. Eventually they leave. If what's gone wrong in your company is promulgation of formal standard appearance, it's too late for a remedy. Get yourself a new job. #### Code word: _professional_ Dress standards might be understandable if you worked in the Customer Relations or in Sales. **Managers with shaky self-confidence are uncomfortable with any kind of behavior that is different from average.** The term _unprofessional_ is often used to characterize surprising and threatening behavior. Anything that upsets the weak manager is almost by definition unprofessional. _Professional_ means unsurprising, a perfect drone. In a healthy environment, professional means knowledgeable and competent. #### Corporate entropy Entropy is the levelness of sameness. The more it increases, the less potential there is to generate energy or do work. > Second thermodynamic law of management > Entropy is always increasing in the organization. The most successful manager is the one who shakes up the local entropy to bring in the right people and let them be themselves, even though they may deviate from the corporate norm. ### Let's talk about leadership #### Leadership as a work-extraction mechanism Using a gun to lead means you have to \"lead\" from behind. In the workplace, is replaced with a delegated authority and position of power. #### Leadership as a service It happens outside the official hierarchy of delegated authority. Leadership is not about extracting anything from us; it's about service. Enables endeavors to go forth. Sometimes directions are set, but their role is as a catalyst, not as a director. In order to lead without positional authority: * Step up to the task * Be fit for the task * Prepare for the task, ahead of time * Maximize value to everyone * Do it all with humor and goodwill It will help to have charisma. #### Leadership and innovation Innovation is all about leadership, and leadership is all about innovation. It takes a bit of a rebel to help even the best innovation achieve its promise: rebel leadership. Nobody knows enough to give permission to the key instigators to do what needs to be done. Leadership as a service almost always operates without official permission. ### Hiring a juggler It would be ludicrous to think of hiring a juggler without first seeing him perform. You need to examine a sample of those products to see the quality of the work the candidate does. #### Aptitude tests Aptitude test may give you people who perform better in the short term, but are less likely to succeed later on. Maybe you should use an aptitude test but hire only those who fail it. You should use them, just not for hiring. Can be a wonderful self-assessment vehicle for your people. #### Holding an audition **We're more dependent on workers' abilities to communicate to each other than their abilities to communicate with machines.** The hiring process needs to focus on at least some sociological and human communication traits. The best way we've discovered to do this is through the use of auditions for job candidates. You ask a candidate to prepare a fifteen-minute presentation on some aspect of past work. The candidate chooses a subject. Hold a debriefing of those present. Each one gets to comment on the person's suitability for the job and whether he or she seems likely to fit well into the team. One caveat, make sure the candidate speaks about something immediately closely related to the work your organization does. ### Childhood's end #### Technology and its opposite One generation's technology is the next generation's environment. #### Turnover: the obvious costs Average employee longevity of 15 to 36 months. The average person leaves after a little more than two years. A reasonable assessment of start-up cost is therefore approximately three lost work-months per new hire. The total cost of replacing each person is the equivalent of four-and-a-half to five months of employee cost or about 20% of the cost of keeping that employee for the full two years on the job. #### The hidden costs of turnover Employee turnover costs about 20% of all manpower expense. In companies with high turnover, people tend toward a destructively short-term viewpoint, because they know they just aren't going to be there very long. Nobody is willing to take the long view. If people only stick around a year or two, the only way to conserve the best people is to promote them quickly. Near beginners being promoted into first-level management positions. This could easily end up with 15% of the staff doing work, and 85% managing. Not only is the structure wastefully top-heavy, it tends to have very lightweight people at the bottom. Products that are developed by workers with an average age in their twenties, and average experience of less than two years. **From the corporate perspective, late promotion is a sign of health.** #### Why people leave * A just-passing-through mentality. * Feeling of disposability. Management think of workers as interchangeable parts. * A sense that loyalty would be ludicrous. Turnover engenders turnover. **The feeling that the company sees nothing extraordinary in the worker makes the worker feel unappreciated as individual.** #### A special pathology: the company move Injecting misery into workers' lives makes the managers feel positively god-like. Usually the real reason for moving the company to another place, is a political deal, or a chance to build a new edifice, or reduction of the boss's commute. The more egocentric the manager, the more intense the fondness for the company move. Corporate move comes down hard on the couple's relationship at a very delicate point. #### A mentality of permanence Companies with low turnover seem to have a preoccupation with being the best. It provides common direction, joint satisfaction and strong binding effect. The sense that you'd be dumb to look for a job elsewhere. **In the best organizations, the short term is not the only thing that matters. What matters more is being the best. And that's a long term concept.** People tend to stay because there is a _widespread sense that you are expected_ to stay. A common feature of companies with lowest turnover is widespread retraining. When people need new skills to make a change, the company provided those skills. No job is a dead end. Retraining helps to build the mentality of permanence that results in low turnover and strong sense of community. ### Human capital An _expense_ is money that gets used up. At the end of the month the money is gone. An _investment_, on the other hand, is use of an asset to purchase another asset. The value has not been used up, but only converted from one form to another. When you treat an expenditure as an investment instead of an expense, you are _capitalizing_ the expenditure. #### How about people? You send that same worker off to a training seminar. His salary and the seminar have been spent on something that is not \"gone\" at the end of the month. Whatever he has leaned persists in his head through the coming months. It's an investment. #### Assessing the investment in human capital How much does your company have invested in you and your colleagues? When someone leaves your company, it will be the investment required to put up to speed the newcomer. Including how much it will cost to recover the past investment in skills and capabilities. #### Playing up to wall street > The object of the exercise is upsizing, not downsizing. Companies that downsize are frankly admitting that their upper management has blown it. What's conveniently forgotten is the investment in the people. ## Growing productive teams **The challenge of the work is important, but not in and of itself; it is important because it gives us something to focus _together_.** The challenge is the instrument for our coming together. Team interactions are everything. ### The whole is greater than the sum of the parts Common definition of success or any identifiable team spirit is a phenomenon we call _jell_. #### Concept of a jelled team A group of people so strongly knit that the whole is greater than the sum of the parts. Once a team begins to jell, the probability of success goes up dramatically. Almost unstoppable, a juggernaut for success. You spend most of your time just getting obstacles out of their way. They've got _momentum_. #### Management by hysterical optimism Believing that workers will automatically accept organizational goals is the sign of naive managerial optimism. #### The guns of Navarone Goals of corporations are always going to seem arbitrary to people. **The goals in sports are always utterly arbitrary. But a lot of people get themselves very involved in the outcome. Their involvement is a function of the social units they belong to.** There is very little true teamwork required in most of our work. But team are still important, for they serve as a device to get everyone pulling in the same direction. > The purpose of a team is not a goal attainment but goal alignment. #### Sign of a jelled team * Low turnover, members aren't going anywhere till the work is done. * Strong sense of identity. Teams may congregate at lunch out at the same watering hole after work. * Sense of eliteness. Team members fell they are part of something unique. * Joint of ownership of the product. Participants are pleased to have their names grouped together on a product or part of one. * Obvious enjoyment. Interactions are easy, confident and warm. #### Teams and cliques The jelled work group may be cocky and self-sufficient, irritating and exclusive, but it does more to serve the manager's real goals than any assemblage of interchangeable parts could ever do. ### Teamicide You can't make teams jell. You can hope they will jell. We should stop talking about _building_ teams, and more of _growing_ them. #### Defensive management You can't protect yourself against your own people's incompetence. If your staff isn't up to the job at hand, you will fail. You should get new people. **Once you've decided to go with a given group, your best tactic is to trust them.** Any defensive measure taken to guarantee success will only make things worse. Let them make some mistakes. That doesn't mean you can't override a decision occasionally or give specific direction to the project. **The right to be right is irrelevant; it's only the right to be wrong that makes you free.** #### Bureaucracy Just telling your people that the goal matters won't be enough if you also have to tell them they should spend a third of their time pushing paper. #### Physical separation With physical separation there is no casual interaction. There is no group space, no immediate and constant reinforcement, no chance of a group culture forming. **Physical separation of people who are expected to interact closely doesn't make much sense.** #### Fragmentation of time > People should be assigned to one and only one project at a time. Fragmentation is bad for team formation, but it's also bad for efficiency. When trying to be part of four working groups, you'll have four times as many interactions to track. You spend all the time changing hears. No one can be part of multiple jelled teams. #### The quality reduced product The typical steps we take to deliver a product in less time result in lower quality. Self-esteem and enjoyment are undermined by the necessity of building a product of clearly lower quality than what people are capable of. An early casualty of quality reduction is team identification. Co-workers who are developing a shoddy product don't even want to look each other in the eye. #### Phony deadlines Tight deadlines can be sometimes demotivating. There are certain cases where a tight but not impossible deadline can constitute an enjoyable challenge. What's never going to help, however, is a _phony_ deadline. If you say the product absolutely has to be out the door by some arbitrary date, they will ask \"why\"? #### Clique control Teams can't be allowed to stay together for one job to another. Organizations take no specific steps to disband teams, but miss every opportunity to keep them together. #### Those damn posters and plaques _Motivational accessories_ are a triumph of form over substance. They seem to extol the importance of Quality, Leadership, Creativity, Teamwork, Loyalty, and a host of other organization virtues. But they do so in such simplistic terms as to send an entirely different message: **Management here believes these virtues can be improved with posters rather than by hard work and managerial talent.** The presence of the posters is a sure sign of the absence of hard work and talent. #### Overtime: an unanticipated side effect Members of good teams are never uniform in any respect, certainly not in their abilities to \"borrow\" time from their personal lives. If a team starts normalizing overtime, people who don't share the pain will become, little by little, estranged from the others. #### Competition **Internal competition has the direct effect of making coaching difficult or impossible.** Coaching cannot take place if people don't feel safe. In a suitably competitive atmosphere, you would be crazy to let anyone see you sitting down to be coached or you coaching someone else that may pass you by. Managerial actions that tend to produce teamicidal side effects: * Annual salary or merit reviews * Management by objectives * Praise of certain workers for extraordinary accomplishment * Awards, prizes, bonuses tied to performance * Performance measurement in almost any form Any action that rewards team members differentially is likely to foster competition. Managers need to take steps to decrease or counteract this effect. ### Competition Internal competition in work team is fostered by managers with lack of time, respect, attention and affection for his or her own people. #### Does it matter? the importance of coaching Team members themselves provide most of the coaching. A well-knit team in action, you'll see a basic hygienic act of peer-coaching that is going on all the time. Team members sit down in pairs to transfer knowledge. Coaching is an important factor in successful team interaction. **What matters is helping all parties understand that the success of the individual is tied irrevocably to the success of the whole.** ### A spaghetti dinner Your chances of jelling into a meaningful team are enhanced by your very first experience together. **Good managers provider frequent easy opportunities for the team to succeed together.** Tiny pilot subprojects, or demonstrations, or simulations, anything that gets the team quickly into the habit of succeeding together. The best success is the one in which there is no evident management. The best boss is the one who can manage this over and over without the team members knowing they've been \"managed\". ### Open Kimono Some managers are pretty good at helping teams to jell. The characteristics of these team-oriented managers are. #### Calling in well Managers of well workers are careful to respect autonomy, once granted. They've prepared to suffer the occasional setback, a direct result of failure by one of their people. **_Open Kimono_ is the exact opposite of defensive management. You take no steps to defend yourself from the people you've put into positions of trust.** A person you can't trust with any autonomy is of no use to you. > The entire staff was assembled as our corporate lawyer handed Jerry the contract and told him to read it and sign on the last page. \"I don't read contracts\" Jerry said, and started to sign. \"Oh, wait a minute,\" said the lawyer, \"let me go over it one more time.\" This was not the time to be defensive, it was time to assume and depend on the competence around him. It is this kind of Open Kimono management that gives teams their best chance to form. #### The getaway ploy The most common means by which bosses defend themselves from their own people is direct oversight. Looking for incompetence about to happen. If you've got decent people under you, there is probably nothing you can do to improve their chances of success more dramatically than to get yourself out their hair occasionally. An easily separable task is perfect opportunity. There is no real management required for such work. Send them away. Find a remote office. **Visual supervision is a joke for development workers. Visual supervision is for prisoners.** You may succeed someday in building a productive office environment. In the short run, use any excuse to get your people out. #### There are rules and we do break them _Skunkworks_ (_insubordination_), management says no and the project goes anyway. People at all levels know whether some sensible insubordination is acceptable or not. #### Who is in charge here? The best bosses take chances on their people. They do this only by exercising their _natural authority_. Between master craftsman and apprentice there is a bond of natural authority. **An insecure need for obedience is the opposite of natural authority.** Each of the workers is known to have some special area of expertise, and is trusted by all as a natural authority in that area. ### Chemistry for team formation _Chemistry_ is the optimal mix of competence and trust and mutual esteem and well-person sociology that provides perfect soil for the growth of jelled teams. #### The cult of quality Still-imperfect product is \"good enough\" is the death knell of a jelling team. **This cult of quality is the strongest catalyst for team formation.** It bind the team together because it sets its members apart from the rest of the world. **Your marketplace, your product consumers, your clients, and your upper management are never going to make the case for high quality. Extraordinary quality doesn't make good short-term economic sense. When team members develop a cult of quality, they always turn out something that's better that what their market is asking for. They can do this, but only when protected from short-term economics. In the long run, this always pays off.** #### Satisfying closure The human creature needs reassurance from time to time that he or she is headed in the right direction. This is _closure_. Team members need to get into the habit of succeeding together and liking it. It builds momentum. The chemistry-building manager takes pains to divide the work into pieces and makes sure that each piece has some substantive demonstration of its own completion. Each new software version is an opportunity for closure. #### Build a sense of eliteness People require a sense of uniqueness to be at peace with themselves, and they need to be at peace with themselves to let the jelling process begin. **The mediocre manager is too insecure to give up the trappings. The great manager knows that people can't be controlled in any meaningful sense anyway.** You give some control, or at least the illusion of control when it jells. **Identity is an essential ingredient of a jelled team**. The team needs to be unique in some sense, not in all senses. #### Preserve and protect successful teams If a team does knit, don't break it up. Give people the option to undertake another project together. #### Provide strategic but not tactic direction Teams are made of peers that function as equals. That's why managers are usually not part of the teams they manage. On the best teams no one is the permanent leader, because that person would then cease to be a peer and the team interaction would begin to break down. **The structure of a team is network, not hierarchy.** #### Allow and encourage heterogeneity A bit of heterogeneity can be an enormous aid to created a jelled team. ## Fertile soil ### The self-healing system #### Deterministic and nondeterministic systems The reason that nondeterministic systems can often heal themselves painlessly is that the humans who make up the system have an easy familiarity with the underlying goals. **Making systems deterministic will result in the loss of its ability to heal itself.** #### The covert meaning of methodology Methodology is the fat book that specifies in detail exactly what steps to take at any time, regardless of who's doing the work, regardless of where or when. People who write Methodology are smart, people who carry it out can be dumb. Methodology makes all the decisions; the people make none. The organization becomes entirely deterministic. Workers can be proceeding operations that could be making no sense to them at all. The difference between Methodology and methodology is that the small _m_ methodology is a basic approach one takes to getting the job done. Big _M_ Methodology is an attempt to centralize thinking. #### Methodology madness Methodologies can do grievous work: * A morass of paperwork. Encouraging people to build documents instead of work. * A paucity of methods. There are a very few competing methods for most of the work we do. * An absence of responsibility. The fault is with the Methodology, not the people making the environment virtually responsibility-free. * A general loss of motivation. Nothing could be more demotivating that management thinking workers are incompetent. #### The issue of malicious compliance Unworkable products and meaningless documentation. #### The baby and the bathwater Methodologies are not the only way to achieve convergence. Better ways: * Training * Tools * Peer review You can't really declare something is standard until it has already become a de facto standard. #### The high-tech illusion revisited The belief that what really matters is the technology. Even with best imaginable Methodology may only give a small improvement in the technology. ### Dancing with risk #### Not running away from risk Project risk is a good thing. Projects that have real value but little or no risk were done ages ago. The ones that matter today are laden with risk. #### The one risk we almost never manage The risk of our own failure. The real reason for risk management is not to make the risk go away, but to enable sensible mitigation should they occur. It's perfectly reasonable not to manage a risk for which the probability of occurrence is extremely low. It's not reasonable to leave unmanaged the risk for which the consequences are \"just to awful to think about\". ### Meetings, monologues and conversations Some organizations are so addicted to meetings that work has to take second place. #### The \"technological enhanced\" meeting Technology provides an escape for people from the pointlessness of what's happening around them. What technology enhances is the dreadfulness of meetings. #### Stand-up meetings They are fine if short, with a clear purpose and focus. #### Basic meeting hygiene Called to get something done might be called _working meeting_. Called to reach a decision. The people that need to agree should be invited. An agenda relevant to its purpose is essential. The meeting is done when a decision has been reached. #### Ceremonies A meeting that is ended by the clock is a _ceremony_. It's all FYI. At any given moment, two people are involved, the other are nominally listening. Conversations are a good thing. What's not such a good thing is all the non-listeners locked in the room while the conversations take place. A ceremony might be called to celebrate some accomplishment, to lay out a strategic change of direction, or to evaluate a project at its end. #### Too many people The cost of a meeting is directly proportional to the number attending. #### Open-space networking Common for professional conferences or congress. The real value of the experience is in the interstices. An Open-Space conference is essentially all coffee break and lunch. The same idea can be useful in meeting planning. #### Prescription for curing a meeting-addicted organization Your goal should be to eliminate most ceremonial meetings and spend time in one-on-one conversation. **Apply the \"What ends this meeting\"?** ### The ultimate management sin is... Wasting people's time. #### Status meetings are about Status Any regular get-together is suspect as likely to have a ceremonial purpose rather than a focused goal of consensus. A status meeting is usually not for serving information to be boss, but for _reassurance_. To state that the boss is boss. Attendance is expected and hierarchy is being respected. #### Early overstaffing Projects begin with planning and design, activities that are best carried out by smallish team. For a two-year project, the bulk of the staff would not come on board until the project is six months to a year underway. PEOPLE │ ■■■■■ │ │ ■■■░░░░░■■│ │ ■■░░░░░░░░░░│ │ ■■░░░░░░░░░░░░│ │ ■■■■■■■■■■■░░░░░░░░░░░░░░│ └──┬─────────────────────────┼──────── TIME BEGIN END PEOPLE │ │ ■■■■■ │ │ ■■■▒▒▒▒▒■■│ │ ■■│▒▒▒▒▒▒▒▒▒│ EFFORT THAT COULD │ ■■░░│▒▒▒▒▒▒▒▒▒│ NOT BE APPLIED │ ■■■■■■■■■■■░░░░│▒▒▒▒▒▒▒▒▒│ └──┬───────────────┼─────────┼──────── TIME BEGIN IMPOSED DEADLINE PEOPLE │ │ ■■■■■■■■■■■■■ │ │ ■▒▒▒▒▒▒▒▒▒▒▒▒■ SAME EFFORT │ │ ■▒▒▒▒▒▒▒▒▒▒▒■■│ APPLIED EARLIER │ │ ■▒▒▒▒▒▒▒▒▒■■░░│ │ │■■■■■■■■■■■░░░░│ └──┼───────────────┼────────────────── TIME BEGIN IMPOSED DEADLINE Early added effort would just be wasted. #### Fragmentation again **Fragmenting any knowledge worker's time over many different tasks assures that he or she will be thrust into two or more different work groups, none of which is likely to jell into a real team.** It is guaranteed to waste the individual's time. The worker will spend significant part of each day switching gears. This time is largely invisible. Fragmentation is particularly injurious when two of the tasks involve qualitatively different kinds of work habits. The mix of a design task with a support task is sure to make progress virtually impossible. **People who suffer from this problem are all too likely to blame themselves.** #### Respecting your investment The human capital invested in your workforce also represents a ton of money. Wasting the time of that huge investment is money poured down the drain. ### E(vil) mail > Yes, your e-mail In-box is full. Very impressive. But what is it full of? #### In days of yore We're coordinating an order of magnitude more than we ever did before. In a relationship over-functions, the others are sure to under-function. When you over-coordinate the people who work for you, they're likely to under-coordinate their own efforts. **A decent coach understands his or her job is not to coordinate interaction, but to help people learn to self-coordinate.** #### What does \"FYI\" even mean? _For your information_, but if you didn't know it, of what value is the information? #### Is this an open organization or a commune? It's nice people allow you to _pull_ information from them about what they're doing, but less nice if they _push_ it on you. #### Repeal passive consent > Silence gives consent. Spending hours each day reading through stuff that is of no value to you, just because you worry about your consent being taken for granted because your name is on the CC line. An effective repeal, is to explicitly establish that only giving consent gives consent. #### Building spam-less self-coordinating organization Start by stating in explicit terms that corporate spam is unwelcome. **Each time you're inclined to send a coordinating e-mail to a colleague think about what steps you have to make to coach that person to self-coordinate.** Telling someone what to do is easy, while instilling self-coordinating abilities in that same person is much more complicated. ### Making change possible > People hate change. While you risk making enemies to those who have mastered the old ways forcing them back to the uncomfortable position of novice, you receive only minor support from those who would gain. **Uncertainty is more compelling than the potential for gain.** #### That's a swell idea, boss. I'll get right on it. Resistance-to-change continuum, resistance in increasing order: 1. Blindly loyal (ask no questions) 2. Believers but questioners a. Skeptics (show me) b. Passive observers (what's in it for me?) c. Opposed (fear of change) d. Opposed (fear of loss of power) 3. Militantly opposed (will undermine and destroy) Blindly loyal: fairly powerless, and they will jump onto whatever appears hot. They will withdraw their support as quickly as they give it. Believers but questioners are the only meaningful potential allies of any change. Blindly loyal and militantly opposed are the real enemies. When we argue logically for change, one tactic is to contrast how the new world will be (good) compared to the current situation (bad). **Never demean our old ways. Instead, _celebrate_ the old as a way to help change happen.** Any improvement involves change. #### A better model of change Naive model of how change happens ┌────────┐ ┌────────┐ │ OLD │ │ NEW │ │ STATUS ├─────────────────────→│ STATUS │ │ QUO │ ↑ │ QUO │ └────────┘ A BETTER IDEA └────────┘ Satir's change model ┌────────┐ ┌───────┐ ┌─────────────┐ ┌────────┐ │ OLD │ │ │ │ PRACTICE │ │ NEW │ │ STATUS ├────────→│ CHAOS ├─────────────→│ AND ├──→│ STATUS │ │ QUO │ ↑ │ │ ↑ │ INTEGRATION │ │ QUO │ └────────┘ FOREIGN └───────┘ TRANSFORMING └─────────────┘ └────────┘ ELEMENT IDEA Change involves at least the four stages. Change happens upon the introduction of a foreign element: a catalyst for change. **When you try to institute change, the first thing you hit is chaos.** Suffering from the dip in the learning curve. It is frustrating and embarrassing to abandon approaches and methods you have long mastered only to become a novice again. The transforming idea is something that people in Chaos can grab as offering hope that end of suffering is near. Structured huddle is sometimes the best medicine. The practice and integration is not yet completely comfortable, but you perceive that the new is now beginning to pay off. You have reached the new status quo when what you changed to becomes what you do. Chaos is an integral part of change. When you're looking for it, your changes of dealing sensibly with it are much improved. #### Safety first Change won't even get started unless people feel safe, not to be demeaned or degrade for proposing change. Temporary loss of mastery is embarrassing enough. Change only has a change of succeeding if failure is also okay. ### Organizational learning Non-learners cannot expect to prosper for very long. #### Experience and learning > Learning is limited by an organization's ability to keep its people. When turnover is high, learning is unlikely to stick or can't take place at all. #### The key question about organizational learning The most natural learning center for most organizations is at the level of middle management. Learning organizations are always characterized by strong middle management. #### The management team Another ingredient required is that middle managers must communicate with each other and learn to work together in effective harmony. #### Danger in white space If middle managers can act together as the redesigners of the organization, then the benefits of learning are likely to be realized. ### The making of community What great managers do best is **making of community**. A need for community is something that is built right into the human firmware. We have strong need for community. Towns no longer satisfy this need. The workplace is where we have our best change of finding a community. #### Digression of corporate politics Community doesn't just happen. It has to be made. **The science of making communities, making them healthy and satisfying for all is called politics.** Refusing to be a political in the Aristotelian sense, is an abnegation of the manager's real responsibility. #### Why it matters Satisfying community tends to keep its people. When people leave they tend to time their departures to minimally inconvenience the community. This means that workers are unlikely to leave during the project. ## It's supposed to be fun to work here Work should be fun. ### Chaos and order Without chaos, we'd be bored to tears. Managers tend to be greedy. The manager's job in this approach is to break it up and parcel it out. The people down below get to have the real fun of putting things shipshape. #### Progress is our most important problem Nostalgic fondness for the days when everything wasn't so awfully mechanical. Progress toward more orderly, controllable methods is an unstoppable trend. The thoughtful manager feel a need to replace some of the lost disorder with a policy of _constructive reintroduction of small amounts of disorder_ with: * Pilot projects * War games * Brainstorming * Provocative training experiences * Trainings, trips, conferences, celebrations, and retreats #### Pilot projects Try some new and unproven technique. The Hawthorne Effect will boost in energy and interest. Projects that try out any modified approach, tend toward higher-than-average net productivity. One caveat about pilot projects: Don't experiment with more than one aspect of development technology on any given project. #### War games It can be an enjoyable experience to try your hand at set of tailored problems, and to be able to compare your performance to statistical performance profile of your peers. Game results won't be used against you, security and confidentiality should be guaranteed. War games help you evaluate your relative strengths and weaknesses and help the organization to observe its global strengths and weaknesses. #### Brainstorming Structured interactive session, specifically targeted on creative insight. People get together to focus on a relevant problem. Strive for quantity of ideas, not quality. Discourage negative comments. #### Training, trips, conferences, celebrations and retreats Combining travel with their peers and one-of-a-kind experience. Better if the travel is to somewhere exotic. When a team is forming, it makes good business sense to fight for travel money to get team members out of the office together. ### Free electrons The mark of the best manager is an ability to single out the few key spirits who have the proper mix of perspective and maturity and then turn them loose. Their own direction is more unerringly in the best interest of the organization than any direction might come down from above. It's time to get out of their way. **Sociology matters more than technology or even money. It's supposed to be productive, satisfying fun to work. If it isn't, then there's nothing else worth concentrating on.** ",
    "url": "/learning-notes/books/peopleware/",
    "relUrl": "/books/peopleware/"
  },"17": {
    "doc": "Personal Kanban: Mapping Work, Navigating Life",
    "title": "Personal Kanban: Mapping Work, Navigating Life",
    "content": "# [Personal Kanban: Mapping Work, Navigating Life](https://www.goodreads.com/book/show/10419045-personal-kanban) - [Introduction](#introduction) - [The basics of Personal Kanban](#the-basics-of-personal-kanban) - [Building your first Personal Kanban](#building-your-first-personal-kanban) - [My time management is in the league with the freeway](#my-time-management-is-in-the-league-with-the-freeway) - [Natural flows](#natural-flows) - [Components of a quality life](#components-of-a-quality-life) - [Ending our priorities](#ending-our-priorities) - [Strive for improvement](#strive-for-improvement) - [Endgame](#endgame) - [Appendix: Personal Kanban design patterns](#appendix-personal-kanban-design-patterns) ## Introduction We cannot make informed decisions or create a quality product without first understanding _why_ are we doing what we are doing. Lack of context creates waste, resulting in long work days, poor planning, and the inability to keep commitments outside the office. Tools should give you control and not take anything. ## The basics of Personal Kanban You need a way to actually _see_ the tasks you're expected to perform so you can do the right work at the right time. Personal Kanban is a visual representation of work that makes the conceptual tangible. It shows what needs to be done, what is complete, what is being delayed, and what is going on at this precise moment. Visualisation tools: * **Lists**: Offer zero context for work, allow no reorganisation or repriorisation, and obscure vital priorities beneath piles of marginally useful work. * **Mind map**: It's an improvement versus lists. It doesn't help to complete tasks or distinguish between work shared by multiple people. Its greatest limitation is that the information conveyed is not obvious. * **Kanban**: It works great for software development. Its focus on team work increases productivity and effectiveness. It shows the flow of work, limit the work-in-progress, and it captures all tasks, not just those directly related to software production. It is flexible enough to relate extremely variable or even chaotic workload. ### Rules 1. **Visualise your work**. It's challenging to understand what we can't see. When we see work in its various contexts, real trade-offs become explicit. We now have a physical record of all those demands on our time. This allow us to make better decisions. 2. **Limit your work-in-progress (WIP)**. We cannot do more than we are capable of doing. This seems obvious, but it's not. Our capacity for work is limited by a host of factors including the amount of time we have, the predictability of the task at hand, our level of experience with the task type, our energy level, the amount of work we currently we have in progress. Limiting our WIP allows us the time to focus, work quickly, react calmly to change, and do a thoughtful job. With Personal Kanban, principles take precedence over process. Process should change with context. When we see the landscape of our work, we identify better courses of action because have clarity. ### Why call it Personal Kanban Personal Kanban tracks items of importance to the individual: tasks, appointments, small projects (_work_). Organisational Kanban tracks items of value to the organisation. The goal is to understand the predictable and repeatable process of creating something. It focuses on standard work products, organisational efficiencies, and waste reduction. People are less predictable than organisations. We want to understand the nature of our work, but not force it. Innovation relies on inspiration through exploration and experimentation. Lean is both a philosophy and a discipline which, at its core, increases access to information to ensure responsible decision making in the service of creating value. _**Kaizen**_ is a state of continuous improvement where people naturally look for ways to improve poorly performing practices. When we visualise our work, we adopt a kaizen mindset; we are weened from the comfort of complacency and actively seek out opportunities for improvement. ### Why it works When we are able to represent each of our tasks, it becomes tangible. Touching, feeling physically interacting with our tasks transforms work-as-a-concept into work-as-a-concrete-experience. Each time we move a sticky note, we receive kinesthetic feedback: the tactile action is both a data point and a reward. A regular succession of these movements creates a cadence, a rhythm of work. > Good leaders provide enough management oversight to give a clear and coherent idea of vision and purpose, but not to the extent that they micromanage. They ensure employees have the information they need to make good decisions, and then step back to let the good decisions happen. ## Building your first Personal Kanban ### Step one: get your stuff ready > I've never drawn the same kanban twice The more you use your Personal Kanban, the more you'll need to tailor it to the situation at hand. ### Step two: establish your value stream Value Stream: The flow of work from beginning to completion The most simple value stream is **READY** (working waiting to be processed), **DOING** (work-in-progress), and **DONE** (completed work). ### Step three: establish your backlog Backlog: Work you have yet to do. When _there is way too much work_, overcoming denial, acknowledging the pain, and accepting it needs addressing are stages necessary in understanding our work. Lay out the backlog sticky notes next to your board. Decide which tasks need to be completed first and pull them into your **READY** column. ### Step four: establish your WIP limit WIP limit: the amount of work you can handle at any given time. The human brain needs closure (Zeigarnik Effect). Unfinished tasks vie for our attention, causing intrusive thoughts that ultimately impede productivity and increase the opportunity for error creating an anxiety feedback loop. Becoming psychologically or physically debilitating. Once you move a sticky note to DONE you know your work is done and you can focus on finishing the next task. Working like this is fulfilling. The close you get to reaching your capacity, the more stress taxes your brain's resources, and impacts your performance. **Research consistently shows we cannot reach our maximum effectiveness while multitasking. Instead, maximum effectiveness results when we limit our WIP and focus on the task before us.** Start by setting an arbitrary WIP limit. Add this number to your DOING column. Expect that number to change. ### Step five: begin to pull Pull: To bring a task into DOING you have capacity for it. Each time you pull a task from READ into DOING you're prioritising based on your current context. Pull no more than your WIP limit. The physical act of moving sticky notes to change their status satisfies our brain's need for closure. We pull work into DOING only when we have room to accommodate it. Pulling is a wilful act. This is different from \"pushing\" where people tell you what to do do and when to do it, regardless of whether you have capacity for it or if it is really the highest priority task at the time. ### Step six: reflect From time to time, reflect on the tasks that have been done: * Which tasks did you do particularly well? * Which tasks made you feel good about yourself? * Which tasks were difficult to complete? * Were the right tasks completed at the right time? * Did the tasks completed provide value? Then ask yourself _Why_? This is what we call a \"retrospective\". ### Kanban boosters Every time your personal work involves another person or something happens outside your control, make sure it is reflected on your board. These are the stages where delay and waste can be injected. When work stagnates, is know as a \"bottleneck\". You want to be able to visualise these points. #### Today The TODAY column is where you pull tasks you expect to accomplish today. We rarely get to tackle or finish everything we set out to do. The TODAY column shows us the difference between what we want to do each day and what we can actually achieve. ## My time management is in the league with the freeway Capacity: How much stuff will fit Throughput: How much stuff will flow A freeway ranges between 0 and 100% capacity; it can be anywhere from totally empty to completely filled with vehicles. But the freeway doesn't optimise for capacity, it optimises for throughput. Capacity is a spatial relationship, while throughput is a flow relationship. Like traffic, work does not fit. Work flows. When we don't acknowledge or respect our work's flow we fall prey to multitasking. We rush through one thing to get next, striving for quantity (productivity) when we know quality (effectiveness) will surely suffer. A 2009 [Stanford University study](http://news.stanford.edu/news/2009/august24/multitask-research-study-082409.html) dispels the myth that multitaskers have mental edge over those who focus on a single task, ultimately deeming multitasking counterproductive. High multitaskers optimise for capacity not for throughput. We want _throughput_. Throughput is a flowbased system. It measures success by the amount of quality work flowing from READY to DONE over time, not just the volume of work. The rate which work moves from READY to DONE is our throughput. > ### In four years I'll send Julie to university > How? > Personally and professionally, we often get stuck in \"analysis paralysis\". We overcomplicate our situation painstakingly planning to the minutest detail, details that in the beginning we have limited insight into. **We allow the planning phase to stall the action phase.** > Carl needs to come up with a few steps he can take immediately. He takes those sticky notes and populates his READY column with them. In his brain immediately understands hat progress is being made towards his goal. He doesn't worry he didn't solve every issue on the first day, he's accepted that we never solve every issue. ### To-do lists: spawns of the devil These are the embodiment of evil. They torment us, controlling what we do, highlighting we haven't. We need context, something to-do lists don't provide. There is no flow from one action to the next, no suspense and ultimately, no reward. **Thoughtful prioritisation and completion beats rigorous up-front planning.** ## Natural flows ### Flow: work's natural movement Flow: The natural progress of work Cadence: The predictable and regular elements of work Slack: The gaps between work that make flow possible Observing a specific event is often less informative than observing a stream of events. It is precisely this flow which give us context, and that context leads to clarity. ### Slack: avoiding too many notes Consider what makes a roadway flow. It's the balance between cars and open space that give us flowing traffic. That open space is called \"slack\". We need slack in our workflow, we need space to adjust. Pull too many sticky notes into your TODAY column and the overload will make you less responsive. ### Pull vs Push Pull is essential for stability and sustainability. The more a system relies to force action, the less sustainable it becomes. Push systems tend to cause bottlenecks by ignoring natural capacity. Work is released downstream whether or not the worker has the capacity to process it. Capacity problems are discovered after the fact, works begins to pile up and, as it grows, can easily escalate into an emergency. In a pull system people take work only when they have the capacity to do so. Pushing tends to be a blind act; the initiator has little idea of the terrain situated before him. Pulling is a rational act. The initiator is familiar with the terrain that lies ahead, and can gauge the amount of room in which to manoeuvre. When you reach into your backlog and pull a task READY into DOING, you're making a conscious choice based on the room you have in your WIP. **Pull when you can, be pushed when you must.** ## Components of a quality life ### Metacognition: a cure for the common wisdom **We obsess over getting _stuff_ done, rather than getting the _right stuff_ done, and at the _right time_. We focus so intently on task completion that we lose sight of the work we're engaged in.** Focusing on productivity is myopic. Effectiveness is our goal, and for that we need clarity. Clarity is not just understanding what we're doing, it's **why** and **how** we're doing it. ### Productivity, efficiency and effectiveness Productivity: You get a lot of work done, but is it the right work? Efficiency: Your work is easily done, but is it focused for maximum effect? Effectiveness: You get the right work done ad the right time... this time. Is this process repeatable? Productivity should not be the ultimate measure of human potential. Personal Kanban is: * A productivity tool: limiting our WIP helps us accomplish more. * An efficiency tool: focusing on our value stream encourages us to find ways to do more while expending less effort. * An effectiveness tool: making our options explicit leads us to make informed decisions. **To feel truly successful, fulfilled, or self-actualised, we need to feel pride in our work.** ## Ending our priorities In the absence of a visual control, we don't estimate, we guess. Many of us consider ourselves experts in our craft and so we estimate based on our memory. Educated guesswork is no substitute for thoughtful observation. According to the book \"Predictably Irrational\", Dr. Ariely hypothesises that deadlines are the best cure for procrastination. Deadlines are a proxy for clarity. ### Smaller, faster, better: controlling task size and limiting WIP Making tasks smaller isn't enough. We should focus on limiting WIP and completing tasks first, and make task size reduction a secondary concern. Planning should occur with minimal waste; it shouldn't become overhead. ### Prioritisation in theory and practice Some options have undeniable immediate value, there are others we should exercise for long-term effectiveness. With an eye for both short-term and long-term effectiveness, we avoid the traps of unchecked productivity planning. #### Urgent and important Tasks whose status escalates into Urgent and Important should be flagged for a retrospective. Our focus should be on avoiding emergencies, not reacting to them. #### Important but not urgent Quality-related tasks, the time and effort you spend here is an investment in future quality. These tasks provide future rather than immediate results. This quadrant is the antidote for panic. #### Urgent but not important This is the quadrant of social investment. #### Not urgent and not important This quadrant can be of assumed wast, or it can be one for actions you find pleasant (pleasant is good!). Here is where experimentation occurs, where we find new options. In the other three quadrants you will find work; here you will find inspiration. All four type of tasks are integral to a balanced life. PRIORITY 1 | PRIORITY 2 | PRIORITY 3 | TODAY | DOING | DONE -----------|------------|------------|-------|-------|----- | | | The priority filter provides you with a deep visual distillation high priority tasks in your backlog. You can use colour and shape for cards. These things are deceptively simple yet robust ways to differentiate between tasks, projects, collaborators, or priority. ### Expert: metrics in Peronal Kanban Metrics should reflect our context, revealing the difference between expected and actual progress. Situational knowledge is seeing the road, metrics are the gas gauge. Metrics gathered but not used are waste, so choose them with care. Note: Don't fall prey to \"metric-blindless\" where you rely too heavily on metrics without having good situational knowledge. #### Metric One: your gut Our intuition can tell us when something needs to be improved. These impulses are what we base our hypothesis on. #### Metric two: the process laboratory We can introduce columns to visualise certain hypotheses about our successes or failures. > By visualising Reginald's actions, his impact on the team is made obvious: Tasks are stalled three days on average by that guy's obsessive need to focus. #### Metric three: the subjective well being box > Doing things you don't enjoy reduces your effectiveness. Which brings us to an easy metric: the Subjective Well-Being Box (SWB). The SWB helps us identify what exactly impacts our mood so that we can begin to optimise for \"pretty good\", \"good\" or even \"great\". It also lets us put into perspective onerous tasks we simply don't like doing. When you complete a task and it moves you to an extreme, annotate it as a positive or negative experience, and why in the SWB. Don't discard the contents of your SWB until you begin to recognise patterns. When you do, consider: * When to refuse work. * When to delegate work. * What changes can help ensure success. * Which processes you might want to recreate. * What your career options really are. * How to balance family, career, recreation and personal development. #### Metric four: time If you want to get all statistical. When you create a sticky note, include the date of creation (Born), the date you pull it into READY (Begin), the date you began working on it (WIP), and when you are finished, the date you pull it into DONE (Done). Lead time is the time it takes a task to travel from your backlog to completion. Cycle time is the time it takes to travel from READY to DONE. ## Strive for improvement Everyone can excel with clarity, everyone can provide value. A need is only truly met if there is clarity around it. The need for shelter is fulfilled only if there is a sense of stability around it, when there is an assurance it won't be taken away. When a need is secured we're better equipped to explore additional needs. We progress towards actualisation when we adopt a mindset that minimises fear and embraces growth. ### Course corrections: the reality of reprioritisation Making course corrections while there're still small ensure success with the least degree of disruption. Projects are seldom precise. Deviation from the original plan is inevitable and frequent small adjustments are unavoidable. We instinctively course correct all the time. ### The bedrock of introspection At the moment we make them, pragmatic decisions and emotional decisions are often indistinguishable. We need to revisit our decisions after the fact, do we understand the motivation? That's where introspection comes in. When we're introspective, we observe our thought processes to understand the reasoning behind our decisions. ### Retrospectives Retrospectives are regular and ritualised moments of collective reflection. Retrospectives allow a team to pause and consider what went well with their project, what didn't go as expected, and what could be improved going forward. Can take place at whatever intervals you're comfortable with, the more frequent, the fresher things are in your mind. It's an opportunity to recognise accomplishments (celebration), bemoan setbacks (catharsis), and re-orient a project for future action (kaizen event). It's helpful to feature a RETROSPECTIVE column as the final column of your Personal Kanban. At the beginning or tend of each week hold a restrospective and quickly examine completed tasks. Don't pass up opportunities to address issues before they escalate. ### Solving problems at their source When things go wrong, our first line of defence is to identify who or what appears to be responsible. #### Pattern matching as a foundation for problem solving Connecting the dots allows us to interpret and make assumptions about our environment. Visualising tasks and engaging with them physically and cognitively allows us to comprehend patterns in our work. Opportunities for improvement usually arise when a change in pattern is detected. Poorly performing patterns are often merely symptoms of an underlying problem. Two simplest ways to analyse where to improve: * **The Five Whys**: Problems often have nested causes. Repeat \"why\" five times to every matter, until you arrive at something with real context. Stopping after the first _Why?_ confuses the symptom with the cause. Drilling down the heart of the matter exposes an actionable reason for the event. * **The Socratic Method**: You question your own assumptions, stripping away confounding information to reveal the truth embedded in your position. Critical self-enquiry – playing your own devil's advocate – requires both patience and honesty, but is essential in the quest for improvement. ## Endgame > Visualise your work. Limit your work-in-progress Visualising our work helps us appropriately channel our efforts by no letting us hide unattractive tasks in the recesses of our minds. Work unseen is work uncontrolled and we can't (and shouldn't) do more work than we can handle. ## Appendix: Personal Kanban design patterns ### Sequestering approach: dealing with repetitive tasks The Sequestering Approach is specifically designed to deal with repetitive tasks in an elegant way. Repetitive tasks can clutter your Kanban and create wasteful overhead. Consider giving repetitive tasks their own visualisation and WIP treatment. Sequester them in a dedicated area of your Kanban. When a task is complete, simply check that day's box. ┌───────┬───────────┬──────┐ │ READY │ DOING (3) │ DONE │ ├───────┼───────────┼──────┤ │ │ │ │ · · · · ┌─────────────────────────────────────────┐ │ RECURRING TASKS │ ├──────────┬─────────┬──────┬──────┬──────┤ │ ITEM │ REPEAT │ LAST │ NEXT │ DONE │ ├──────────┼─────────┼──────┼──────┼──────┤ │ STANDUP │ DAILY │ ── │ ── │ X │ │ MEETING │ MONTHLY │ 6/15 │ 7/15 │ X │ │ CHECK IN │ MONTHLY │ 6/20 │ 7/20 │ │ · · · · · · ### Emergency response approach: taming unexpected workloads In an emergency response situation, you simply don't have time to fully complete each task before starting a new one, and tracking individual sub-tasks is a waste of your time. This is multitasking by necessity, but it's _controlled_ multitasking. The value stream for emergency response approach shows: * **BEGUN**: Begun to work in the task * **ASSEMBLING**: Gathering paperwork or other requirements * **ASSEMBLED**: Requirements are complete * **ACTIVE**: If you are waiting someone else to act * **COMPLETE**: If it's complete It might be useful to include a NOTES column. ### Time capsule approach We invariably amass a lot of small tasks that are important but not urgent. These tasks may start out benign, but the longer we put them off the more likely they'll spiral into crisis. All those little tasks pull them off the board, go to your desk, and start doing them until they're done or your day is over. This is a speed tasking exercise. If you find yourself de-cluttering more than once a month, then it's likely you are overcommitting yourself. ### Balanced throughput approach Give a WIP limit of three small tasks to get done quickly, then a WIP limit of two larger tasks to do later. Don't move completed tasks off your board until the end of the day or until all five tasks (large and small) are complete. If you replace them, you aren't actually balancing your throughput. ### Personal Kanban and Pomodoro Pomodoro Technique is useful when complete immersion is the only way to get a task out of WIP. Pomodoro is a perfect complement to Personal Kanban, helping you process your WIP in 25 minute bursts. ",
    "url": "/learning-notes/books/personal-kanban/",
    "relUrl": "/books/personal-kanban/"
  },"18": {
    "doc": "Radical Focus: Achieving Your Most Important Goals with Objectives and Key Results",
    "title": "Radical Focus: Achieving Your Most Important Goals with Objectives and Key Results",
    "content": "# [Radical Focus: Achieving Your Most Important Goals with Objectives and Key Results](https://www.goodreads.com/book/show/28951428-radical-focus) - [Introduction](#introduction) - [The framework for Radical Focus](#the-framework-for-radical-focus) - [Why we can't get things done](#why-we-cant-get-things-done) - [A path to success](#a-path-to-success) - [OKR fundamentals](#okr-fundamentals) - [Setting a rhythm of execution](#setting-a-rhythm-of-execution) - [How to hold a meeting to set OKRs for the quarter](#how-to-hold-a-meeting-to-set-okrs-for-the-quarter) - [The timing for OKRs](#the-timing-for-okrs) - [Preparing for the next quarter](#preparing-for-the-next-quarter) - [The first time](#the-first-time) - [OKRs for MVPs](#okrs-for-mvps) - [Improve weekly status emails with OKRs](#improve-weekly-status-emails-with-okrs) - [Common OKR mistakes](#common-okr-mistakes) - [Quick tips on OKRs use](#quick-tips-on-okrs-use) > Don't tell people how to do things, tell them what you need done and let them surprise you with their results – General George Patton You can release all the features you want, but if it doesn't solve the underlying business problem, you haven't really solved anything. Executives and other stakeholders all too often come up with the quarterly \"roadmap\" of features and projects and then pass them down to the product teams, essentially telling them how to solve the underlying business problems. Progress is measured by output and not by outcome. ## Introduction Ideas, like NDAs, aren't worth the paper they are printed on. **Ideas are easier to come up, what's hard, really hard is moving from an idea to reality.** It's not important to protect an idea. It's important to protect the time it takes to make it real. You need a system to keep you and your team aimed at your goal. **A startup enemy is time, and the enemy of timely execution is distraction.** ## The framework for Radical Focus ### Why we can't get things done If you are CEO or a manager, you want things for your company. Even in the most successful companies, the thing we have determined must happen, often doesn't. This is due: 1. **We haven't prioritised our goals** > If everything is important, nothing really is. You could probably put stuff in order of importance, choose to work one thing at a time. Setting a single Objective with only three Key Results to measure keeps you focused. 2. **We haven't communicated the goal obsessively and comprehensively** > When you are tired of saying it, people are starting to hear it. Once you have the goal projects must be evaluated against it. By repeating the goal in commitment meetings, weekly status emails or Friday wins celebrations, we assure that the goal is in the front of everyone's mind and tied to all activities. 3. **We don't have a plan to get things done**. Will power is a finite resource. you need a process that helps you make sense of the work you need to do, and keeps you on track even when you are tired. 4. **We haven't made time for what matters**. The Eisenhower Box (Important/Urgent matrix) is a time management tool. Urgent things get done because the pressure of time. Unless we bring that pressure to important but not urgent things, they won't get done. Block out time to do what matters. Nothing is as invigorating as a deadline. By committing to work towards the Objective, you assure you'll be held accountable to progress. 5. **We give up instead of iterate**. You'll fail the first time you implement OKRs. Maybe a company will find they have sandbaggers where no one ever sets hard goals; a company that is afraid to fail. Or maybe you have a company where no one makes their Key Results, because people over-promise and under-deliver. The most common fail is no follow-through, people set OKRs just to ignore them afterward. Successful one try again in. The only hope for success is iteration; you track what works, and what does not. ### A path to success We start our journey to our dreams by wanting, but we arrive by focusing, planning and learning. #### Before starting OKRs, check your mission If you think that you create a startup to make money, you are misinformed. [90% of startups fail](https://www.allmandlaw.com/blog/2013/january/mapping-tech-startups.aspx). If you want to change the world you will need a mission. The mission has to be simple, memorable and act as a guide when you make a decision about how to spend your time. Great missions are inspirational, yet directed. A mission and an Objective in the OKR model have a lot in common, they are aspirational and memorable, the key difference is time scale. An Objective takes you through a year or a quarter. A mission should last at least five years. A mission keeps you on the rails. The OKRs provide focus and milestones. Using OKRs without a mission is messy, undirected and potentially destructive. Once you have a mission, selecting each quarter's Objective is straightforward, you are ready to have conversations on how to move the mission forward. ## OKR fundamentals OKR stands for **Objectives** and **Key Results**, they came from Andy Grove implementing Peter Drucker's Management by Objective system at Intel. John Doerr, former intel executive, began evangelising them to all the startups he invested. Some like Google and then Zynga embraced them fully. The Objective is qualitative, and the KRs (most often three) are quantitative. The objective establishes a goal for a set period of time, usually a quarter. The Key Results tell you if the Objective has been met by the end of the time. Your **Objective** is a single sentence that is: * **Qualitative and Inspirational**. Designed to get people jumping out of bed in the morning with excitement, it provides meaning and progress. Use the language of your team. * **Time bound**. For example, doable in a month or a quarter. * **Actionable by team independently**. Your Objective has to be truly yours, and you can't have the excuse of \"Marketing didn't market it\". **Key Results** take all that inspirational language and quantify it. \"How would we know if we met our Objective?\". Typically you have three Key Results. They can be based on anything you can measure like Growth, Engagement, Revenue, Performance or Quality. If you select your KRs wisely, you can balance forces like growth and performance, or revenue and quality by making sure you have potentially opposing forces represented. OKRs are always stretch goals, **KRs should be difficult, not impossible**. A great way to do this is to set a confidence level of five of ten on an OKR. If you want to achieve great things, you have to find a way to make it safe to reach further than anyone has before. Pushing yourself and your team to bigger things yet not making it impossible. The sweet spot is when you have 50/50 chance of failing. The company should set an OKR, and then each department should determine how _their_ OKR leads to the company's successful OKR. A team can focus their OKR ona single Key Result or try to support the entire set. Much of the value in OKRs comes form the conversations on what matters. When setting OKRs for individuals, each one should set OKRs that reflect both personal growth and support the company's goal. Individual OKRs are about becoming better at your job as well as helping your product get better. When working with individual OKRs, you can set goals to correct problems _before_ they blossom into full disciplinary actions. OKRs are part of your regular rhythm. Bake them into your weekly team meetings and your weekly status emails. Adjust your confidence levels every single week. Have discussions about why they are going up and down. Do not change OKRs halfway through the quarter. Fail or nail then, and use that learning to set them better next time. Changing them halfway through teaches your team not to take OKRs seriously. **OKRs aren't about hitting targets, but about learning what you are really capable of. Failure is positive indicator of stretching.** ## Setting a rhythm of execution It's important to have a cadence of commitment and celebration. Each Monday, the team should meet to check in on progress against OKRs, and commit to the tasks that will help the company meet its Objective. A format of four key quadrants is recommended: * **Intention for the week**: What are the 3-4 most important things you must get done this week toward the Objective? Discuss if these priorities will get you close to the OKRs. * **Forecast for month**: What should our team know is coming up that they can help with or prepare for? * **Status toward OKRs**: If you set a confidence of five out of ten, has that move up or down? Have a discussion about why. * **Health metrics**: Pick two things you want to protect as you strive toward greatness. What can you not afford to eff-up? Key relationships with customers? Code stability? Team well-being? Now mark when things start to go sideways and discuss it. ``` ┌─────────────────────────────────┬──────────────────────────────────────────┐ │ PRIORITIES THIS WEEK │ OKR CONFIDENCE │ │ │ │ │ - P1: Close deal with TLM foods │ Objective: │ │ - P1: New order flow Spec'd │ Establish clear value to │ │ - P1: 3 sales solid candidates │ distributors as quality tea provider │ │ for interview │ KR: Reorders at 85% 5/10 │ │ │ KR: 20% of reorders self-serve 5/10 │ │ │ KR: Revenue of 250K 5/10 │ ├─────────────────────────────────┼──────────────────────────────────────────┤ │ NEXT 4 WEEKS - PROJECTS │ HEALTH │ │ │ │ │ - Passive reorder notifications │ Team health: #yellow │ │ - New self serve flow for │ Team struggling with │ │ distributors │ direction change │ │ - Metrics for distributors on │ │ │ tea sales │ Distributor satisfaction health: #green │ │ - Hire customer service head │ │ └─────────────────────────────────┴──────────────────────────────────────────┘ ``` This document is first and last a conversation tool. Set the tone of the meeting to be about team members helping each other to meet the shared goals they have committed to. Missing your goals without also seeing how far you've come is often depressing. Friday wins session is critical. Teams demo whatever they can, every team should share something. ### How to hold a meeting to set OKRs for the quarter Keep the meeting small, ten or fewer people if possible. Should be run by the CEO and must include senior executive team. Take away phones and computers. A few days before the meeting, solicit all the employees to submit the Objective they think the company should focus on. Have someone collect and bring forward the best and most popular ones. Set aside 4.5 hours to meet. Two 2-hour sessions, with 30 minute break. Each exec head should have an Objective or two in mind to bring to the meeting. Combine similar Objectives and narrow them to three. Have all members of the exec team _freelist_ as many metrics as they can think of to measure the Objective. You put one idea on each Post-it so you can rearrange, discard or modify the data you have. If two people write down DAU, you can put those on top of each other and count two votes for that metric. Pick three types of metrics. It's easier to discuss what to measure, then what the value should be and if it's really a \"shoot for the moon\" goal. **It's recommended having a usage metric, a revenue metric and a satisfaction metric for the KRs. Find different ways to measure success. Two revenue metrics might have an unbalanced approach to success.** Set the values for the KRs. Make sure they really are \"shoot for the moon\" goals. You should have only 50% confidence you can make them. Challenge each other. Take five minutes to discuss the final OKR set. ## The timing for OKRs Plan out the timing for OKR implementation, teh standard rhythm from quarter to quarter: 1. All employees submit the Objective they think the company should pursue next quarter. 2. Exec team in half-day session discusses the Objective propose. They choose one. The team sets the KRs, as outlined previously. 3. Execs introduce the OKR for quarter to their direct reports, and have them develop department OKRs. 4. CEO approval. 5. Department head gives the company and department OKRs to any subteams, and these teams develop their own. 7. All hands meeting in which CEO discusses why the OKR is what it is for the quarter, and calls out a few exemplary ones set by directs. Covers last quarters OKRs and points out a few key wins from the quarter. ### Preparing for the next quarter Admit you have missed a KR, or admit you set a KR too low and it it too easily.Get that learning, and roll it into your next goal-setting exercise. OKRs are about continuous improvement and learning cycles. ### The first time The first time you try OKRs, you are likely to fail. Your team may become disillusioned with the approach, and be unwilling to try them again. To reduce this risk: 1. Start with only one OKR for the company. 2. Have _one_ team adopt OKRs before the entire company does. 3. Start out by applying OKRs to projects. ## OKRs for MVPs Anyone initiating a new feature should outline a clear Objective and a set of Key Results to better understand why we are doing the work. Including OKRs on the Kanban cards forces the team to answer two important questions before anything is built: 1. What are we trying to achieve with this feature? 2. How do we measure success or failure? ``` ┌───────────────────────────┐ │ MVP: EXAMPLE │ │ │ │ - Hypothesis (Objective): │ │ - Key result: │ │ - Method: │ │ - Stories: │ └───────────────────────────┘ ``` This allows to prioritise work on the roadmap based on its expected impact in business goals. If the business has an Objective, we can prioritise features that we think will have the biggest impact on that area. Being able to quickly recite the business logic behind a feature and its position in the queue can make conversations much more efficient and less emotional! ## Improve weekly status emails with OKRs At Zynga, reports were sent to the entire management team, these status reports were very successful because they ahd important information laid out in a digestible format. 1. **Lead with your team's OKRs, and how much confidence you have that you are going to hit them this quarter.** You list OKRs to remind everyone _why_ you are doing the things you do. How likely you feel you will meet your Key Results on a scale from 1 to 10. Red falls below 3, green as it passes 7. 2. **List last week's prioritised tasks and if they were achieved.** If they were not, a few words to explain why. 3. **Next list next week priorities.** Only P1s, they have to have a clear outcome. 4. **LIst any risks or blockers.** Things you can't solve yourself. Do _not_ play the blame game. List anything you know tof that could keep you from accomplishing what you set out to do. 5. **Notes.** Anything that doesn't fit in these categories. ``` To: exec-team@example.com Subject: Week of 10/15/2020 Body: Objective: Establish clear value to restaurant suppliers as a quality tea provider, KR: Reorders at 85% 6/10 KR: 20% of reorders self-service 6/10 KR: Revenue of 250K 4/10 LAST WEEK P1 Close new deal with TLM Foods NOT DONE - extra surprise level of approvals P1 New Order flow spec'd and approved P1 3 solid sales candidates in for interview NOT DONE one flaked, need better pipeline. Discuss? P2 Customer service job description to recruiter DONE NEXT WEEK P1 Close deal with TLM P1 Offer out to Dave Kimton P1 Usability tests: discover and prioritise key issues with self-serve NOTES Anyone know the procurement VP at Johnson Supplies? Also, lmk if you want to sit in on usability! It's good for your soul, y'know.... ``` This format also fixes another key challenge large organisations face: coordination. Coordinating organizational efforts is critical to compete and innovate. Giving up on the status email is a strategic error. ## Common OKR mistakes * **You set too many goals per quarter.** Try setting only one. * **You set OKRs for a week or a month.** Startups maybe shouldn't use OKRs before achieving product/market fit. If you can't keep on track longer than a week, you probably aren't ready for OKRs. * **You set a metric-driven Objective.** The Objective needs to be inspirational. * **You don't set confidence levels.** OKRs are there to encourage you to shoot for the moon. Setting a confidence level of five out of ten means you have a 50% chance of hitting the goal. That's stretching. * **You don't track changing confidence levels.** Mark changes as you get new information. * **You use the four-square on Monday as a status report.** Discuss what needs discussing. * **You talk tough on Friday.** Let's crack a beer and toast what we _did_ accomplish. Let's be proud of what setting big goals did let us accomplish. ## Quick tips on OKRs use * Set only one OKR for the company, unless you have multiple business lines. * Give yourself three months for an OKR. * Keep the metrics out of the Objective. * In the weekly check in, open with company OKR, then do groups. Don't do individuals, that's better in private. * OKRs cascade; set the company OKRs, then group's/role's, and then individuals. * OKRs are not the only thing you do; they are the one thing you _must_ do. Don't jam every task into OKRs. * The Monday OKR check in is a _conversation_. * Encourage employees to suggest company OKRs. OKRs are great bottom up, not just top down. * Make OKRs available publicly for the whole company. * Friday celebrations is an antidote to Monday's grim business. ",
    "url": "/learning-notes/books/radical-focus/",
    "relUrl": "/books/radical-focus/"
  },"19": {
    "doc": "Resilient Management",
    "title": "Resilient Management",
    "content": "# [Resilient Management](https://www.goodreads.com/book/show/45767533-resilient-management) - [Meet your team](#meet-your-team) - [Work style and preferences](#work-style-and-preferences) - [Grow your teammates](#grow-your-teammates) - [Mentoring](#mentoring) - [Coaching](#coaching) - [Sponsoring](#sponsoring) - [Constructing and delivering feedback](#constructing-and-delivering-feedback) - [Striking a balance](#striking-a-balance) - [Set clear expectations](#set-clear-expectations) - [Roles and responsibilities](#roles-and-responsibilities) - [Team vision and priorities](#team-vision-and-priorities) - [Team practices](#team-practices) - [Communicate effectively](#communicate-effectively) - [Planning your communication](#planning-your-communication) - [Delivering sensitive information](#delivering-sensitive-information) - [Choosing your medium](#choosing-your-medium) - [Iterating and evolving](#iterating-and-evolving) - [Build resiliency](#build-resiliency) - [Managing times of crisis](#managing-times-of-crisis) - [Managing your energy](#managing-your-energy) - [Building a support network](#building-a-support-network) - [Resources](#resources) A team is composed of a least two people who share the same strategic objective. Glossary: * Teammates: People you're responsible as a manager. * Discipline: The skill set you primarily use at work. Peg: Design, or Brand Design. * Functional team: People who work on the same discipline. * Feature (Product) team: People from different disciplines who work together on a specific feature or product. Sometimes, a team might be described as both functional team _and_ feature team, peg: Mobile Platform team. As a manager, you must be able to assess and improve your team's dynamic. Tuckman's stages of group development: 1. **Forming**, when the group comes together. It might have a name and some understanding of tis goal, but processes and patterns still need to be defined. 2. **Storming**, you'll start to see some friction. You've gotta feel some confusion and clashing to make it to the next stage. 4. **Norming**, where things start to iron themselves out. Individuals resolve their differences, and clarity is introduced. 5. **Performing**, flow state. You're effective, you are communicating well, and you're shipping. When a person joins, or a manager changes, or the mission changes, these stages of group development can restart. ## Meet your team One of your primary jobs is to foster a foundation of trust on your team. To foster trust, you've gotta start by understanding each other. To get to know your teammates as individuals, start from a place of genuine curiosity and authenticity. Paloma Medina research indicates that humans have six core needs (_BICEPS_). Needs that must be met in order for us to feel safe and secure. * **Belonging**, a connection to a community or to a group of people. When you feel rejected or left out from a group, you'll feel threatened. * **Improvement/progress**, A sense of making progress, whether for your organisation, your team, or your personal life. People may feel frustrated when their work doesn't affect the greater good, or when there are no enough challenges, or when they change from the maker path to the manager path. * **Choice (or autonomy)**, the power to make decisions about your own life and work. This needs calls for balance, too many choices can feel overwhelming; too few choices can make you feel powerless. * **Equality/fairness**, the idea that your environment includes equal access to resources, information, and support for everyone in it. When this doesn't happen (or there's a perception that doesn't happen), people take to the streets and revolt. * **Predictability**, requires balancing too. If change is the only constant, it can be exhausting. If there are _never_ any surprises, it gets boring. When our sense of certainty is threatened, we may have a _really_ hard time focusing. * **Significance**, or status. Understanding where we are in a hierarchy, especially in relation to others. How do we get recognised also plays into it. Each person might care about some particular code needs more than others. Often we project our _own_ core needs onto someone else, which means you'll likely be trying to help them have their needs met while not quite addressing the _core_ of their core-need problem. ### Work style and preferences It can be really helpful to gather insights about each teammate's growth areas, preferences about feedback and recognition, and other aspects of their work. You can ask a list of simple questions to your teammates in your first 1:1 together. A template. ``` What makes you grumpy? How will I know when you're grumpy? How can I help you when you're grumpy? In what medium (Slack, email, in person, etc.) do you prefer to receive feedback? When do you prefer to receive feedback—routinely in 1:1s, or as-it-happens? How do you prefer to receive recognition—publicly or privately? What makes 1:1s the most valuable for you? What are your goals for this year? And for the next three months? What do you need from your manager? What do you need from your teammates? What do you need from your peers outside the team? Human learning and growth requires the right amount of four things: new challenges, low ego, space to reflect and brainstorm, and timely and clear feedback. How are these four going for you? Is there one you need more or less of? What's your favourite way to treat yourself? ``` As you're kicking off a relationship with a new direct report, it's just as important for them to get to know _you_ as you get to know _them_. When managers share their approach to management with their teammates, it can create an opportunity to develop better working relationships. Articulate what you're personally optimising for in your role, so you can share that with your teammates. #### What are you optimising for? The framing of this question can help you identify your \"style\" or \"philosophy\" as a manager or leader. > I was optimising for growing emerging leaders. I asked open questions and offered reflections much more than I issued directives. Direct reports who wanted more direction grew frustrated. Think about the scenarios in which your particular management approach might manifest day to day, such when you are: * Coaching, mentoring, or sponsoring your reports * Requesting and delivering feedback * Goal setting or vision setting for/with your team * Scoping, delegating, and shipping work for/with your team * Communicating to/with your teammates in different mediums In those environments, what do you find you _optimise_ for most as a manager? --- Optimise for long-term relationships. In the beginning of your relationships, share with your teammates: * What do you optimise for in your role? * What do you hope your teammates will lean on you for? * What management skill are you currently working on learning or improving? Sharing the _one thing_ that you want to make sure your teammates know about you, so that they aren't mystified day to day. ## Grow your teammates This is the Storming stage. Previously, each person had been doing their own thing asn individuals, now a few things need to be ironed out: how to collaborate, how to hit goals, how to determine priorities. There will be friction. You'll end up wearing four different hats: Mentoring, coaching, sponsoring and delivering feedback. ### Mentoring In mentoring mode, we're giving out advice, sharing our experience and perspective, and helping someone else problem based on that information. If you are not a member of a marginalised group, and you have a mentee who _is_, be aware of the way members of underrepresented groups are perceived. Make sure that your advice is going to be helpful in practice for your mentee. Our mentee success is ultimately _our_ success. Managers often default to mentoring mode because it feels lke the fastest way to solve a problem, but it falls short in helping your teammate connect their _own_ dots. ### Coaching In mentoring mode, you're focused on both the problem and the solution. While coaching, you'll be asking open questions and reflecting upon them. #### Asking open questions To explore more of the shape of the topic, rather than staying at the surface level. The best questions are about the problem, not the solution. Questions that start with _why_ tend to make the other person feel judged, questions that start with _how_ tend to go into problem solving mode, bot we should be avoided. _What_ questions can be authentically curious. * _What's most important to you about it?_ * _What's holding you back?_ * _What does success look like?_ Coaching would start a two-way conversation, which would help make an otherwise tricky conversation feel more like a shared exploration. Open questions, asked from a place of genuine curiosity, help people feel seen and heard. Forming lots of open questions (instead of problem solving questions, or giving advice) is tremendously hard, but totally worth it. #### Reflecting Holding up a mirror for the other person and describing what you see or hear, or asking them to reflect themselves. Help your teammates reflect by repeating back to them what you hear them say. * _What I'm hearing you say is that you're frustrated with how this project is going. Is that right?_ Don't be worried about giving a bad reflection; reflecting back what you're hearing will still help your teammate. Sometimes the act of reflection forces the other person to do some _introspection_ an realise new aspects of the problem. When coaching, you don't need to have all the answers, you're just there as a mirror and as a question-asker, to help the other person to think deeply and come to some new, interesting conclusions. It may not feel all that effective but coaching can generate _way more growth_ than giving them advice or sharing your perspective. ### Sponsoring The sponsor hat is more often worn when they're _not_ around. Sponsorship is all about feeling _on the hook_ for getting someone to the next level. You will put your personal reputation on the line on behalf of the person you're sponsoring. * Giving visible/public recognition * Assigning stretch tasks and projects that are _just beyond_ their current skill set, to help them grow and gather evidence for future promotion * Opening the door for them to write blog posts, give company or conference talks, or contribute open-source work. Members of underrepresented groups are typically over-mentored, but under-sponsored (_in-group bias_). Put in the time and intention to ensure that you're sponsoring members of underrepresented groups too. ### Constructing and delivering feedback It's important to routinely deliver both positive and negative feedback to your teammates about their performance within their role. You'll want to look for opportunities to help your teammates grow or work better together by giving them feedback. The best feedback is specific, actionable, and delivered in a way that ensures the receiver can actually absorb it. The feedback equation: **Observation** of behaviour + **Impact** of behaviour + **Request** or **Question** = Specific, actionable feedback * **Observation**, describe the simple facts of what happened, the who/what/when/where part of the feedback. * **Impact**, _this_ is where you can share how you feel. Share the effects of the behaviour you're seeing. The impact is often personal. Framing the impact in terms of what the recipient cares about will more quickly motivate them to change their behaviour. * **Request or question**, if you're always defaulting to making a request, the other person doesn't have to do much thinking or problem solving on their own. Hit pause on the mentoring mode and go into coaching mode instead. Asking an open question at the end of delivering feedback is often much more powerful than making a request. #### Coaching teammates to deliver feedback Feedback is _really_ hard to deliver when you don't personally own it. Feedback is also much more impactful and easy to understand when receiving it directly from the person who has it. The first step _usually_ should be taken by the person who has the feedback to share. Start by coaching your teammate, what they might want the outcome to be, and what they want to optimise for as they have the conversation. Then, in mentoring mode, walk them through the feedback equation and help them craft some good words and phrases for each of the three parts. Facts without judgement. If there's no impact, then likely it is not feedback. Coach the teammate who wants to give the feedback to introspect and develop a new perspective on it. If there _is_ feedback to be given, mentor and coach your teammate to develop a solid set of questions, to be more of a conversation. If you ever find yourself needing to deliver feedback as a third party, find an aspect of the feedback that you can personally own. Care about team trust and psychological safety. If you want your teammates to get into the habit of giving feedback, encourage them to align it to their career goals. ### Striking a balance Managers and individuals often aim to accomplish the following in their 1:1s 1. **Build trust**. A manager wants to develop a relationship and get to know their report so they can best support them. A teammate wants to know if their manager cares about them and is invested in their success. 2. **Gain shared context**. A manager wants to disseminate clear, relevant information about day-to-day work and company goals. A teammate wants to hear rumours, news and higher-level strategy. 3. **Plan out and support career growth**. A manager wants to help their teammate identify goals and plan their career trajectory. A teammate wants to keep growing their skills, gain new opportunities, and get feedback on their blind spots. 4. **Solve problems**. A manager wants to hear about blockers and other challenges so that they can help their teammates get unblocked. A teammate wants advice, mentorship, and assistance in making progress on their projects. Open questions and reflections (coach) do so much more for building trust and helping someone grow than mentoring activities do. Sponsoring is great for growth _and_ for building trust. If a teammate needs a bit more direction, mentoring is great. Feedback is the icing on the cake. Once you've build a foundation of trust, be open to feedback about which skills you should practice more. Sometimes the other person won't fully understand feedback, it won't take it seriously, or won't be able to change. Consider whether or not this teammate might flourish if they moved ot a different part of the organisation. When someone's behaviour is damaging to those around them, talk with another leader in the organisation about what to do. It's possible that there's another way you could be approaching this person to help them see how serious the situation is, and what needs to change; it's also possible that it's time for them to go. Lean on your network of support, you don't need to do this alone. ## Set clear expectations Develop clear expectations in _collaboration_ with your team, document them in a searchable, central location, and keep them updated over time. Team-wide expectations that are worth documenting and iterating: * Teammates' roles and responsibilities, including the manager's role * The team mission or priorities * How teammates should be collaborating, communicating, and shipping work ### Roles and responsibilities Clarity typically comes in the form of documentation like a career ladder. Individual's day-to-day or project-specific responsibilities can feel ambiguous. You can use two tools to nail down specifics, the assignment matrix and Venn diagram. #### Responsibility assignment matrix A responsibility assignment matrix (_RACI matrix_, Responsible, Accountable, Consulted and Informed), the most frequent roles for people involved in a project or decision: * **Responsible**: People who do the work * **Accountable**: The person (_just one_) who must ensure that the project gets completed or the decision gets made. Often the one that do the most communication with stakeholders * **Consulted**: Folks whose opinions are sought as the work progress, they're actually not working on the project * **Informed**: People who are updated either as major milestones are hit, or when the deliverable is complete Naming some folks as Informed or Consulted can add clarity and reduce design by committee. #### Responsibility Venn diagram Venn diagram that could answer \"who should be doing what?\". You can held meetings with product managers, engineering managers, and engineering leads to chat about their responsibilities. Then write down the things that need to get done to ship work. For example: * Product Manager owns the story of \"what\" * Translates company goals into tech roadmap * Is communication conduit to Product and broader org * UNderstands customer needs + produces customer insights * Engineering Manager owns the story of \"who\" * Coaches makers' career paths and personal growth * Recruits and owns interview workflow * Delivers annual performance reviews * Monitors team health * Engineering Lead owns the story of \"how\" * Pairing/technical mentorship * Surfaces major arch changes to Eng leaders * Translates \"why\" for project to \"how\" (arch decisions, UX, product health) * Identifies technical risks All of those bubbles, intersect on: * Define + improve team processes (standups, etc) * Deliver feedback to teammates * Communicate hiring needs * Ensure team is hitting key results * Identify and solve execution roadblocks early * Understand, own, and share the story of \"why\" You can use this tool with _any_ intersection of roles. It's helpful any time you have people in distinct roles or functions all working together toward a united goal. The most important outcome of these exercises is that everyone on the team will have a shared understanding for who is doing what, and who they should go for which kinds of questions and decisions. ### Team vision and priorities Companies usually have a high-level \"north star\" to inspire and align the work that gets done by individual teams. Goal-measuring systems like OKRs or KPIs to measure progress over time. What's your team's \"north star\" and how does your team work toward it every day? * **Vision**: The team's north star; a dream of what the future could look like. * **Mission**: A more grounded version of the vision that describe the team's role as it works towards that north star. * **Strategy**: How this team goes about achieving that mission every day. * **Objectives**: Measurable goals that reflect the mission and strategy, to help benchmark the team's progress. Collaborating on an publishing a vision and mission helps teams to gut check project plans, share what they are doing with stakeholders around the company, and back up on hiring strategy. ### Team practices To help add clarity to your teams, it's a good idea to document details on your team practices * **Meetings**: Identify all of the meetings. Jot down the purpose, frequency, and participants. Add descriptions so potential attendees decide which meetings are important to attend, which might be _interesting_ to attend, and which they can probably skip. * **Email and chat channels**: Document your team's messaging channels, email lists, and resource pages, include context about _why_ someone might (or must) choose one channel or method over another. Make it clear that you want your teammates to take part in improving the way the team works together. * **Collaboration and interaction**: The kind of work your team does, the processes your team has collaborating and delivering projects, and what _you_ value in team settings. At Etsy they had the \"Charter of Mindful Communication\" * _Reflect on the dynamics in the room_, to reflect when they were dominating a conversation * _Elevate the conversation_, by being constructive to feedback * _Assume best intentions_ * _Listen to learn_ to stay curious about other's perspectives Documentation should be developed collaboratively, like a living document. ## Communicate effectively You will be continually tasked with communicating information to your team as the organisation around you evolves, especially when you don't have a say. You can give feedback to those in charge and help sculpt messaging so it will be heard and understood. ### Planning your communication When you need ot deliver a critical new message, it's a good idea to prepare a _communications plan_. Julia Grace, director of engineering at Slack, created a simple communications plan template ``` Header: author, date, status (e.g. draft) Background: * The What (most important thing you want to communicate) * The Why (why's it changing) People: * Who knows * Who will be directly impacted Timeline: * What will be said in [IRL or channel] when Talking points ``` If you need to coordinate with others, you can turn the template into a table including date and time for each message, who's communicating it, the medium used, an dhow the talking points will evolve for each step of the plan. Share the document with whomever else is doing the heavy lifting of communicating, so that you can hone the messaging and the timing together. ### Delivering sensitive information Sensitive information should be executed as swiftly as possible, there are too many things that could go sideways. Take care to understand how your team will absorb this new _sudden_ information. It can be helpful to brainstorm ways to address their reactions so you can adequately prepare * Map big changes back to the things you know people care about * Choose your planned words carefully * Plan out who can be informed early on, and who should be informed later * Optimise for creating clarity and transparency as soon as the information is set in stone * Remember that other's reactions can affect and threaten yourself You are in the best possible position to help changes _land_ with your teammates. Share this knowledge with the people above you if they're spearheading the communication rollout. #### Navigating confidentiality Consider who can be trusted with sensitive information that will impact their coworkers in the early stages of a change, and think through the ripple effects at each stage if someone were to share information they shouldn't. **The top things on your mind should be cleaning up any misrepresentations, clarifying the message, and then learning from this moment.** #### Wrestling with misalignment Sometimes, you'll be tasked with communicating information that you don't agree with or believe in. The absence of trust is the foundation of most team dysfunction. If you agree to decisions on the surface, but don't support them or commit to them, _way_ more friction will emerge over time. This behaviour will be seen as undermining or backstabbing and it will create fissures in your team environment. When you disagree and can't commit to a higher-up's decision, first be transparent and professional about it, give feedback. Sometimes the decision will be already made and your feedback won't change it. If the misalignment between you and leadership is sever, you might decide it's worth walking away from the team or the company, otherwise it's time to \"disagree and commit\". Disagreeing and committing is the most mature and transparent move you can make. Avoid sharing your brutally honest feelings with or win front of your teammates. If you need to disagree publicly, make sure you phrase it in a way that won't sow seeds of disgruntlement and uncertainty with others. Whenever possible, add steps in your communications plan explicitly designed to collect feedback, to make people feel seen and heard as they learn about this new information. ### Choosing your medium Your go-to communication style, cadence, and channels depend on what's typical for your company culture, as well as what feels right for your message. #### Meetings Meetings are there for you to both _push_ information and _pull_ information, especially when that information requires additional context. Sensitive, difficult, or surprise information is best communicated in person first (and followed up with an email). You can pivot your message based on the questions or reactions in the room. Sharing information in a meeting can significantly reduce negative gut reactions to hard-to-hear news. #### All hands meeting Gathering everyone for the same meeting is helpful when you want to roll out big news. It can be organised for subsections of the company. Walk people through the following: 1. A high-level description of what's changing 2. Instructions on what to do with questions 3. An explanation for the change 4. A list of specific changes, as well as what's not changing and what's still undecided 5. Answers to questions you foresee them asking right away The bigger or scarier the change, the less likely you'll create space for feedback, so the people can process individually or in a smaller setting afterwards. If you do hold open Q&A, avoid saying you're disappointed in a question or behave in a demeaning way toward a question-asker. You can always answer a question by saying you don't know, or that you've got to find some more information first; just be sure to follow up. It is recommended to do routine All Hands meetings, so you don't bring people up only when there is something and often scary happens. #### Email Emails are great for communicating status and announcements when there's not a lot of nuance or context to share. Beware that your emails might last _forever_. Choose emails when you're ready to set facts in stone, especially after big verbal announcement in a meeting. You can practice repeating decisions or actions after a meeting with a _recap email_. It creates a paper trail of decisions that come out of meetings. You can use images for communicating: 1. The stated company line on the topic 2. Facts about what it means in practice 3. Your personal take 4. An invitation, for questions, feedback, etc. Don't just drop bullet points, share your broader context! ### Iterating and evolving Your job isn't just to communicate out information, but rather to ensure it _connects_ with your audience. To help _absorb_ the communication. People _rarely_ remember information the first time they hear it. You'll have to repeat it several times until it gets assimilated. You'll find also that particular word choices land better with different audiences, or different mediums get more feedback or attention from others. There is another element to think about: your energy as you're communicating. There is a range of styles for communicating the same message. For example, you can use colours, so you are aware of different styles available for your toolbox * **Red**, a bit of anger, frustration, edge, or urgency. * **Orange**, cautious, hesitant, tiptoes around topics * **Yellow**, lighthearted, effervescent, crack jokes * **Green**, in tune with other's feelings, loving, high emotional intelligence * **Blue**, calm, cool, collected, steady * **Purple**, creative, flowy, great at storytelling * **Brown**, adds (and lives in) nuance, complexity, or ambiguity * **Black**, blunt, unfeeling, no nuance, cut and dry Sometimes people need to hear a different tone, or feel a different energy. Strong leaders understand the spectrum of communication styles. We also tend to assume that everyone understand, prioritises, and cares about the same things we care about, but that's not often the case. Do the leg work of listening and learning what others around you are motivated by, are optimising for, and are prioritising. Then translate your message into something that's more digestible and easy for that audience to care about. Just like your communication style and cadence will evolve over time, your organisation's will, too. As the company grows, as the vision pivots or becomes better articulated, as people leave and new people are hired, how and what they communicate will necessarily evolve too. ## Build resiliency Every organisation-wide action ripples out to both teammates and managers. Change tends to be our only constant; as managers, it's on us to step up and support our teammates as best as we can. Each new storm is an opportunity to gain experience, try out different tactics, and build up new skills so you can manage the next inevitable wave of change. ### Managing times of crisis #### Before a crisis The clearer you can describe processes, the easier the cognitive load will be for dealing with a crisis. Ways you can prepare in advance: * **Know your benefits.** Research company-sponsored support mechanisms for employees. * **Lead by example** * **Ask for input.** Ask the team about what information or resources they need to stay resilient in times of rapid change. * **Keep setting expectations.** Give them the gift of clarity, so they can also communicate with you about whether they can accomplish the work they've been assigned. #### During a crisis If you sense that something has gone awry for one of your teammates, you can say \"As your manager, I want to make sure I'm supporting you as best I can. Is there anything that would be helpful to you to chat about?\" If your teammate _does_ share with you that they're experiencing something difficult, partner with them to figure out next steps. * How else could we meet this goal? * What can I do to help you meet this goal? * What would be the impact of moving this goal? A natural instinct is to express deep concern, or deep sympathy; please do not respond in a way that requires _them_ to calm _you_. Do not put more hardship onto your direct report. Instead, acknowledge that you feel for your teammate, and refocus on their needs. \"Would it be helpful to take the afternoon off?\". If it's not obvious, ask your teammate how you can best support them. ### Managing your energy During times of major change, managers will find themselves on the receiving end of their teammates emotions; this sometimes will personally affect managers. Some managers find that the labour of managing or more strategic thinking drains their energy much faster than the work they did as individual contributors. This is normal, there are a few tactics that may help you out. #### Tracking your energy levels You can color-code your calendar events based on the kind of brain that is going to be used during that time. You can get a sense of how you are spending your mental energy each day. It can help you realise what kind events drain your energy and which ones _reenergise_ you. Experiment with your scheduling and see how the changes impact your energy or stress levels. #### Repriorising your tasks Scheduling and context-switching might not be the source of your energy drain; you might be overwhelmed by your volume of work and spread too thin. The Eisenhower matrix can help you to figure out what to do with the tasks on your plate. | Urgent | Not urgent |---------------|----------------------|------------------------------- | Important | Do these tasks now | Schedule these tasks for later | Not important | Delegate these tasks | Eliminate these tasks #### Delegating projects When carrying too many tasks, delegate work to others. As managers, we want to give our teammates beautifully packaged, cleanly wrapped gifts of leadership work. However, the best gift you can give a teammate is a messy, hard-to-measure, unscoped project. It's the biggest opportunity for someone to grow as a leader. * Hones folks' problem solving abilities * Forces them to lean on more people around them * Stretches them into new leadership skills faster It's not just okay, but actually _important_ to hand off bigger, scarier projects to emerging leaders. And as you do, you'll need to support them along the way. * Tell them how and in what medium you will support them * Tell them that you expect this to be a stretch for them, and that's the point. You'll trust them to raise a flag if they are stuck * Use a RACI. Tell your teammate that they are the \"Responsible\" role in the RACI matrix for this project, and you are \"Consulted\" and \"Accountable\". Some folks may be wary of giving away your Legos, by what most don't realise up front is that by doing so, you're creating space to pick up new ones. Be there to lend support when your teammate needs it, by asking coaching questions or reflecting back what you're hearing. Warning about pitfalls, or helping them get unstuck. Stay in coaching mode when delegating. #### Saying no The best way to troubleshoot your energy drain may be to reduce how often you say yes. Priorities and objectives are there for a reason. Wield them as a tool to help you practice saying no more often. Several tactics you can experiment with to help you say no more efficiently. * Ask someone you trust to hold you accountable to saying no more often. * Make a calendar item at the beginning of the day to spend twenty minutes figuring out what to say no to. * Draft emails that you can copy and paste whenever you need to say no clearly and gracefully. ### Building a support network Diverse group of people to lean on when you encounter a workplace challenge. Consider each individual as a facet of what an imaginary ideal manager would be. This is what is known as a _Manager Voltron_ (referring to the show where heroes join forces to create a giant super robot). Always lookout for people who: * Will push you out of your comfort zone * Have different levels of experience than you do (both more and less!) * Have experience in a different industry * Are good at the things you're terribly at Where do you find more supporters? And what do you say when you do? #### Growing your Voltron Consider folks you're connected to on Twitter, LinkedIn, and other asynchronous networks like industry email lists or Slack channels. Attend local Meetups to meet more folks there, or even better, give a talk at one! #### First-team mentality > Leaders who are strong team players understand that the people who report to them are not their first team. Their first team is their peers across the company. This first-team consider the needs of the company as a whole before focusing on the needs of their team. Shouldn't you be prioritising what your teammates need over what your peers need? In order to support your teammates well, it's important to build a strong relationships with your peers. > Instead of spending time and energy watching their backs, your leaders can be focused on moving your organisation forward. When your leaders have built trust with each other it becomes significantly easier to manage change, exhibit vulnerability, and solve problems together. If you consider your peers your first-team, you would have a more holistic picture of how others across the company are tackling changes to strategic direction, or are coaching their teammates to grow in their careers. And they would be learning from _you_, too. --- A good metaphor for growth is what happens to caterpillars. A caterpillar will have to wrap itself into a cocoon, digest and convert itself to some kind of a gruesome soup, and reform itself into a butterfly. Growth is beautiful, growth is magnificent, growth is what we should be aiming for. But in actually, growth is _painful_. Embrace the uncertainty. Know that it's there for a purpose, and that you will emerge a transformed manager on the other side. ## Resources ### Books * [Switch: How to Change Things When Change is Hard](https://www.goodreads.com/book/show/6570502-switch), by Chip Heath and Dan Heath. * [The Manager's Path: A Guide for Tech Leaders Navigating Growth and Change](https://www.goodreads.com/book/show/33369254-the-manager-s-path), by Camille Fournier ### Papers * [Mentoring: Strategies for building an effective mentoring relationship](https://www.americanjournalofsurgery.com/article/S0002-9610(13)00413-3/pdf), by doctors Hilary Sanfey, Celeste Holands, and Nancy L. Gantt * [Psychological safety and learning behaviour in work teams](http://www.iacmr.org/Conferences/WS2011/Submission_XM/Participant/Readings/Lecture9B_Jing/Edmondson,%20ASQ%201999.pdf) by Amy Edmonson ### Articles * [Research: When Managers Are Overworked, They Treat Employees Less Fairly](https://hbr.org/2018/06/research-when-managers-are-overworked-they-treat-employees-less-fairly) * [Engineering growth: assessing progress](https://medium.com/s/engineering-growth-framework/engineering-growth-assessing-progress-743620e70763) * [Coaching Skills: Listening](https://partners.coactive.com/CTI-learning-hub/fundamentals/res/FUN-Co-Active-Coaching-Skills-Listening.pdf) * [Followership](https://www.attack-gecko.net/2018/09/04/followership/) * [Remote friendly meetings and hygiene](https://larahogan.me/blog/better-meetings/) * [How to organise a working group](https://larahogan.me/blog/running-working-groups/) * [Tracking compensation and promotion inequity](https://larahogan.me/blog/inclusion-math/) * [Five Keys to Successful Google Team](https://rework.withgoogle.com/blog/five-keys-to-a-successful-google-team/) ### Tools * [Balanced 1:1s template](https://larahogan.me/resources/Balanced-One-on-Ones.pdf) * [1:1 coaching questions deck](https://shop.beplucky.com/products/the-plucky-1-1-starter-pack) * [Template for organising consistent, repeatable, and inclusive interviews](https://larahogan.me/blog/onsite-interview-loop-template/) * [Manager hand-off meeting template](https://larahogan.me/blog/manager-handoffs/) * [Google Tool to Foster Psychological Safety](https://rework.withgoogle.com/guides/understanding-team-effectiveness/steps/foster-psychological-safety/) ",
    "url": "/learning-notes/books/resilient-management/",
    "relUrl": "/books/resilient-management/"
  },"20": {
    "doc": "Screw It, Let’s Do It: Lessons In Life",
    "title": "Screw It, Let’s Do It: Lessons In Life",
    "content": "# [Screw It, Let's Do It: Lessons In Life](https://www.goodreads.com/book/show/198863.Screw_It_Let_s_Do_It) - [Just do it](#just-do-it) - [Have fun](#have-fun) - [Be bold](#be-bold) - [Challenge yourself](#challenge-yourself) - [Stand on your own feet](#stand-on-your-own-feet) - [Live the moment](#live-the-moment) - [Value family and friends](#value-family-and-friends) - [Have respect](#have-respect) - [Do some good](#do-some-good) ## Just do it - Believe It Can Be Done - Have Goals - Live Life to Full - Never Give Up - Prepare Well - Have Faith in Yourself - Help Each Other > Whatever your goal is you will never succeed unless you let go of your fears and fly. ## Have fun - Have Fun, Work Hard and Money Will Come - Don't Waste Time - Grab Your Chances - Have a Positive Outlook On Life - When it's Not Fun, Move On > Sometimes, you are just glad to have a job - any job. So you grab the job in the factory or the store or the call center. You might hate it, but you try to make the best of things. But is that fun? I would say do you really have to stay stuck in a rut? Is that job you hate really your only option? Whoever you are, you have other choices. Look around. See what else you can do. ## Be bold - Calculate the Risks and Take Them - Believe in Yourself - Chase Your Dreams and Goals - Have No Regrets (Never look back. You can't change the past. Learn from it) - Be Bold (don't be foolish and *don't gamble on things you cannot control*) - Keep Your Word ## Challenge yourself - Aim high - Try New Things - Always Try - Challenge Yourself ## Stand on your own feet - Rely on Yourself - Chase your Dreams but Live in the Real World - Work Together > If you want milk, don't sit on a stool in the middle of the field in the hope that the cow will back up you ## Live the moment - Love Life and Live It To The full - Enjoy the Moment - Reflect on your Life - Make Every Second Count - Don't Have Regrets > Hard-won things are more valuable than those that come too easily > Regrets weigh you down. They hold you back in the past when you should move on. > Always living in the future can slow us down as much as always looking behind. Many people are always looking ahead and they never seem content. They look for quick fixes, like winning the lottery ## Value family and friends - Put the Family and the Team First (We all need a strong support network) - Be Loyal - Face Problems Head On (best to bring things out into the open) - Money is for Making Things Happen - Pick the Right People and Reward Talent (Even if someone is hired to do one thing, if they have good ideas, or can handle something else, just let them do it) ## Have respect Respect is about how to treat everyone, not just those you want to impress. - Be Polite and Respectful - Do the Right Thing - Keep Your Good Name (You could be rich, but if people didn't trust you, it counted for nothing. *Your reputation is everything*) - Be Fair in All Your Dealings ## Do some good - Change the World, Even if in a Small Way - Make a Difference and Help Others - Do No Harm - Always Think What You Can Do To Help > People in business and the very wealthy are in a unique position. They can connect with everyone, whether high or low, in any country, though a network of good will. ",
    "url": "/learning-notes/books/screw-it-lets-do-it/",
    "relUrl": "/books/screw-it-lets-do-it/"
  },"21": {
    "doc": "Test-Driven Development: By Example",
    "title": "Test-Driven Development: By Example",
    "content": "# [Test-Driven Development: By Example](https://www.goodreads.com/book/show/387190.Test_Driven_Development) - [TDD videos](#tdd-videos) - [Preface](#preface) - [Money session](#money-session) - [Xunit session](#xunit-session) - [Test-Driven Development Patterns](#test-driven-development-patterns) - [Red bar patterns](#red-bar-patterns) - [Red bar patterns](#red-bar-patterns-1) - [Green bar patterns](#green-bar-patterns) - [Design patterns](#design-patterns) - [Refactoring](#refactoring) - [Mastering TDD](#mastering-tdd) ## TDD videos I skipped the TDD sessions in the book, _Money Pattern_ with Java and _implementing xUnit_ with Python, so the summary can focus on the learnings. Some interesting videos on TDD - [TDD intro by Kent Beck](https://www.youtube.com/watch?v=VVSSga1Olt8) - [Making making CoffeeScript by Kent Beck](https://www.youtube.com/watch?v=nIonZ6-4nuU) - [The Three Laws of TDD by Uncle Bob](https://www.youtube.com/watch?v=qkblc5WRn-U) ## Preface Two simple rules: * Write new code only when you first have a failing automated test. * Eliminate duplication The technical implications of these two simple rules: * Design organically * Write your own tests * Provide rapid response to small changes * Design highly cohesive, loosely coupled components, just to make testing easy. Order of programming tasks: 1. **Red**: Write a little test that doesn't work, perhaps doesn't even compiler 2. **Green**: Make the test work quickly, committing whatever sins necessary in the process 3. **Refactor**: Eliminate all the duplication created in just getting the test to work Why would a programmer work in tiny little steps when their mind is capable of great soaring swoops of design? Courage. ### Courage Test-driven development is a way of managing fear during programming, this-is-a-hard-problem-and-I-can't-see-the-end-from-the-beginning. TDD isn't an absolute like Extreme Programming. TDD is an awareness of the gap between decision and feedback during programming, and techniques to control that gap. There are certainly programming task that can't be driven solely by tests, like security software and concurrency. > My goal is to write clean code that works. Imagining a programming world in which all code was this clear and direct, where there were no complicated solutions, only apparently complicated problems begging for careful thought. TDD is a practice that can help you lead yourself exactly that careful thought. ### Story time TDD is a set of techniques any programmer can follow, that encourage simple designs and test suites that inspire confidence. *If you are a genius, you don't need these rules.* If you are dolt, the rules won't help. Two rules: * Write a failing automated test before you write any code. * Remove duplication ## Money session The rhythm of TDD: 1. Quickly add a test 2. Run all the tests and see the new one fail 3. Make a little change 4. Run all tests and see them all succeed 5. Refactor to remove duplication #### Create a test We don't start with objects, we start with tests. When we write a test, we imagine the perfect interface for our operation. We are telling ourselves a store about how the operation will look from the outside. Our story wont' always come true, but better to start from the best possible API and work backwards than to make things complicated, ugly and \"realistic\" from the get go. You'll might end up with multiple compile errors from a single test. It's common to start from the following code when non of the code exists yet. ```java public void testMultiplication() { Dollar five = new Dollar(5); five.times(2); assertEquals(10, five.amount); } ``` We might need a constructor, but it doesn't have to do anything. We might need a stub implementation for methods. **We'll do the least work possible just to get the test to compile.** Once we solve compilation errors, we run the test and we watch it fail. Failure is progress. We've transformed a much broader problem to \"make this test work and make the rest of the tests work\", which is in fact a much simpler and much smaller scope for fear. #### Make it pass The goal right now is not to get the perfect answer, the goal is to pass the test. #### Make it right Dependency is the key problem in software development at all scales. If dependency is the problem, duplication is the symptom. Objects are excellent for abstracting away the duplication of logic. Eliminating duplication in programs eliminates dependency. By eliminating duplication before we go on the next test, we maximize our chance of being able to get the next test running with one and only one change. ### Degenerate objects The general TDD cycle 1. **Write a test.** You are writing a story. Invent the interface you wish you had. Include all the elements int he story that you imagine will be necessary to calculate the right answers. 2. **Make it run.** Quickly getting that bar green dominates everything else. 3. **Make it right.** Remove the duplication you have introduced to get to quick green. The goal is clean code that works. Divide an conquer, solve the \"that works\" part, then solve the \"clean code\". Strategies for quickly getting to green: - **Fake it**, return a constant and gradually replace constants with variables until you have the real code. - **Obvious implementation**, type in the real implementation. When you know what to type, go for the obvious implementation. As soon as you get the unexpected red bar, back up and shift to fake implementation and refactor to the right code. Once confidence is back, go back to obvious implementation. ### Equality for all There is a third strategy, **triangulation**. We briefly ignore the duplication between test and model code. When the second example demands a more general solution, then and only then do we generalize. Use triangulation when you are completely unsure of how to refactor. > Why would I need to write another test to give me permission to write what I probably could have written the first time? Triangulation gives you a chance to think about the problem from slightly different direction. By varying the axes of variability to get a clearer answer. ### Privacy From time to time, our reasoning will fail us and a defeat will slip through. When that happens, we learn our lesson about the test we should have written and move on. ### Equality for all, redux When you don't have enough tests, you are bound to come across refactorings that aren't supported by tests. Write the tests you wish had. If you don't, you'll eventually break something while refactoring. ### Makin' objects When you have two subclasses that aren't doing enough work to justify their existence and you want to eliminate them, you can't do it with one big step. We could be a step closer eliminating the subclasses if there were fewer references to the subclasses directly. We could use Factory Method in the parent class to remove references to the subclasses in the tests. By decoupling the tests from the existence of the subclasses, we have given ourselves freedom to change inheritance without affecting any model code. ### Time we're livin' in Working in tiny steps is a recommendation, if it feels restrictive take bigger steps. If you feel unsure, take smaller steps. TDD is a steering process. ### Interesting times We end up having clean code and tests that give us confidence that the clean code works. Rather than apply minutes of suspect reasoning, we can just ask the computer by making the change and running tests. Without the tests you have no choice, you have to reason. With the tests you can decide whether an experiment would answer the question faster. We'd prefer not to write a test when we have a red bar. We can't change model code without a test. The conservative course is back our the change that caused the red bar so we're back to green. Then we change the test, the implementation and re-try the original change. ### The root of all evil When the object you have doesn't behave like you want, make another object with the same external protocol (an Imposter) but different implementation. TDD can't guarantee that you will have flashes of insight at the right moment. However, confidence-giving tests and carefully factored code give you preparation for insight, and preparation for applying that insight when it comes. ### Change No need for writing tests for `equals()` and `hashCode()` as we are writing those in the context of a refactoring. ### Abstraction, finally For TDD to make economic sense, either you will have to be able to write twice as many lines per day as before, or write half as many lines for the same functionality. You'll have to measure and see effect TDD has on your own practice. Be sure to factor debugging, integrating, and explaining time into your metrics, though. TDD can be used as a way to strive for perfection, but that isn't its most effective use. If you have a big system, the parts that you touch all the time should be absolutely rock solid, so you can make daily changes confidently. ### Retrospective > I like running a code critic. Automated critics don't forget, if you don't delete an obsolete implementation, you don't have to stress. The critic will point it out. The number of changes per refactoring should follow a \"fat tail\". #### Test quality Coverage is certainly not a sufficient measure of test quality, but it is a starting place. Another way of evaluating test quality is defect insertion. The idea is simple, change the meaning of a line of code and a test should break. #### One last review The three items that come up time and again as surprises when teaching TDD are - The three approaches to making a test work cleanly: fake it, triangulate and just typing the right solution to begin with. - Removing duplication between test and code as a way to drive the design. - The ability to control the gap between tests to increase traction when the road gets slippery and cruise faster when conditions are clearer. ## Xunit session ### Invoke test method Lots of refactoring has this feel, separating two parts so you can work on them separately. If they go back together when you are finished, fine, if not, you can leave them separate. Another general pattern of refactoring, take code that works in one instance and generalize it to work in many by replacing constants with variables. Starting from scratch is about the worst possible case for TDD, because we are trying to get over the bootstrap step. Once you mastered TDD, you will be able to work in much bigger leaps of functionality. ### Set the table Patterns inside tests: 1. Arrange, create some objects. 2. Act, stimulate them. 3. Assert, check the results. Two common constraints on tests come into conflict: - *Performance*, we would like to run as quickly as possible and that might mean to share objects between tests. - *Isolation*, we would like the success or failure of one test to be irrelevant to other tests. One test can be simple if and only if another test is in place and running correctly. Simplicity of test writing is more important than performance. ### Cleaning up after Doing a refactoring based on a couple of early uses, then having to undo it soon after is fairly common. ### Retrospective The details of the implementation are not nearly as important as the test cases. ## Test-Driven Development Patterns ### Test _n_ No programmers release even the tiniest change without testing, except the very confident and the very sloppy. Be careful with the positive feedback of not listening to stress and avoiding tests: The more stress you feel, the less testing you'll do. The less testing you do, the more errors you will make. The more errors you make, the more stress you feel. With automated tests, when you start to feel stress, just run the tests. With automated tests you have a chance to choose your level of fear. ### Isolated test **Make the tests so fast to run that you can run them yourself, and run them often. That way you can catch errors before anyone else sees them.** Tests should be able to ignore each other completely. If you have one test broken, you want to have one problem. If you have two tests broken, you want to have two problems. Isolating tests encourages you to compose solutions out of many highly cohesive, loosely coupled objects. > I never knew exactly how to regularly achieve high cohesion and loose coupling until I started writing isolated tests. ### Test list The first part of our strategy for dealing with programming stress is to never take a step forward unless we know where our foot is going to land. What is it we intend to accomplish? One strategy for keeping track of what we're trying to accomplish is to hold it all in our heads. > I tried this for several years, and found I got into a positive feedback loop. The more experience I accumulated, the more things I knew that might need to be done. The more things I knew might need to be done, the less attention I had for what I was doing. The less attention I had for what I was doing, the less I accomplished. The less I accomplished, the more things I knew that needed to be done. Another strategy is to try to plan everything in advance and implement tests _en masse_. Probably the best option is to always test first. Then we can get into a virtuous cycle: When we test first, we reduce the stress, which makes us more likely to test. The immediate payoff for testing, a design and scope control tool, suggests that we will be able to start doing it, and keep doing it even under moderate stress. ### Assert first - Where should you start building a system? With stories you want to be able to tell about the finished system. - Where should you start writing a bit of functionality? With the tests you want to pass with the finished code. - Where should you start writing a test? With the asserts that will pass when it is done. ### Test data Use data that makes the tests easy to read and follow. Don't have a list of 10 items as the input data if a list of 3 items will lead you the same design and implementation decisions. The alternative to Test Data is Realistic Data, where you use data from the real world. Realistic Data is useful when: - You are testing real-time systems using traces of external events - You are matching the output of the current system with output of the previous system. - You are refactoring a simulation and expect precisely the same answers when you are finished. ### Evident data The intent of data should be represented including expected and actual results in the test itself, trying to make their relationship apparent. Remember, you are writing the tests for the reader, not just the computer. ## Red bar patterns If you don't find any test on the list that represents one step, add some new tests that would represent progress towards the items there. ### Starter test The first question you have to ask with a new operation is \"Where does it belong?\" Beginning with a realistic test will leave you too long without feedback. You can shorten the loop by choosing inputs and outputs that are trivially easy to discover. ### Explanation test Ask for and give explanations in terms of tests. ### Another test When a technical discussion is straying off topic with a tangential idea, add a test to the list and go back to the topic. New ideas are greeted with respect, but not allowed to divert the attention. You write them down on the list and get back to what you were working on. ### Regression test When a defect is reported write the smallest possible test that fails, and once it runs, the defect will be repaired. ### Break When you feel tired or stuck take a break. **Shower methodology**: if you know what to type, type. If you don't know what to type, take a shower. TDD is a refinement of that strategy, if you don't know what to type, fake it. If the \"right\" design still isn't clear, triangulate. If you still don't know hat to type, then you can take that shower. ## Red bar patterns - At the scale of a week, weekend commitments help get your conscious, energy-sucking thoughts off work. - At the scale of a year, mandatory vacation policies help you refresh yourself completely. ### Do over When you are feeling lost throw away the code and start over. If you pair program, switching partners is a good way to motivate productive do overs. ### Child test How do you get a test case running that turns out to be too big? Write a smaller test case that represents the broken part of the bigger test case. ### Mock object In order to test an object that relies on an expensive or complicated resource, you create a fake version of the resource that answers constants. Mock objects add a risk to the project, what if the mock doesn't behave like the real object? You can reduce this strategy by having a set of tests for the mock that can also be applied to the real object when it becomes available. ### Self Shunt How do you test one object communicates correctly with another? Have the object under test communicate with the test case instead of the object it expects. We don't need Spy objects. ```python def testNotification(self): self.count = 0 result = TestResult() result.addListener(self) WasRun(\"testMethod\").run(result) assert 1 == self.count def startTest(self): self.count = self.count + 1 ``` Self Shunt tend to read better than tests written without. ### Log string If you want to test that a sequence of messages are called correctly, keep a log in a string, and append to the string when a message is called. Log strings are particularly useful when implementing Observer and you expect notifications to come at certain order. It works well with Self Shunt. ### Crash test dummy If you want to test the flow for an error code, invoke a special object that throws an exception instead of doing real work. ### Broken test How do you leave a programming session when you are programming alone? Leave the last test broken. When you come back to the code, you have an obvious, concrete bookmark to help you remember. ### Clean check-in How do you leave a programming session when you are programming in a team? Leave all the tests running. When you are responsible to your teammates, the picture changes. You don't know in detail what has happened to the code since you saw it last. You need to start from a place of confidence and certainty. ## Green bar patterns ### Fake it ('til you make it) The first implementation once you have a broken test should returning a constant. Once you have the test running, gradually transform the constant into an expression using variables. Having something running is better than not having something running. Effects that make _fake it_ powerful: - **Psychological**, having a green bar feels completely different than having a red bar. - **Scope control**, Starting with one concrete example and generalizing from there prevents you from prematurely confusing yourself with extraneous concerns. ### Triangulate How do you most conservatively drive abstraction with tests? Only abstract when you have two or more examples. Once we have the two assertions and we have abstracted the correct implementation, we can delete one of the assertions on the grounds that it's completely redundant with the other. > I only use triangulation when I'm really, really unsure about the correct abstraction for the calculation ### Obvious abstraction Sometimes you know how to implement an operation. Go ahead. Beware that solving \"clean code\" at the same time you solve \"that works\" can be too much to do at once. ### One to many If you need to implement an operation that works with collection of objects, implement it first without collections and then make it work with them. ### Fixture When you need objects needed by several tests, convert local variables in the tests into instance variables. Sometimes one fixture serves to test several classes. ### External fixtures Release external resources in the `tearDown`. The goal of each test is to leave the world in exactly the same state as before it ran. ### Test method All the tests sharing a single fixture will be methods in the same class. Tests requiring a different fixture will be in a different class. The name of the method should suggest to a future clueless reader why this test was written. > When I write tests, I first create a short outline of the tests I want to write like: > // Adding to tuple spaces > //// Taking a non-existing tuple > // Taking from tuple to spaces ## Design patterns The design patterns book seems to have a subtle bias towards design as a phase. It certainly makes no nod towards refactorign as a design activity. ### Command When we need an invocation to be just a little more concrete and manipulable than a message, objects give us the answer. Make an object representing an invocation. Seed it with all the parameters the computation will need. When we're ready to invoke it, use generic protocol like `run()`. Lambda would work pretty well in this case. ### Value object Objects are better than primitives because they are a great way to organize logic for later understanding and growth. However, there is one little problem. **Aliasing problem**: If two objects share a reference to a third, if one object changes the referred object, the other object better not rely on the state of the shared object. One solution is to never give out the objects that you rely on, but instead to always make copies. Another solution is Observer, where you explicit register with objects on which you rely and expect to be notified when they change. Another solution is to treat the object as less than an object and eliminate the \"that change over time\". Immutability and value is one of the core concepts of value objects. Every mutating operation just returns a new object. It makes reading and debugging so much easier. All value objects have to implement equality. ### Null object When you have a special case using objects, create an object representing the special case. Give it the same protocol as regular objects. ```java SecurityManager security = SecurityManager.getSecurityManager(); if (security != null) { security.checkWrite(path); } ``` to ```java class LaxSecurity { public void checkWrite(String path) { } } class SecurityManager { public static SecurityManager getSecurityManager() { return security != null ? security : new LaxSecurity(); } } ``` Now we don't have to worry about someone forgetting to check for `null`. ```java SecurityManager security = SecurityManager.getSecurityManager(); security.checkWrite(path); ``` ### Template method A way of representing invariant sequence of a computation while providing for future refinement is to write a method that is implemented entirely in terms of other methods. A superclass can contain a method written entirely in terms of other methods, and subclasses can implement those methods in different ways. Template methods are best found through experience instead of designed that way from the beginning. ### Pluggable object The simplest way to express variation is with explicit conditionals ```java if (circle) { } else { } ``` Such explicit decision making beings to spread. ```java class SelectionTool { Figure selected; public void mouseDown() { selected = findFigure(); if (selected != null) select(selected); } public void mouseMove() { if (selected != null) move(selected); else moveSelectionRectangle(); } public void mouseUp() { if (selected == null) selectAll(); } } ``` The answer is to create Pluggable Object, a `SelectionMode` with two implementations `SingleSelection` and `MultipleSelection`. ```java class SelectionTool { Figure selected; SelectionMode mode; public void mouseDown() { selected = findFigure(); if (selected != null) mode = SingleSelection(selected); else mode = MultipleSelection(); } public void mouseMove() { mode.mouseMove(); } public void mouseUp() { mode.mouseUp(); } } ``` ### Pluggable selector If you want to invoke different behaviour for different instances, store the name of a method, and dynamically invoke it. When you have ten subclasses of a class, each implementing only one method, subclassing is a heavyweight mechanism. One alternative is to have a single class with a switch statement. The Pluggable Selector solution is to dynamically invoke the method using reflection ```java void print() { Method runMethod = getClass().getMethod(printMessage, null); runMethod.invoke(this, new Class[0]); } ``` ### Factory method When you want to create an object wanting flexibility creating the new object, create the object in a method instead of using a constructor. Constructor slack expressiveness and flexibility. By introducing a level of indirection, through a method, we gained the flexibility of returning an instance of a different class without changing the test. The downside of Factory Method is precisely its indirection. You'll have to remember that the method is really creating an object. Use it when you need flexibility. Otherwise, constructors work just fine. ### Imposter When you want to introduce a new variation into a computation, introduce a new object with the same protocol as an existing object but with different implementation. Two examples of Imposters that come up frequently during refactoring: - **Null Object**, treat the absence of data the same as the presence of data. - **Composite**, treat a collection of objects the same as a single objects. Finding Imposters during refactoring is driven by eliminating duplication. ### Collecting parameter If you want to collect results spread over several objects, you add a parameter to the operation in which the results will be collected. ### Singleton Avoid providing global variables through singletons, think about the design instead. ## Refactoring Usually, refactoring cannot change the semantics of a program under any circumstances. In TDD, the circumstances we care about are the tests that are already passing. We can replace constants with variables in TDD and call this operation a refactoring because it doesn't change the set of tests to pass. > _Leap of faith_ refactoring is exactly what we're trying to avoid with our strategy of small steps and concrete feedback. ## Mastering TDD ### What don't you have to test? > Write tests until fear is transformed into boredom Unless you have a reason to distrust it, don't test code from others. ### How do you know if you have good tests? The tests are a canary in a coal mine revealing by their distress the presence of evil vapors. Signals that your design is in trouble. - **Long setup code**. If you have to spend a hundred lines creating the objects for one simple assertion, something is wrong. Your objects are too big and need to be split. - **Setup duplication**. If you can't easily find a common place for common setup code, there are too many objects too tightly intertwingled. - **Long running tests**. TDD tests that run a long time won't be run often, and often haven't been run for a while, and probably don't work. Worse than this, this might suggest that testing the bits and pieces of the application is hard, and this is a design problem. - **Fragile tests**. Tests that break unexpectedly suggest that one part of the application is surprisingly affecting another part. ### How does TDD lead to frameworks Paradox: By not considering the future of your code you make your code much more likely to be able to adapt in the future. ### When should you delete tests? Never delete a test if it reduces your confidence in the behavior of the system. If you have two tests that exercise the same path through the code, but they speak to different scenarios for readers, leave them alone. If you have two tests that are redundant with respect to confidence and communication, delete the least useful of the two. ### How does the programming language and environment influence TDD? In programming languages and environments where TDD cycles (test/compile/run/refactor) are harder to come by, you will be likely to be tempted to take larger steps. ### Can you drive development with application-level tests (ATDD)? The problem with driving development with small scale test (\"unit tests\") is that you run the risk of implementing what you think a user wants. If we wrote the tests at the level of the application, then the users could write tests themselves for what exactly they wanted the system to do next. There is a technical problem, how you can write and run a test for a feature that doesn't exist yet? There is a social problem with ATDD, writing tests is a new responsibility for users and organizations resist this kind of shift of responsibility. TDD is a technique that is entirely under your control. Mixing up the rhythm of red/green/refactor, the technical issues of application fixturing, and the organizational change issues surrounding user-written tests is unlikely to be successful. Another aspect of ATDD is the length of the cycle between test and feedback. ### How do you switch to TDD mid-stream? The biggest problem is that code that isn't written with tests in mind typically isn't very testable. \"Fix it\", you say. Yes, well, but refactoring (without automated toos) is likely to result in errors, errors that you won't catch because you don't have the tests. What you don't do is go write tests for the whole thing and refactor the whole thing, that would take months. Parts of the system that don't demand change at the moment we will leave them alone. We have to break the deadlock between tests and refactoring. We can get feedback working very carefully and with a partner. System level tests give us some confidence. ### Who is TDD intended for? If you are happy slamming some code together that more or less works and never looking a the same result again, TDD is not for you. TDD rests on a charmingly naive geekoid assumption that if you write better code, you'll be more successful What's naive is assuming that code is all there is to success. Good engineering is maybe 20% of a project's success. Bad engineering will certainly sink projects, but modest engineering can enable project success as long as the other 80% liens up for it. Those whose souls are healed by the balm of elegance can find in TDD a way to do well by doing good. TDD is also good for geeks who form emotional attachments to code. > My goal is to feel better about a project after a year than I did in the starry-eyed beginning, and TDD helps me achieve this. ### Why does TDD work? Let's assume for the moment that TDD helps teams productively build loosely coupled, highly cohesive systems with low level defect rates and low cost of maintenance profiles. The sooner you find and fix a defect, the cheaper it is. Programmers really do relax, teams really do develop trust, and customers really do learn to look forward to new releases. It's effect is the way it shortens the feedback loop on design decisions. > Adopt programming practices that \"attract\" correct code as a limit function, not as an absolute value. ### What's with the name? One kind of the ironies of TDD is that it isn't a testing technique, it's an analysis technique, a design technique, really a technique for all the activities of development. ### Darach's challenge - You can't test GUIs automaticaly - You can't unit test distributed objects automaticaly - You can't test-first develop your database schema - There is no need to test third party code - You can't test first develop a language compiler / interpreter. ",
    "url": "/learning-notes/books/test-driven-development/",
    "relUrl": "/books/test-driven-development/"
  },"22": {
    "doc": "The Elements of Style",
    "title": "The Elements of Style",
    "content": "# [The Elements of Style](https://www.goodreads.com/book/show/33514.The_Elements_of_Style) - [Elementary rules of usage](#elementary-rules-of-usage) - [Elementary principles of composition](#elementary-principles-of-composition) - [A few matters of form](#a-few-matters-of-form) - [Words and expressions commonly misused](#words-and-expressions-commonly-misused) ## Elementary rules of usage 1. ### Form the possessive singular of nouns with 's. > Charlie's friend > Burn's poems Exceptions are possesives of ancient names in _-es_ and _-is_. > Achilles' heel > Moses' laws Commonly replaced by > The heel of Achilles > The laws of Moses 2. ### In a series of three or more terms with a single conjunction, use a comma after each term except the last. > red, white, and blue > honest, energic, but headstrong In names of businesses firms the last comma is ommited > Brown, Shipley and Company **_etc.,_** even if is a single word it always comes preceded by a comma. 3. ### Enclose parenthesic expressions between commas. > The best way to see a country, unless are pressed for time, is to travel on foot. If the interruption to the flow of the sentence is but slight, the writer may safely omit one or the other comma. > Marjorie's husband, Colonel Nelson paid us a visit yesterday, or > My brother **you will be pleased to hear,** is now in perfect health, 4. ### Place a comma before _and_ or _but_ introducing an independent clause. > The early records of the city have disappeared, and the story of its first years can no longer be reconstructed. > The situation is perilous, but there is still one chance of escape. 5. ### Do not join independent clauses by a comma. The proper mark of punctuation is a semicolon > Stevenson's romances are entertaining; they are full of exciting adventures. > It is nearly half past five; we cannot reach town before dark. It is also correct ro replace semicolons with periods. If a conjuntion is inserted the proper punctuation is a comma. > Stevenson's romances are entertaining, for they are full of exciting adventures. 6. ### Do not break sentences in two. Or basically, don't use periods as commas. 7. ### A participial phrase at the beginning of a sentence must refer to the grammatical subject. > **Walking slowly down the road**, **he saw** a woman accompanied by two children. 8. ### Divide words at line-ends, in accordance with their formation and pronunciation. A. Devide the word according to its formation > know-ledge, de-scribe, atmo-sphere B. Devide on the vowels > edi-ble, propo-sition, ordi-nary C. Devide between double letters > apen-nines, Cincin-nati, refer-ring; but tell-ing ## Elementary principles of composition 1. ### Make the paragraph the unit of composition: one paragraph to each topic. 2. ### As a rule, begin each paragraph with a topic sentence; end it in conformity with the beginning. Ending with a digression, or with an unimportant detail, is particularly to be avoided. 3. ### Use the active voice. The active voice is usually more direct and vigorous than the passive. > **I shall always remember my first visit to Boston.** Is much better than > My first visit to Boston will always be remembered by me. 4. ### Put statements in positive form. Make definite assertions. Avoid tame, colorless, hesitating, non-committal language. > He was not very often on time. > **He usually came late.** > He did not think that studying Latin was much use. > **He thought the study of Latin useless.** It is better to express a negative in positive form. > not honest, **dishonest** > not important, **trifling** > did not remember, **forgot** 5. ### Omit needless words. > the question as to whether, **whether** > there is no doubt but that, **no doubt** > used for fuel purposes, **used for fuel** > he is a man who, **he** 6. ### Avoid a succession of loose sentences. Loose sentences of a particular type, those consisting of two co-ordinate clauses, the second introduced by a conjunction or relative. Although single sentences of this type may be unexceptionable (see under Rule 4), a series soon becomes monotonous and tedious. 7. ### Express co-ordinate ideas in similar form. > The French, the Italians, Spanish, and Portuguese – Bad > **The** French, **the** Italians, **the** Spanish, and **the** Portuguese – Good > In spring, summer, or in winter – Bad > **In** spring, summer, or winter – Good 8. ### Keep related words together. > **Wordsworth**, in the fifth book of The Excursion, **gives a minute description of this church**. – Bad > In the fifth book of The Excursion, **Wordsworth gives a minute description of this church**. – Good > **Cast iron**, when treated in a Bessemer converter, is **changed into steel**. – Bad > By treatment in a Bessemer converter, **cast iron** is **changed into steel**. – Good 9. ### In summaries, keep to one tense. In summarizing the action of a drama, the writer should always use the present tense. In summarizing a poem, story, or novel, he should preferably use the present, though he may use the past if he prefers. 10. ### Place the emphatic words of a sentence at the end. The proper place for the word, or group of words, which the writer desires to make most prominent is usually the end of the sentence. > Humanity **has hardly advanced in fortitude** since that time, though it has advanced in many other ways. – Bad > Humanity, since that time, has advanced in many other ways, but it **has hardly advanced in fortitude.** – Good ## A few matters of form - **Headings**. Leave a blank line, after the title or heading of the manuscript. On succeding pages, begin on the first line. - **Numerals**. Don't spell them, write them in figures or Roman notation. - **Parentheses**. A sentence containing an expression in parenthesis is punctuated, outside of the marks of parenthesis, exactly as if the expression in parenthesis were absent. - **Quotations**. - Formal quotations as documentary evidence with semicolon an quotation marks > The provision of the Constitution is: \"No tax or duty shall be laid on articles exported from any state.\" - Quotations grammatically in apposition or the direct objects of verbs are preceded by a comma with quotation marks > Aristotle says, \"Art is an imitation of nature.\" - Quotations of an entire line, or more, of verse, are begun on a fresh line and centred, but not enclosed in quotation marks. - Proverbial expressions and familiar phrases of literary origin require no quotation marks. - **References**. Give the references in parenthesis or in footnotes, not in the body of the sentence. Omit the words act, scene, line, book, volume, page, except when referring by only one of them. > 2 Samuel i:17-27 > Othello II.iii 264-267, III.iii. 155-161 - **Titles**. Italics with capitalized initials. > The _Iliad_; the _Odyssey_; _As You Like It_; _To a Skylark_; The Newcomes. ## Words and expressions commonly misused - **_All right_**. _Agreed_ or _go ahead_. In other cases better to be avoided. - **_As good or better_**. > My opinion is **as good or better** than his. – Bad > My opinion is **as good as** his, **or better** – Good - **_To whether_**. _Whether_ is sufficient. - **_Bid_**. Takes infinitive without _to_. The past tense is _bade_. - **_Case_**. The word is usually unnecessary. - **_Certainly_**. Bad in speech is even worse in writing. - **_Character_**. Ofter simply redundant. - **_Claim_**. Not to be used as a subsitute for _declare_, _maintain_, or _charge_. - **_Compare_**. _Compare to_ point out resemblances between different objects. _Compare with_ to point out differences in resembling objects. - **_Clever_**. Greatly overused, best to restrict it to small matters. - **_Consider_**. Not followed by _as_ when it means \"believe to be\" - **_Dependable_**. Could be substituted by _reliable_, _trustworthy_. - **_Due to_**. Incorrectly used for _through_, _because of_ or _owing to_. - **_Effect_**. Not to be confused with _affect_ (to influence). Often can be replaced by something else on writing about fashions, music, painting and other arts. - **_Etc._**. Not to be used of persons. It should not left the reader with doubt of the continouty of the serie. - **_Fact_**. Used when verified, not of matters of judment. - **_Factor_**. Usually can be replaced by something more direct and idiomatic. - **_Feature_**. It usually adds nothing to the sentence it occurs. - **_Fix_**. In writing restrict it to its senses, _fasten_, _make firm or immovable_, etc. - **_He is a man who_**. Redundant. - **_However_**. In the meaning of _nevertheless_, not to come first in its sentence or clause. - **_Kind of_**. Not to be used as a replacement for _rather_. Prefer to use _something like_. Same for _sort of_. - **_Less_**. Not to be misused for _fewer_. _Less_ refers to quantity, _fewer_ to number. - **_Line, along these lines_**. Has been overworked. Better to be discarded entirely. - **_Literal, literally_**. Oftern incorrectly used in support of exaggeration or violent metaphor. - **_Lose out_**. Just use _lose_. - **_Most_**. Not to be used for _almost_. - **_Nature_**. Often redundant. - **_Near by_**. _Near_, or _near at hand_ is better. - **_Oftentimes, oftimes_**. Not used, just use _often_. - **_One hundred and one_**. Don't forget the _and_. - **_One of the most_**. Avoid begining paragraphs with it. - **_People_**. _The people_ is a political term, not to be confused with _the public_. Not to be used with words of number. - **_Phase_**. Means a stage of transition. - **_Possess_**. Not to be used as a substitute of _have_ or _own_. - **_Respective, respectively_**. May be ommited. - **_So_**. Avoid using it as an intensifier. - **_State_**. Restrict it to the sense of _express fully or clearly_. - **_Student body_**. Awkard expression. Use _students_. - **_System_**. Frequently used without need. - **_Thanking you in advance_**. Simply write \"Thanking you\". - **_They_**. The intention being either to avoid the awkward \"he or she,\" or to avoid committing oneself to either. Just use \"he\". - **_Very_**. Try to avoid it. - **_Viewpoint_**. Better to use _point of view_. Do not misuse it for _view_ or _opinion_. - **_While_**. Avoid the indiscriminate use of this word for _and_, _but_, and _although_. - **_Whom_**. Often incorrectly used for who before _he said_ or similar expressions. - **_Worth while_**. Try to avoid it. Can be applied for actions. - **_Would_**. A conditional statement in the first person requires _should_, not _would_. > I should not have succeeded without his help. The equivalent of _shall_ in indirect quotation after a verb in the past tense is _should_, not _would_. > He predicted that before long we should have a great surprise. To express habitual or repeated action, the past tense, without would, is usually sufficient, and from its brevity, more emphatic. > Once a year he would visit the old mansion. – Bad > Once a year he visited the old mansion. – Good ",
    "url": "/learning-notes/books/the-elements-of-style/",
    "relUrl": "/books/the-elements-of-style/"
  },"23": {
    "doc": "The Lean Startup",
    "title": "The Lean Startup",
    "content": "# [The Lean Startup](https://www.goodreads.com/book/show/10127019-the-lean-startup) - [Introduction](#introduction) - [Lean Startup method](#lean-startup-method) - [Define](#define) - [Learn](#learn) - [Experiment](#experiment) - [Leap](#leap) - [Test](#test) - [Measure](#measure) - [Pivot (or persevere)](#pivot-or-persevere) - [Accelerate](#accelerate) - [Growth](#growth) ## Introduction > Hard work and perseverance don’t lead to success. It’s the boring stuff that matters the most Startup success can be engineered by following the right process, which means it can be learned, which means it can be taught. We do everything wrong: instead of spending years perfecting our technology, we build a minimum viable product, an early product that is terrible, full of bugs and crash-your-computer-yes-really stability problems. Then we ship it to customers way before it’s ready. And we charge money for it. > I kept having the experience of working incredibly hard on products that ultimately failed in the marketplace. At first, largely because of my background, I viewed these as technical problems that required technical solutions: better architecture, a better engineering process, better discipline, focus, or product vision The business and marketing functions of a startup should be considered as important as engineering and product development and therefore deserve an equally rigorous methodology to guide them. ### Five principles of Lean Startup 1. **Entrepreneurs are everywhere** and the Lean Startup approach can work in any size company, even a very large enterprise, in any sector or industry 2. **Entrepreneurship is management**. “entrepreneur” should be considered a job title in all modern companies that depend on innovation for their future growth. 3. **Validated learning**. Startups exist to learn how to build a sustainable business. This learning can be validated scientifically by running frequent experiments that allow entrepreneurs to test each element of their vision. 4. **Build-Measure-Learn**. Turn ideas into products, measure how customers respond, and then learn whether to pivot or persevere 5. **Innovation accounting**. We need focus on the boring stuff: how to measure progress, how to set up milestones, and how to prioritize work ### Why startups fail? Planning and forecasting are only accurate when based on a long, stable operating history and a relatively static environment. Startups have neither. ## Lean Startup method Many entrepreneurs take a “just do it” attitude, avoiding all forms of management, process, and discipline. Unfortunately, this approach leads to chaos more often than it does to success. Progress in manufacturing is measured by the production of high-quality physical goods. As we’ll see in Chapter 3, the Lean Startup uses a different unit of progress, called **validated learning**. Consider the recommendation that you build cross-functional teams and hold them accountable to what we call learning milestones instead of organizing your company into strict functional departments (marketing, sales, information technology, human resources, etc.) that hold people accountable for performing well in their specialized areas. When people are used to evaluating their productivity locally, they feel that a good day is one in which they did their job well all day. When I worked as a programmer, that meant eight straight hours of programming without interruption. That was a good day. In contrast, if I was interrupted with questions, process, or—heaven forbid—meetings, I felt bad > Code and product features were tangible to me. Learning, by contrast, is frustratingly intangible. **The goal of a startup is to figure out the right thing to build—the thing customers want and will pay for—as quickly as possible**. Every new version of a product, every new feature, and every new marketing program is an attempt to improve this engine of growth ### Lean Startup Instead of making complex plans that are based on a lot of assumptions, you can make constant adjustments with a steering wheel called the **Build-Measure-Learn feedback loop**. Startups also have a true north, a destination in mind: creating a thriving and world-changing business. I call that a startup’s vision. Startups employ a strategy, which includes a business model, a product road map, a point of view about partners and competitors, and ideas about who the customer will be. The product is the end result of this strategy. Less frequently, the strategy may have to change (called a pivot). However, the overarching vision rarely changes. Entrepreneurs are committed to seeing the startup through to that destination. ## Define Entrepreneurial prerequisites: proper team structure, good personnel, a strong vision for the future and an appetite for risk taking. > A startup is a human institution designed to create a new product or service under conditions of **extreme uncertainty** Take a note that it does not say anything about the size of the company. Anything customers experience from their interaction with a company should be considered part of that company's product. _Clayton Christensten The Innovator's Dilemma_: Companies very good at incremental improvements to existing products and serving customers (_sustaining innovation_), but struggle to create breakthrough new products (_disruptive innovation_) that can create new sustainable sources of growth. Innovation is a bottoms-up, decentralized, and unpredictable thing. It does not mean it cannot be managed. Cultivating entrepreneurship is the responsibility of senior management. Leadership requires creating conditions that enable employees to do the kinds of experimentation that entrepreneurship requires. ## Learn As an engineer or manager, people are accustomed to measure progress by making sure work proceeded according to the plan, it is high quality and cost fits projections. As an entrepreneur, this might not be the best way of measuring progress. **What if we found ourselves building something that nobody wants?** If the fundamental goal of entrepreneurship is to engage in organization building under conditions of extreme uncertainty, its most vital function is learning. We must learn what customers really want, not what they say they want or what we think they should want. Validated-learning is not after-the-fact rationalization or a good story designed to hide failure. It's a rigorous method for demonstrating progress. It's a way of demonstrating empirically that a team has discovered valuable truths about a startup's present and future business prospects. The question is not \"Can this product be built?\" but more like \"Should this product be built?\" or \"Can we build a sustainable business around this set of products and services?\". **Everything a startup does is understood to be an experiment designed to achieve validated learning.** ## Experiment **If the plan is to see what happens, a team is guaranteed to succeed – at seeing what happens – but won't necessarily gain validated learning**. If you cannot fail, you cannot learn. A true experiment follows the scientific method. It begins with clear hypothesis that makes predictions and it then tests predictions empirically. Startup experimentation is guided by startup's vision. The goal of startups is to discover how to build a sustainable business around that vision. The two most important assumptions entrepreneurs make are: - The _value hypothesis_ tests whether a product or service really delivers value to customers. Experiments provide a more accurate gauge. - The _growth hypothesis_ tests how new customers will discover a product or service. The point is not to find the average customer but to find _early adopters_. Those customers tend to be more forgiving of mistakes and are especially eager to give feedback. ### An experiment is a product Good questions for the team to answer why developing product: - Do consumers recognise that they have the problem you are trying to solve? - If there was a solution, would they buy it? - Would they buy it from us? - Can we build a solution for that problem? > **Success is not delivering a feature; success is learning how to solve the customer's problem.** # How vision leads to steering - **Qualitative feedback**: What customers like and don't like. - **Quantiative feedback**: How many customers use the product and find it valuable. The riskiest elements of a startup's plan are the _leap-of-faith_ assumptions. The two most important ones are the **value hypothesis** and the **growth hypothesis**. Once clear on these leap-of-faith assumptions the first step is to enter the Build phase as quickly as possible with a minimum viable product (MVP). **MVP**: Is the version of the product that enables a full turn of the _Build-Measure-Learn_ loop with the minimum amount of effort and the least amount of development time. Lacks many features that may prove essential later on. **We must be able to measure its impact**. We also need to get in front of potential customers to gauge their reactions. The biggest challenge once we get into the _Measure_ phase is determining if the product development efforts are leading to real progress. We do this by a method called _**innovation accounting**_. Innovation accounting is a quantitative approach that allow us to create _learning milestones_ instead of the traditional business and product milestones. Upon completing the _Build-Measure-Learn_ loop we confront the most difficult question any entrepreneur faces: whether to **pivot** the original strategy or persevere. **Planning works in the reverse order: we figure out what we need to learn, use innovation accounting to figure out what we need to measure to know if we are gaining validated learning, and then figure out what product we need to build to run that experiment and get that measurement**. ## Leap ### Strategy is based on assumptions Every business plan begins with a set of assumptions. A startup early efforts should be to test them as quickly as possible. The first challenge for an entrepreneur is to build an organization that can test these assumptions systematically. The second challenge is to perform that rigorous testing without losing sight of the company's overall vision. What differentiates the success stories from the failures is that the successful entrepreneurs had the foresight, the ability, and the tools to discover which part of their plans were working brilliantly and which were misguided, and adapt their strategies accordingly. ### Growth The first step in understanding a new product or service is to figure out if it is fundamentally value-creating or value-destroying. It's essential that entrepreneurs understand the reasons behind a startup's growth. There are many value-destroying kinds of growth that should be avoided (a business that grows through continuos fund-raising from investors and lots of paid advertising but does not develop a value-creating product). ### Design and customer archetype The goal of the early contact with customers is not to gain definitive answers but to clarify at a basic level that we understand our potential customer and what problems they have. **The archetype is a brief document that represents a target customer and serves as a guide for product development.** An archetype is an hypothesis, not a fact. ### Analysis paralysis There are two ever-present dangers: - **Just-do-it** school of entrepreneurship are impatient to get started and don't want to spend time analyzing their strategy. - Others fall victim to **analysis paralysis**, talking to customers, reading research reports and whiteboard strategizing are all equally unhelpful. How do entrepreneurs know when to stop analyzing and start building? The answer is a concept called Minimum Viable Product. ## Test An **MVP** is simply the fastest way to get through the _Build-Measure-Learn_ feedback loop with the minimum amount of effort. The goal of an MVP is to begin the process of learning, not end it. Unlike a prototype or concept test, an MVP is designed not just to answer a product design or technical questions but to **test fundamental business hypothesis**. ### First products aren't mean to be perfect Before new products can be sold successfully to the mass market, they have to be sold to early adopters. ### The concierge minimum viable product This is a personal in-home visits and manual works (no software) just to learn as fast as possible. In a concierge MVP, the personalized service is not the product but the learning activity designed to test the leap-of-faith assumptions in the company's growth model. **Wizard of Oz test**: Customers believe they are interacting with the actual product but behind the scenes human beings are doing the work. ### The role of quality and design in an MVP It challenges the traditional notions of quality. > If we do not know who the customer is, we do not know what quality is. MVPs require the courage to put one's assumptions to the test. **The Lean Startup method is not opposed to building high-quality products**, but only in service of the goal of winning over customers. This does not mean operating in a sloppy or undisciplined way. There is a category of quality problems that have a net effect of slowing down the _Build-Measure-Learn_ feedback loop. Defects make it more difficult to evolve the product. They actually interfere with our ability to learn and so are dangerous to tolerate in any production process. Instead of reducing the quality, reduce the scope: remove any feature process, or effort that does not contribute directly to the learning seek. ## Measure A startup job is to: 1. Rigorously measure where it is right now. 2. Devise experiments to learn how to move the real numbers closer to the ideal reflected in the business plan. ### Accountability framework It begins by turning the leap-of-faith assumptions into quantitative financial model. **The rate of growth depends primarily on three things: the profitability of each customer, the cost of acquiring new customers, and the repeat purchase rate of existing customers.** ### Three learning milestones First, use an MVP to establish real data on where the company is right now. Second, startups must attempt to tune the engine from the baseline towards the ideal. Then the company reaches a decision point. Third: Pivot or persevere. If the company is making good progress toward the ideal, that means it's learning appropriately and using that learning effectively. ### Establish the baseline Smoke tests (ability to preorder a product that is yet to be built) with its marketing materials. It will measure if customers are interested in trying a product. These MVPs provide the first example of _learning milestone_. When one is choosing among the many assumptions in the business plan, it makes sense to test the riskiest assumptions first. ### Tuning the engine Every product development, marketing, or other initiative that a startup undertakes should be targeted at **improving one of the drivers of its growth model**. Remember that a good design is one that changes customer behavior for the better. ### Pivot or persevere If we are not moving the drivers of our business model, we are not making progress. That becomes a sure sign that it's time to pivot. ### Improving a product on five dollars a day > We tracked the \"funnel metrics\" behaviours that were critical to our engine of growth. Five dollars bought us a hundred clicks every day. From a marketing point of view this was not very significant, but for learning it was priceless. Every single day we were able to measure our product's performance with a brand new set of customers. Each day was a new experiment. ### Cohort analysis Instead of looking at cumulative totals or gross numbers such as total revenue and total number of customers, one looks at the performance of customers that comes into contact with the product independently. > Out of frustration of not knowing what to do I was finally ready to turn to the last resort: talking to customers. I was ready to ask the right questions. ### Optimization versus learning Engineers, designers and marketers are all skilled at optimization. A startup has to measure progress against a high bar: evidence that a sustainable business can be built around its products or services. This can only be assessed only if a startup has made clear, tangible predictions ahead of time. ### Actionable metrics versus vanity metrics How do we know which features to prioritize? How can we get more customers to sign up and pay? How can we get out the word about our product? ### Cohorts and split-tests > Instead of looking at gross metrics, _Grockit_ switched to cohort-based metrics, and instead of looking for cause-and-effect relationships after the fact, _Grockit_ would launch each new feature as a true split-test experiment. Many features that make the product better in the eyes of engineers and designers have no impact on customer behaviour. ### The value of the three a's Three A's of metrics: - **Actionable**: It must demonstrate clear cause and effect. - **Accessible**: Reports as simple as possible so everyone understands them. Use tangible, concrete units. Cohort-based reports are the gold standard of metrics: they turn complex actions into people reports. Reports deal with people and their actions. Accessibility also refers to widespread access to the reports. - **Auditable**: The data is credible to employees. The loser of the argument would challenge the veracity of the data. **We need to test the data by hand by talking to customers. This is the only way to be able to check if the reports contain true facts.** This also has the benefit to giving insight into why customers are behaving the way the data indicate. ## Pivot (or persevere) Pivot: A structured course correction designed to test a new fundamental hypothesis. The heart of the scientific method is the realization that although human judgment may be faulty, we can improve our judgment by subjecting to theories to repeated testing. Startup productivity is not about cranking out more widgets or features. It is about aligning our efforts with a business and product that are working to create value and drive growth. **The problem with the notion of shipping a product is that you are guaranteed to succeed at seeing what happens.** Direct contact with customers proved essential. A startup's runway is the number of pivots it can still make. ### Pivots require courage Entrepreneurs need to face their fears and be willing to fail, often in a public way. Run away from vanity metrics as they form false conclusions and create an alternate private reality. #### The pivot or persevere meeting **The decreasing effectiveness of product experiments and the general feeling that product development should be more productive. Whenever you see those symptoms, consider a pivot.** #### Catalog of pivots - **Zoom-in pivot**: What previously was considered a single feature in a product becomes the whole product. - **Zoom-out pivot**: Sometimes a single feature is insufficient to support the whole product. - **Consumer segment pivot**: The company realizes that the product it is building solves a real problem for real customers but that they are not the type of customers it originally planned to serve. - **Customer need pivot**: It sometimes becomes clear that the problem we're trying to solve for them is not very important, we discover other related and more important problems. - **Platform pivot**: From an application to a platform or vice versa. - **Business architecture pivot**: Two major business architectures: High margin, low volume (complex systems) or low margin, high volume (volume operations model). - **Value capture pivot**: Capture the value of a company creates. Monetization or revenue models. Capturing value is an intrinsic part of the product hypothesis. - **Engine of growth pivot**: Three primary engines of growth that power startups: the viral, sticky and paid growth models. - **Channel pivot**: The mechanism by which a company delivers its product to customers is called the sales channel or distribution channel. A channel pivot is a recognition that the same basic solution could be delivered through a different channel with greater effectiveness. _It is precisely because of its destructive effect on sales channels that the Internet has had such a disruptive influence in industries that previously required complex sales and distribution channels._ - **Technology pivot**: A company discovers a way to achieve the same solution by using a completely different technology. The only question is whether the new technology can provide superior price and/or performance compared with existing technology. #### A pivot is a strategic hypothesis A pivot is better understood as a new strategic hypothesis that will require a new minimum viable product to test. Even after a company achieves initial success, it must continue to pivot. It's a special kind of structured change designed to test a new fundamental hypothesis about the product, business model, and engine of growth. ## Accelerate How much time and energy should companies invest in infrastructure and planning early on in _anticipation_ of success? Spend too much and you waste precious time that could have been spent learning. Spend too little and you may fail to take advantage of early success and cede market leadership to a fast follower. Traditional departments create incentive structures that keep people focused on excellence in their specialities: marketing, sales, product development. The critical first question for any lean transformation is: which activities create value and which are a form of waste? What products do customers really want? How will our business grow? Who is our customer? Which customers should we listen to and which should we ignore? These are the questions that need answering as quickly as possible to maximize a startup's chances of success. Today's companies must learn to master a management portfolio of sustainable and disruptive innovation. ### Batch > Daddy, first you should fold all of the newsletters. Then you should attach the seal. Then you should put the stamps. Their father wanted to do it the counterintuitive way: complete each envelope one at a time. They - like most of us - thought that was backward \"That wouldn't be efficient!\" **Doing it one at a time is faster**. Intuition does not take into account the extra time to sort, stack and move around the large piles of half-complete envelopes when it's done the other way. In process-oriented work like this, individual performance is not nearly as important as the overall performance of the system. The small-batch approach produces a finished product every few seconds, whereas the large-batch approach must deliver all the product at once, at the end. What if it turns out that the customers have decided they don't want the product? Which process would allow a company to find this out sooner? Lean manufacturers discovered the benefits of small batches ago. Innovators as Taiichi Ohno, Shigeo Shingo and others found a way to succeed by using small batches. Instead of buying large specialized machines that could produce thousands of parts at a time, Toyota used smaller general-purpose machines that could produce a wide variety of parts in small batches. Because of its smaller batch size, Toyota was able to produce a much greater diversity of products. **The biggest benefit of working in small batches is that quality problems can be identified much sooner.** This is the origin of Toyota's famous _andon_ cord, which allows any worker to ask for help as soon as they notice any problem, such a defect in physical part, stopping the entire production line if it cannot be corrected immediately. It can interrupt the flow as the line is halted repeatedly. The benefits of finding and fixing problems faster outweigh this cost. This process of continuously driving out defects has been a win-win for Toyota and its customers. #### Small batches in entrepreneurship In the Lean Startup the goal is not to produce more stuff efficiently but to learn how to build a sustainable business. **as quickly as possible**. Working in small batches ensures that a startup can minimize the expenditure of time, money, and effort that ultimately turns out to have been wasted. Nowadays large batches are still the rule. The work that goes into the development of a new product proceeds on a virtual assembly line. Product managers figure out what features are likely to please customers; product designers then figure out how those features should look and feel. These designs are passed to engineering, which builds something new or modifies an existing product and, once this is done, hands it off to somebody responsible for verifying that the new product works the way the product managers and designers intended Instead of working in separate departments, engineers and designers would work together side by side on one feature at a time. Whenever that feature was ready to be tested with customers, they immediately would release a new version of the product, which would go live on our website for a relatively small number of people. #### Continuous deployment beyond software - **Hardware becoming software.** What can be built out of software can be modified much faster than a physical or mechanical device can. - **Fast production changes**. When the design changes, there is no excess inventory of the old version to slow things down. Since machines are designed for rapid changeovers, as soon as the new design is ready, new version can be produced quickly. - **3D printing and rapid prototyping tools**. By reducing the batch size, we can get through the Build-Measure-Learn feedback loop more quickly than our competitors can. ## Growth ### Where does growth come from? Sustainable growth is characterized by one simple rule: New customers come from the actions of past customers. - **Word of mouth**. Natural level of growth that is caused by satisfied customers' enthusiasm for the product. - **As a side effect of product usage**. When you see someone dressed int he latest clothes or driving a certain car, you may be influenced to by that product. - **Through funded advertising**. As long as the cost of acquiring a new customer (the so-called marginal cost) is less than the revenue of that customer generates (the marginal revenue), the excess (the marginal profit) can be used to acquire more customers. The more marginal profit, the faster growth. - **Through repeat purchase or use.** Some products are designed to be purchased repeatedly either through a subscription plan or through voluntary repurchases. ### The three engines of growth There are always a zillion new ideas about how to make the product better floating around, but the hard truth is that most of those ideas make a difference only at the margins. They are mere optimizations. Startups have the focus on the big experiments that lead to validated learning. #### The sticky engine of growth Products are designed to attract and retain customers for the long term. There is an expectation that once you start using their product, you'll continue to do so. Companies using the sticky engine of growth track their attrition rate or churn rate very carefully. The churn rate is defined as the fraction of customers in any period who fail to remain engaged with the company's product. The rules that govern the sticky engine of growth are pretty simple: if the rate of new customer acquisition exceeds the churn rate, the product will grow. The speed of growth is determined by what I call the rate of compounding, which is simple the natural growth rate minus the churn rate. Example: 61% retention rate, 31% growth rate, compounding growth rate of just 0.02%, almost zero. #### The viral engine of growth The viral engine is powered by a feedback loop that can be quantified. It is called the _viral loop_, and its speed is determined by a single mathematical term called the _viral coefficient_. The higher this coefficient is, the faster the product will spread. How many friends will each customer bring with him or her? A viral loop with a coefficient that is greater than 1.0 will grow exponentially. Many viral products do not charge customers directly but rely on indirect sources of revenue such as advertising. This is the case because viral products cannot afford to have any friction impede the process of signing customers up and recruiting their friends. #### The paid engine of growth If either company wants to increase its rate of growth, it can do so in one of two ways: increase the revenue from each customer or drive down the cost of acquiring a new customer. The paid engine of growth is powered by a feedback loop. Each customer pays a certain amount of money for the product over his or her \"lifetime\" as a customer. Once variable costs are deducted, this usually is called customer _lifetime value_ (LTV). This revenue can be invested in growth by buying advertising. Example: If the CPA remains at $2 but the LTV falls bellow $2, the company's growth will slow down. It may make up the difference with one-time tactics such as using invested capital or publicity stunts, but those tactics are not sustainable. ### Technical caveat More than one engine of growth can operate in a business at a time. It's strongly recommended to focus one engine at a time. What really matters is not the raw numbers or the vanity metrics but the direction and degree of progress. ",
    "url": "/learning-notes/books/the-lean-startup/",
    "relUrl": "/books/the-lean-startup/"
  },"24": {
    "doc": "The Manager’s Path: A Guide for Tech Leaders Navigating Growth and Change",
    "title": "The Manager’s Path: A Guide for Tech Leaders Navigating Growth and Change",
    "content": "# [The Manager's Path: A Guide for Tech Leaders Navigating Growth and Change](https://www.goodreads.com/book/show/33369254-the-manager-s-path) - [Management 101](#management-101) - [Mentoring](#mentoring) - [Tech lead](#tech-lead) - [Managing people](#managing-people) - [Managing a team](#managing-a-team) - [Managing multiple teams](#managing-multiple-teams) - [Managing managers](#managing-managers) - [The big leagues](#the-big-leagues) - [Bootstraping culture](#bootstraping-culture) Engineering management is not just about people management. Hands-on expertise gives you credibility and helps leading your team effectively. ## Management 101 You should expect from your manager ### One-on-one meetings **1-1s allow human connection.** You should let your manager know about your life a bit, it would be easier to ask for time off when times become stressful. Great managers notice your energy level. Being an introvert is not an excuse to avoid treating people like human beings. Your manager should treat you like a human that has a life outside work, you should expect to talk a few minutes about that life when you meet. **It is an opportunity to speak privately.** You should expect 1-1s to be scheduled with some predictability, so that you can plan for them. Good 1-1s are not status meetings. You can use the email or chat for those. It's a good idea to come with an agenda of things to discuss. ### Feedback and workplace guidance Good managers will let you know when you screw up. The sooner you know about your bad habits the easier they will be to correct. **Praise should be public and criticism should be private.** If you do things apart from code, your manager should help you out to improve in those things. Asking for advice is a great way to show respect to her. **Your manager is your number one ally.** In difficult situations with your teammate or people from other teams, your manager should be there to help you navigate the situation. Good managers will also help you understand the value of the work you are doing when is not fun or glamourous. The more senior you become, the less feedback you will be likely to receive. You should become comfortable driving your 1-1s. ### Training and career growth Your manager holds some responsibility helping you out with training and other resources for career growth. Your manager should be essential in advocating for your promotion and getting approved. She should also know if you are qualified to be promoted. Managers cannot guarantee promotions, but good ones will know how to help you out to perform well in the system. ### You are responsible for yourself Knowing yourself is step one. Step two is going after what you want. When you want to work on a project, ask. When you are persistently unhappy, speak out. You will not get everything you want, and asking is not fun, but it's the fastest way forward. ### Give your manager a break **The job of your manager is to do the best thing for the company and the team, not to do whatever it takes to make you happy.** The only person you can change is yourself. Your manager expect you to bring solutions, not problems. ### Choose your managers wisely Consider not only the job, the company and the pay, but also the manager when evaluating job opportunities. Strong managers have strong networks, and they can get you jobs even after you stop working for them. ## Mentoring ### Being a mentor It is an opportunity in a safe way about the job of management, and the feeling of being responsible for another person. The mentee gets a supervisor that is solely focused on him. #### Mentoring an intern You want him to love you as he will go to his friends and tell about you and the company he worked for. You will have to have a project in mind, look for small features of your current project that would take a few days to complete and start from there. Try to sit with him as much as possible the first days. * **Listen carefully.** Listening is one of the basic skills for people management. Listening is the precursor of empathy, which is one of the core skills of a quality manager. When you mentee is speaking to you, pay attention to your behaviour. Listening goes beyond hearing words, people are not usually precise on what they mean. Let your mentee correct you. * **Communicate clearly.** Communicate what needs to happen. * **Calibrate your response.** During the first weeks of the internship, you'll learn the frequency you need to check in with your mentee to provide the right adjustments. #### Mentoring a new hire Your job consists of onboarding this person in the company effectively, helping out adjusting to the life in the company and building out her network of contacts. There will be many opportunities to clarify many bits of process, culture and jargon that will be completely foreign to a new joiner. Effective teams have good onboarding documents to provide to new hires. Part of the mentoring is introducing the new person around, bringing this person into some of your networks will help her to get up to speed faster. Adopt the mindset that network building is a worthwhile investment of your time and energy. #### Technical career mentoring The best mentoring relationships evolve naturally and in the context of larger work. * **When you are a mentor.** Don't do it unless you think it will be rewarding for you and the person you're mentoring. Don't say yes and then fail to actually do the mentoring work. * **When you are a mentee.** Think about what you want to get out of this relationship, and come prepared to your sessions. If you don't have time to prepare or you don't think preparing is necessary, ask yourself if mentoring is really something you need at all. Maybe instead you need a friend, or a therapist, or a coach. ### Good manager, bad manager: the alpha geek The alpha geek is driven to be the best engineer on the team, to always have the right answer, and to be the person who solves all the hard problems. He values intelligence and technical skill above all other traits. Can't deal with dissent. He has all the answers. **The alpha geek tries to create a culture of excellence, but ends up creating a culture of fear.** The alpha geek believes that every developer should know exactly what she knows, and if you don't know something, she will point out your ignorance. She can be very rigid about how things should be done and closed off to new ideas that she didn't come up with. **If you have ever wondered why people don't seem to come to you for help despite your clearly strong technical skills, ask yourself whether you're showing some signs of being an alpha geek.** Mentoring can be a great opportunity to break out of that habit. **Practicing the art of teaching can help us learn how to nurture and coach, how to phrase things so that others will listen, instead of just shouting them down.** **Alpha geeks make absolutely terrible managers.** Better off to give them a focus on technical strategies and system design. ### Tips for the manager of a mentor What you measure, you improve. You help your team succeed by creating clear, focused, measurable goals. Figure out what you're hoping to achieve then find the person who can help meet those goals. If your company is setting up mentoring programs, make sure that there is some guidance and structure. Recognise that the mentor productivity will slow down during the mentoring period. Look for someone that you believe can succeed who wants to distinguish herself beyond her coding ability. Skills that address emotional needs of people and teams. Because the outcome can be difficult to quantify is often dismissed as less important. Mentors need to be recognised, should be treated as first-class citizens. Best mentors are going to be people who are further along in their mastery of the job skills that the mentee is trying to develop. Use this opportunity to reward and train future leaders of your team. ### Key takeaways for the mentor * **Be curious and open-minded.** When we close our minds and stop learning, we start to lose the most valuable skill for maintaining and growing a successful technical career. Working with new people who are learning things for the first time can shed light on hidden patterns and help you make connections you may not otherwise have made. * **Listen and speak their language.** Mentoring forces you to hone your communication skills. You must be able to listen and communicate in a way that the person can understand. Teams have to communicate effectively to get anything done. * **Make connections**. Your career ultimately succeeds or fails on the strength of your network. Mentoring is a great way to build this network. Your career is long and the tech world can be very small, so treat the other person well. ## Tech lead The idea that the tech lead role should automatically be given to the most experienced engineer, the one who can handle the most complex features or who writes the best code, is a common misconception that even experienced managers fall for. _The tech lead role is not a point in the latter, but a set of responsibilities that any engineer may take once they reach the senior level._ * Regular (weekly) 1-1 touchbases * Regular feedback on career growth, progression towards goals, areas for improvement, and praise as warranted. * Working with reports to identify areas for learning and helping them grow in these areas via project work, external learning, or additional mentoring. The tech lead is learning how to be a strong technical project manager by delegating work effectively without micromanaging. They focus on the whole team's productivity and strive to increase the impact of the team's work product. It is very hard to grow past senior engineer 2 without ever having acted as a tech lead._ You can't lead without engaging with other people, and people skills should be important, much more than technical expertise. **Tech leads will work on one major new technical skill: project management, or the work of breaking down a project.** > Being a tech lead is an exercise in influencing without authority. How do I empower my team? How do I remove the obstacles slowing them down? ### All great tech leads know this one weird trick The willingness to step away from the code and figure out how to balance your technical commitments with the work the whole team needs. You'll often need to balance doing things you know how to do and enjoy doing, such as writing code, with things you don't know how to do. You'll feel uncomfortable about it. The worst scheduling mistake is allowing yourself to get pulled randomly into meetings. It's important to get your team into a schedule that allows them to be focused on development for long stretches of time. **Part of your leadership is helping the other stakeholders, such as your boss and the product manager, respect the team's focus.** ### Being a lead 101 #### The main roles of a tech lead * **System architect and business analyst.** Have a good sense of the overall architecture of your systems and a solid understanding of how to design complex software. Understand business requirements and translate them into software. * **Project planner.** Breaking down work into rough deliverables. Part of the challenge here is getting as much productive work done in parallel as possible. * **Software developer and team leader.** Software developers and leaders write code, communicate challenges and delegate. In your position as a tech lead, you should _continue writing code, but not too much_. Start looking for opportunities to _delegate work_. You have to act as a software developer, a system architect, a business analyst, and a team leaders who knows when to do something single-handedly, and when to delegate work to others. ### Managing projects When faced with a project with many unknowns and relatively hard deadlines, you'll find agile processes tricky. You need to understand how to break down work that has complexity beyond the scope of what you can do as an individual. Hiring for project management is bad, but still project management has to happen. As a tech lead, you should be doing it when needed, especially for deeply technical projects. **The value of planning isn't that you execute the plan perfectly, that you catch every detail beforehand, or that you predict the future; it's that you enforce self-discipline to think about the project in some depths before diving in and seeing what happens.** > Take time to explain. Never hesitate to take the opportunity to explain basics and motivations to senior or junior members. It educates them without making them feel small. ### Managing a project 1. **Break down the work.** Start breaking down your big deliverables into tasks. 2. **Push through the details and the unknowns.** Work through the unknowns until you really feel that there is no more value to be gained in spending time on them. 3. **Run the project and adjust the plan as you go.** The value of planning is that it helps you know how far the project has come, and how far it is from completion. 4. **Use the insights gained in the planning process to manage requirement changes.** 5. **Revisit the details as you get close to completion.** Run a _premortem_, an exercise where you run through all the things that could fail on the launch of the projects. Make a launch plan; make a rollback plan. Celebrate. ### Good manager, bad manager: the process czar The process czar believes that there is is one true process that will solve all the team's biggest problems. Agile, Kanban, scrum, Lean, or even waterfall. Engineers sometimes turn into process czars when they become tech leads, seeking out the right tool to solve all issues with planning, focus, time management, and prioritisation. Processes must meet the needs of the team and the work. **Be careful on relying on processes to solve problems that are a result of communication or leadership gaps on your team.** It's a waste of your time to play rule cops, and automation can often make the rules more obvious. ### How to be a great tech lead * **Understand architecture.** Take time to understand it. It's almost impossible to lead projects well when you don't understand the architecture you're changing. * **Be a team player.** If you're doing all the interesting work yourself, stop. Working on the less exciting, boring or frustrating parts of the code base can teach you a lot about where the process is broken and how to fix it. Also, if you are doing the most boring work, stop that too. You want to encourage others on your team to learn the entire system. * **Lead technical decisions.** Determine which decisions must be made by you, which decisions should be delegated to others with more expertise, and which decisions require the whole team to resolve. Make it clear what the matter under discussion is, and communicate the outcome. * **Communicate.** Your productivity is now less important than the productivity of the team. You will pay the price of communication overhead, you represent the team. If one universal talent separates successful leaders it's communication skills. Successful leaders write well, they read carefully, and the get up in front of a group and speak. Don't forget to listen and give others a chance to speak. If you aren't a good note taker, you may need to become one. ## Managing people Your team is only as healthy as its individuals. The main tasks required to manage people: * Taking on a new report * Holding regular 1-1s * Giving feedback on career growth, progression toward goals, areas for improvement and praise as warranted * Working with reports to identify areas for learning and helping them grow in these areas via project work, external learning, or additional mentoring. ### Starting a new reporting relationship off right * **Build trust and rapport**. Get to know each other. * **Create a 30/60/90 day plan**. This plan includes basic goals. The more senior the hire, the more he should participate in creating the plan. * **Encouraging participation by updating new hire documentation**. Team onboarding documents get updated on every new hire to get up to speed. * **Communicate your style and expectations**. How often will you meet him, how will you share information, and when and how often you'll want to review his work. * **Get feedback from your new hire**. Get as much feedback as you can about the new hire's perspective on the team in that first 90 days. ### Communicating with your team * **Have regular 1-1s**. * **Scheduling 1-1s**. The default should be weekly. Try to avoid as much as possible interrupting your reports in the middle of productive workflow hours. * **Adjusting 1-1s**. Adapt the cadence of your 1-1s, junior people or people who need frequent feedback or help on difficult times will appreciate more dedicated time. ### Different 1-1 styles #### The to-do list meeting One or both parties comes with a list of goals to cover in order of importance. This style is very professional and efficient, but sometimes is a bit cold. It forces reports to think beforehand what do they want to discuss. #### The catch up Listen to anything your reports want to discuss. Let them drive the meeting. It should be as much as a creative discussion as a planning meeting. **If you start focusing a lot of energy on hearing reports' complaints and commiserating, you're quite possible making the problem worse. There is very little value to repeatedly focusing on drama.** #### The feedback meeting Quarterly is frequent enough to give the topic attention without feeling like all you talk about is career development. **Progress towards goals, whether they are formal or personal.** If you have an employee with performance issues, feedback meetings should happen more frequently, if you are thinking of firing someone I advise you to document these feedback meetings. When someone does something that needs immediate corrective feedback, don't wait for the 1-1 to provide that feedback. The longer you wait, the harder it will be for you to bring it up, and the less effective the feedback will be. #### Getting to know you Leave room to get to know the person reporting to you as a human being. Show them that you care about them as individuals. Show that you are invested in helping them now and in the future. #### Mix it up Remember that when you're not taking notes you'll probably forget some important things. Do your 1-1 meetings in private so that you can feel free to discuss sensitive topics. **Tip: For each report, keep a shared document as a log of the things that you discuss in your 1-1s.** ### Good manager, bad manager: micromanager, delegator The hardest thing about micromanagement is that there are times when you need to do it. Trust and control are the main issues around micromanagement. _Autonomy_ is an important element of motivation. _Delegation is not the same thing as abdication_. When you're delegating responsibility, you're still expected to be involved as much as is necessary to help the project succeed. ### Practical advice for delegating effectively Being a good leader means being good at delegating. * **Use team's goals to understand which details you should dig into.** If the team is making progress on its goals, the systems are stable, and the product manager is happy, I rarely dig into the details beyond cursory overview. However, it requires goals with a plan for people to be making progress against, and a product manager who can give you another perspective. **When you are managing a team that doesn't have a clear plan, use the details you'd want to monitor to help them create one.** * **Gather information from the systems before going to the people.** The worst micromanagers are those who constantly ask for information they could easily get themselves. Use a light touch. The team will not be productive or happy spending half of their time gathering information for you that you could easily find yourself. * **Adjust your focus depending on the stage of projects.** You should know all of the details of the project status as part of your regular team process. * **Establish standards for code and systems.** * **Treat the open sharing information, good or bad in a neutral to positive way.** If you don't figure out how to let go of details, delegate, and trust your team. ### Creating a culture of continuous feedback Continuous feedback is a commitment to regularly share both positive and corrective feedback. Raise issues as they happen. Steps you can take to be great at continuous feedback: 1. **Know your people.** What are their goals, strengths and weaknesses and what do they need to do to get to the next level. 2. **Observe your people.** Good managers have an intuition for identifying talents and helping people draw more out of their strengths. Lookout for things to praise. Every week there should be at least one thing you can recognise about someone in your team. 3. **Provide lightweight, regular feedback.** Start with positive feedback. 4. **Bonus: provide feedback.** When things are going well, praise them, but also **make suggestions as to what could be even better in the future**. Going beyond a simple \"good job\" to really help your people grow. ### Performance reviews Continuous feedback is not a replacement for a more formal, 360-based performance review process. The _360 model_ is a performance review that includes feedback from, in addition to a person's manager, his teammates, anyone who reports to him, and coworkers he regularly interacts with, **as well as a self-review**. Your job as a manager is to gather all that feedback together and summarise it into a high-level view of what other people think about your direct reports. #### Writing a performance review * **Give yourself enough time, and start early.** Spend solid, uninterrupted time working on reviews. * **Try to account for the whole year, not just the past couple months**. Keep a running summary of your 1-1s. * **Use concrete examples, and excerpts from peer reviews.** Forcing yourself to be specific will steer you away from writing reviews based on underlying bias. * **Spend plenty of time on accomplishments and strengths.** Celebrate achievements, talk about what's going well, and give plenty of praise for good work. Those strengths are what you'll use to determine when people should be promoted. * **When it comes to areas for improvement, keep it focused.** If the feedback seems valuable for the person to hear, share it, but don't just blindly report all the grudges. If you have very little feedback for improvement maybe that means the person is ready for promotion or given more challenging work. * **Avoid big surprises.** If someone is underperforming across the board, the review should not be his first time getting that feedback. * **Schedule enough time to discuss the review.** Give them a copy of the review as they're leaving on the evening before the review is scheduled. This gives them a chance to read it at home and come prepared to talk about what it says next day. ### Cultivating careers If you're a manager, you are going to play a key role in getting people on your team promoted, you'll need to make a case for their promotion. Learn how the game is played at your company. You need to be transparent with your team. Start identifying promotion-worthy projects and trying to give those projects to people who are close to promotion. If there is no growth potential on your team there's no room for people to work at a more senior level, it may be a sign that you need to rethinking the way work is done in order to let individuals take on bigger responsibilities. ### Challenging situations: firing underperformers You might have to provide a _performance improvement plan_, a set of clearly defined objectives that a person must achieve within a fixed period of time. One of the basic rules of management is the rule of no surprises, particularly negative ones. You'll always need to have a record of negative feedback to fire someone in any environment where HR is active. Give people clear improvement feedback in writing, with a timeline for improvement, and have them acknowledge it in writing as well (an email is ok). A final warning: don't put anyone on a plan whom you wouldn't be happy to lose. > Coaching someone out of the company > If you think your team is not the right place for somebody to grow his career. You aren't firing him, but you are telling that he needs to move on if he wants to progress. > Give the employee a chance to find a job in another part of the organisation or at another company. When he does, let him go happily. ## Managing a team > The engineering lead will spend less time writing code, but they still engage in small technical deliverables, such as bug fixes and small features, without blocking or slowing down the progress of their team. > > The person who fills this role is expected to have a large impact on the success of the organisation as a whole. > > In addition to strong management skills, the engineering lead acts as a leader for their technical roadmap for their product group pillar. **Being a good manager isn't about having the most technical knowledge. The work of supporting people is far more important to management success.** ### Staying technical Engineering management is a technical discipline, not just a set of people skills. Technical instincts honed over years of doing the job are very important for guiding that process. If you truly wish to command the respect of an engineering team, they must see you as technically credible. Without technical credibility you face an uphill battle. You have to learn how to balance. If you don't stay in the code, you risk making yourself technically obsolete too early in your career. You need to stay enough in the code to see where the bottlenecks and process problems are. Strong engineering managers can identify the shortest path through the systems to implement new features. It's hard to make up lost time when you stop writing code, and if you do it too early in your career, you may never achieve sufficient technical savvy to get beyond the role of middle management. ### Debugging dysfunctional teams: the basics * **Not shipping**. The trick is to learn how to balance pushing your team and holding back. Start to push for the removal of bottlenecks. When people are contending for a scarce resource, conflicts and unhappiness among team members are almost inevitable. * **People drama**. Make it clear that a bad behaviour has to change, bring clear examples, and provide corrective feedback quickly after things happen. The best defence is a good offence, quick actions are essential. * **Unhappiness due to overwork**. If overwork is due to (in)stability of the production system, it's your job as the manager to slow down the product roadmap in order to focus on stability for a while. Measures of alerts, downtime, incidents, and strive to reduce them. Dedicate 20% of your time in every planning sessions to system sustainability (technical debt). For time-critical releases you should be playing cheerleader helping out with the work yourself. Order dinner. Tell them you appreciate the hard work. Offer breaks after the push and make it as fun as you can. You should avoid it next time. * **Collaboration problems**. Regular touch-bases with the appropriate peers to work through issues. Gather actionable feedback from your team, and have productive conversations about possible improvements. Try to stay positive and supportive of their efforts in public. If your team isn't working well together, look into creating some opportunities for them to hang out. ### Managing a former peer If you are now managing someone who was truly your peer, acknowledge the weirdness of the transition. Be honest and transparent with this person and communicate that you're going to do your best job you can, but you'll need his help to do it. **You're going to have to be a little bit vulnerable with him.** You may now have the ability to override his decisions, but use this power very cautiously. Resist the temptation to micromanage people. They are going to be sensitive. You're going to have to let go of some of your previous work. Your goal is to show the team that you're committed to helping them succeed. ### The shield It's valuable for everyone to realise that they can and should focus on the things they can impact and change, and ignore the things they can't Helping them understand the key important goals and focusing them on those goals is important. However, it's unrealistic to expect that you can or should shield your team from everything. **Sometimes it's appropriate to let some of the stress through the team. Help them get context into what they're dealing with.** _You are not their parent_. Your team is made up of adults who need to be treated with appropriate respect. ### How to drive good decisions While the product manager is responsible for the product roadmap, and the tech lead is responsible for the technical details, you are usually accountable for the team's progress. * **Create a data-driven team culture**. Get used to give to the product or business head person data about team productivity (time that takes to complete features) or data about quality measures (number of outages, bugs found, etc). * **Flex your own product muscles**. Taking the time to develop customer empathy is important because you'll need to give your engineers context for their work. * **Looking into the future**. Think two steps ahead, from a product and technology perspective. Getting a sense of where the product roadmap is going helps you guide the technical roadmap. Start asking the product team questions about the future might look like, and spend some time keeping up with technological developments that might change the way you think about the software you're writing or the way you're operating it. * **Review the outcome of your decisions and projects**. Review assumptions after the project is done. * **Run retrospectives for the processes and day-to-day**. Discuss what happened during the sprint and pick a few events, good, bad or neutral, to discuss in detail. ### Good manager, bad manager: conflict avoider, conflict tamer Creating a safe environment for disagreement to work itself out is far better than pretending that all disagreement does not exist. * **Don't rely exclusively on consensus or voting**. Don't set people up for votes that you know will fail instead of taking the responsibility as a manager of delivering bad news yourself. * **Do set up clear processes to depersonalise decisions**. Start with a shared understanding of the goals, risks and the questions to answer before making a decision. * **Don't turn a blind eye to simmering issues**. Conflict avoidance manifests as an inability to address problems until they've gone on for way too long. It's probably an indication that you are not paying attention. * **Do address issues without courting drama**. The goal is to identify problems that are causing the team to work less effectively together and resolve them, no to become the team's therapist. * **Don't take it out on other teams**. * **Do remember to be kind**. Your goal as a manager, should not to be _nice_, it should be to be _kind_. Nice is saying \"please\" and \"thank you\". It's kind to tell someone who isn't ready for promotion that she isn't ready. * **Don't be afraid**. * **Do get curious**. Be thoughtful about your behaviour. ### Challenging situations: team cohesion destroyers Most gelled teams have a sense of camaraderie that makes them joke together, get coffee, share lunch, and feel friendly toward one another. They don't view their team as something they're eager to escape every day. The goal is to reach psychological safety, take risks and make mistakes in front of one another. Get to know people as human beings. It fosters relatedness, sense of people as individuals and just anonymous cogs. Teams that are friendly are happier, gel faster, and tend to produce better results. This is why those who undermine team cohesion are so problematic. #### The brilliant jerk Produces individually outsized results, but is so ego-driven that she creates a mixture of fear and dislike in almost everyone around her. **It's incredibly hard of a manager to justify getting rid of someone who produces great work.** Simply not hire one. Getting rid of brilliant jerks takes a level of management confidence that I think is uncommon. These folks will often get rid of themselves, **it's unlikely that you'll be stupid enough to promote them.** Simply and openly refuse to tolerate bad behaviours. One of the few instances where \"praise in public, criticise in private\" is upended. You don't want your culture to mimic, you need to say something in the moment to make the standard clear. If you seem emotional, it may undermine you. **Your first goal is to protect your team as a whole, the second is to protect each individual on the team, and your last priority is protecting yourself.** #### The noncommunicator The person who hides information from you, from his teammates, from his product manager. You have to nip this information-hiding habit in the bud as soon as possible. He doesn't feel safe sharing his work in progress, and his fear often sets an example for the rest of the team. Address the root cause of the hiding. #### The employee who lacks respect A person who simply doesn't respect you as a manager, or who doesn't respect her teammates. If your team member doesn't respect you or her peers, why is she working there? ### Advanced project management Some rules of thumb to keep in mind. * **None of this is a replacement for agile project management**. You are responsible for the larger picture, the accomplishments that are measured in months instead of weeks, and this is where you have to start exerting some higher-level planning. * **You have 10 productive engineering weeks per engineer per quarter**. Don't expect to get more than 10 weeks worth of focused effort. * **Budget 20% of time for generic sustaining engineering work**. Testing, debugging, cleaning up legacy code, migrating language or platform versions, and doing other work that has to happen. * **As you approach deadlines, it's your job to say no**. The only way to achieve this goals is to cut scope at the end of the project. Figure out what \"must-haves\" are not actually must-haves. * **Use the doubling rule for quick estimates, but push for planning to estimate longer tasks**. Whenever asked for an estimate, take your guess and double it. ### Joining a small team as a manager Get someone to walk you through the systems and architecture, as well as the process for testing and releasing the software. Follow the developer onboarding process. Work on at least a couple of features in your first 60 days. Pair with one of the engineers on a feature he's working on, have him pair with you as you start working on a feature of your own. Get your code reviewed. Perform a release and do rotation of supporting systems. ## Managing multiple teams Understanding details across a couple of teams probably means one important thing: you're not writing (much, any, production). > The engineering director leads engineers across multiple product areas, or multiple technology functions. Tech leads and individuals report into them. > > The engineering director is not generally expected to write code on a day-to-day basis, however she will be responsible for their organisation's overall technical competence, guiding and growing it as necessary via training and hiring. They should have a strong technical background and spend some of their time researching new technologies and staying abreast of trends. They should help debug and triage critical systems, and should understand the systems they oversee well enough to perform code reviews. They should contribute to architecture and design efforts primarily by serving as the technical savvy voice. > > They focus on ensuring that we continually evaluate and refine our development/infrastructure standards. Being responsible for creating high-performance, high-velocity organisations, measuring and iterating on process. They are leaders for recruiting, headcount management and planning, career growth and training for the organisation. > > They are responsible for creating and growing the next generation of leadership and management talents, and they are obsessed with creating high-functioning, engaged, and motivated organisations. > > They both create a strategic and tactical tech roadmap that tackles business needs, efficiencies and revenue, and fundamental technology innovation. They are strong communicators being able to explain technical concepts to nontechnical partners. It is very difficult for a person responsible for hands-on management of multiple teams to write code. Debugging and production support are also valuable. You might be more helpful doing pair programming, or fixing minor bugs or features. **You should spend the time to gain mastery of programming before moving into management.** One of the critical parts of the job at this level: debugging team issues and keeping your teams producing quality software smoothly. Even if you don't program on a daily basis, it is strongly advised to code half a day per week on some creative pursuit. > ### I miss code! > _I miss code terribly. Is this a sign that I shouldn't be a manager?_ > > There is a transition period where people question frequently whether they've made a mistake. Management is a job, it is a necessary and important job, and in particular, it's your job now. > > Writing code is full of quick wins. Management has fewer obvious quick wins. It's natural to feel some longing for simpler times. You can't do everything all at once. Becoming a great manager requires you to focus on the skills of management, and that requires giving up some of your technical focus. It's a tradeoff, and one you'll have to decide if you're up to making. ### Managing your time: what's important, anyway? It's time to figure out how to manage your time. Setting goals for the team, helping your product team put details on the product roadmaps. Time management is a personal thing. Managing your time comes down to one important thing: understanding the difference between _importance_ and _urgency_. | Not urgent | Urgent |-------------|----------------------|-----------------------| Important | Strategic, make time | Obvious work | Unimportant | Obvious avoid | Tempting distractions | Urgency is often more clearly felt than importance. Emails feel urgent, but they aren't urgent. We also tend to substitute _obvious_ for _urgent_ in determining something's value. It's likely that you're spending a lot of your time on things that are urgent but only slightly important, and sacrificing things that are important but not urgent. Important but not urgent is actually preparing for meetings so that you can guide them. As a manager of multiple teams, you can win back a lot of time by pushing an efficient meeting culture down to your teams. When you stop going to all of their internal meetings, you run the risk of missing out on the clues that will help you catch problems early. Your attendance at these meetings is partially to pay attention to the dynamics and morale of your team. Ask yourself: **How important is the thing I'm doing?** > If your team needs a manager more than they need an engineer, you have to accept that being a manager means that you by definition can't be that engineer. If you are going to suck at one, which one that will be. > I feel bad when I suck at being an engineer, but sucking at being a manager would be a choice inflicted on other people. That's not fair. ### Decisions and delegation The first several months of managing multiple teams can feel like a death march, even when your hours are not excessive. The only way out of this situation is to go through it. If you don't feel a little bit overwhelmed, you're likely missing something. Feeling of management from here on out is plate spinning. Your plates are the people and projects you're overseeing, and your job is to figure out how much attention each one needs at what time. You'll get better at this over time. Your instincts will improve. | Frequent | Infrequent |---------|----------------------|--------------------------------| Simple | Delegate | Do it yourself | Complex | Delegate (carefully) | Delegate for training purposes | Strong managers spend a lot of their time developing members of their teams in these areas. Your goal is to make your teams capable of operating at a high level without much input for you. **Project management. Onboarding new team members. Working with the product team to break down product roadmap goals into technical deliveries. Production support.** Delegation is a process that starts slow but turns into the essential element of career growth. ### Challenging situations: strategies for saying no * **\"Yes, and\".** \"Yes, we can do that project, and all we will need to do is delay the start of this other project that is currently on the roadmap\". * **Create policies**. Making a policy helps your team know in advance the cost of getting to \"yes\". * **\"Help me say yes\"**. Means you ask questions and dig on the elements that seem so questionable to you. This line of questioning helps people come to the realisation themselves that their plan isn't a good idea. * **Appeal to budget**. Lay out the current workload in plain terms, and show how there is little room to maneuver. \"Not right now\". * **Don't prevaricate**. When you know that you need to say no, it's better to say it quickly that to delay and drag out the process. You'll be wrong sometimes, so when you discover that you were too quick to say no, apologise for making that mistake. ### Technical elements beyond code Assuming that the job at this level becomes essentially nontechnical is a mistake. You now need to develop an eye for technical health signals for the overall team, As the popular management book _First, break all the rules._ ### Measuring the health of your development team Interrogate every process to determine the value it should provide, and always ask yourself if it can be automated further. * **Frequent releases**. Frequency of code change is one of the leading indicators of a healthy engineering team. Part of moving fast requires breaking work down into small chunks. As a technical leader, while you may not be writing code much, you're still responsible for the technical side of getting work done. You're also responsible for keeping your team happy and productive, and often the solution to this is not cheerleading or paying them better or praising them more. **You have to be the advocate and push for technical process improvements that can lead to increased engineer productivity, even if you're not implementing them all yourself.** * **Frequency of code check-ins**. The importance of breaking stuff into chunks. Engineers who don't write tests often have a harder time breaking down their work, and learning how to test-driven development can help them better at this skill. * **Frequency of incidents**. Determining the level of software quality you need for the product you're building and adjusting that measure over time is a technical challenge for you, the manager, to help address. Developers support code or systems they write. Incident management, when it becomes merely reacting to incidents rather than working to reduce them, can turn into a task that diminishes your team's ability to do what they do best. ### Good manager, bad manager: us versus them, team player. It can be hard for new managers to create a shared team identity. They unite the team by empathising how this identity is special as compared to other teams. Taking this too far, this identity is used to make the team feel superior to the rest of the company, and the team is more interested in its superiority than the company's goals. This attitude make the team vulnerable of many dysfunctions. * **Fragile to the loss of the leader**. When you hire a manager who builds a clique, that clique is likely to dissolve and leave the company if the manager leaves the company. * **Resistant to outside ideas**. Miss opportunities to learn and grow. * **Empire building**. Leaders who favour and us-versus-them style tend to be empire builders, seeking opportunities to grow their teams and their mandates without concern for what is best for the overall organisation. * **Inflexibility**. Struggle against changes that comes from outside the group. Before you try to change everything to fit your vision, take the time to understand the company's strengths and culture. The trick is not to focus on what's broken, but to identify existing strengths and cultivate them. By creating a strong and enduring alignment between the team, its individuals, and the overall company, purpose-based binding makes teams. * **Resilient loss of individuals**. * **Driven to find better ways to achieve their purpose**. * **First-team focused**. Consider the needs of the company as a whole before focusing on the needs of their team. * **Open to changes that serve their purpose**. Getting clarity about the purpose of your team and your company can take time. In startups especially. ### The virtues of laziness and impatience Impatience paired with laziness is wonderful when you direct it at processes and decisions: figuring out what's important, and going home. Be impatient to figure out the nut of what's important. Laziness as \"faster\", about \"the same value to the company in less total time\". Go home! Forcing yourself to disengage is essential for your mental health. ## Managing managers Now you are responsible for several teams, for overseeing the health of those teams, and for helping them set goals. There are more projects and people than you could possible handle by yourself. Instead of managing a couple of closely related teams, you may manage a larger scope of efforts. Managing managers adds a whole new level of complexity. It's easy to miss out on the details because you no longer engage regularly with all of the individual developers on each team. You'll need to practice honing your instincts. Let's take the case of managing a team that is doing work outside of your skill set. Sometimes a person might be playing project manager instead of training their managers to do that job themselves. This position is the first level in a much bigger game. > ### The fallacy of the open-door policy > > _I've told my team I have an open-door policy, they can come to me whenever they want to discuss problems._ > > One thing managers have to keep in mind is that a part of their job is to ferret out problems proactively. > > The open-door policy is nice in theory, but it takes an extremely brave engineer to willingly take the risk of going to her boss to tell him about problems. > > When you manage managers, you ultimately evaluate them on the performance of their teams. > > Part of the job is simply to make sure that your 1-1s have room for real conversations. ### Skip-level meetings No one wants to add yet more meetings to their calendar. A skip-level meeting it's a meeting with people who report to people who report to you. Their purpose is to help you get perspectives on the health and focus of your teams. It's a short 1-1 meeting, held perhaps once a quarter, between the head of an organisation and each person in that organisation. It's a personal relationship between you and everyone in your organisation. It also gives individuals time to ask you questions. Each person should come prepared to focus on what he or she is interested in talking to you about. Possible prompts for a skip-level 1-1: * What do you like best/worst about the project you are working on? * Who on your team has been doing really well recently? * Do you have any feedback about your manager, what's going well, what isn't? * What changes do you think we could make to the product? * Are there any opportunities you think we might be missing? * How do you think the organisation is doing overall? Anything we could be doing better/more/less? * Are there any areas of the business strategy you don't understand? * What's keeping you from doing your best work right now? * How (un)happy are you working at the company? * What could we do to make working at the company more fun? If you have a larger organisation, there are other ways to get skip-level time. Like holding lunches with whole teams a couple of times a quarter for each team. However, people act differently in group scenarios. It is a good opportunity to get a sense for where the team believed their focus needed to be. Some questions that can be used to draw out information: * What can I, your manager's manager, provide for you or your team? Anything I should be helping with? * Is this team working poorly with any other teams, from your perspective? * Are there any questions I can answer? Skip-level lunches provide familiarity, which in turn generates more willingness for people to come to you. At this level, you're constantly making tradeoffs between investing in expensive engagements, such as 1-1s, or casual engagements that are more efficient but provide less detailed information. ### Manager accountability There is one universal goal for these relationship: _they should make your life easier_. Your managers should allow you to spend more time on the bigger picture, and less time on the details of any one team. Sometimes managers make your life easier by hiding problems and telling you what you want to hear, until months later you see things failing apart and wonder where you went wrong. **You have to hold managers accountable**. The manager ultimately needs to take responsibility for pulling the team out of problematic situations (unstable product roadmap, errand tech lead, full-time firefighting mode). **The manager is accountable for the health and productivity of the team.** When the product organisation is constantly changing goals, the manager should identify that the changes are causing problems on the team, and work with product to explain the problem and refocus on what's important. If the team can't do anything but fight fires, the manager should put together a plan for tackling causes of the fires, and if necessary bring requests for hiring more people or adding more people to the team so that they can get the situation under control. You'll need to help your managers, sometimes they won't have the energy to push back against product and they'll need your support. **Managers need coaching and guidance in the same way that individual contributors need coaching and guidance.** ### Good manager, bad manager: the people pleaser The people pleaser has deep aversion to ever directly making people he cares about unhappy. If you're in the group that he cares, he'll always say yes. People pleasers often burn themselves out. * Her team loves her as a person but is increasingly frustrated with her as a manager. * He's more interested in a team that runs smoothly and avoids mistakes than really become excellent. * When she is feeling bad, she wears it on her face, and the whole team lose confidence. * He never pushes back work. * She overpromises and underdelivers. * He says yes to everyone and sends contradictory messages. * She seems to know about all of the problems, but hasn't directly addressed any of them. The _therapist_ people pleaser can inspire huge loyalty, but unfortunately this can mean he amplifies drama and negativity, and disappoints his team by making promises that he cannot possibly keep. There is the _external_ pleaser, the one that wants to make her boss and her external team partners happy, and is terrified of revealing problems on her team. **The people pleaser struggles to say no.** **These managers make it hard for the team to fail in a healthy way**, because of the manager's own fears of failure and possible rejection. An externally focused people pleaser shuts down honest conversation by evasion. A team pleaser fails by promising things that aren't realistic, making the team bitter toward either the manager or the company for failure to live up to these inflated expectations. If it happens you end up managing somebody in this situation, you can work on help the person feel safer saying no and externalising more decisions so he doesn't take failure personally. Providing strong partners who can take on the task of determining the work roadmap. Creating better processes for getting work scheduled, having a structure that specifies the requirements for getting promotions or accessing other opportunities, the people pleaser can rightly point to the process as something outside of her control. One of the best things you can do is show the person that he's exhibiting the behaviour, and highlight the downsides. ### Managing new managers First-time managers need a lot of coaching, it will be an up-front cost that pays long-term dividends for your organisation. No book or training can replace you spending some time asking your new manager how her 1-1s are going. When people start quitting because their manager hasn't give them a career path or isn't inspiring them, it's the ultimately your responsibility. Use skip-level meeting to help out detect areas where you need to support your new manager fully. **A new manager who is working all the time is probably failing to hand off her old responsibilities to other people on her team.** Make it clear that you expect the new manager to hand off some of her old work, and help her identify opportunities to do so. **Managers who neglect the job are bad, but managers that take to the job with gusto because they believe it's the key to realising authority are worse.** A skip-level meeting will reveal their frustration that they have no ability to make decisions themselves. If your new manager is skipping your 1-1s or evading questions about what the team is working on, you may have a control freak on your hands. **The manager you're training should be ultimately making your job easier**. Clearly set the expectations up front that you'll hold her accountable for the team, and help her build the skills to achieve this. If they truly don't have the willingness to learn and the aptitude to become solid managers, they're a big problem. **Making the wrong person a manager is a mistake, but keeping her in that position once you've realise she's wrong for it is a critical error.** ### Managing experienced managers Management tends to be a very culture-specific task in a company, if you either work as a manager or hire a manager for a company that's not a good culture fit, you'll have problems. First challenge is making sure this person fits in with the culture of your team. Often we overvaluate expertise in product areas and allow it to blind us to cultural and process fit with our companies and teams. You need managers who understand how to work with teams who ship software frequently, who are comfortable with modern development process best practices, and who can inspire creative product-centric engineers. These skills are so much more important than industry-specific knowledge. Collaborate on areas of difference, allow him to teach you things, and take an active role in the process. ### Hiring managers Make sure that the person has the skills you need, then make sure that she's a culture match for your organisation. You can verify the latter by asking the people who would report to the new manager to interview her by asking her to help with problems they have right now, or have had in the recent past. You can role-play difficult situations, like dealing with an employee who is underperforming, or delivering a negative performance review. A manager must be able to debug teams. Ask the manager to describe a time when she ran a project that was behind schedule, and what she did in that scenario. Ask her to role-play with an employee who is thinking about quitting. Describe how she's coached employees who were struggling. Ask about her management philosophy. If she doesn't have one at all, that might be a red flag. What's the role for a manager? How does she stay hands-on, how does she delegate? Speaking skills are useful for certain types of leadership but not all. Give her an abbreviated version of your standard technical interview. To check for cultural fit, you need first to understand the values of the company around you. If you value servant-leadership and you hire a manager who wants to dictate exact marching orders to the team, there will be a bad fit. Similarly, if you value collaboration and hire a manager who thinks that the loudest voice in any conversation should win, you will also have problems. **Managers shape their teams to their culture. If you hire a manager who doesn't fit culturally with the team she's managing, the manager will fail and you'll have to fire her, or most of the team will quit and then you may still have to fire her.** Most new hires act in self-interest until they get to know their colleagues, and then they move into group interest. **Critical elements to hiring a new managers: the reference check.** Ask the references to describe the ways the person succeeds as well as the ways she fails. Ask them if they would work with or for this person again. > #### Managing outside your skill set > > Be very curious, ask lots of questions, but in an open way. Make it clear to that your goal is to understand what people do so that you'll be capable of appreciating it better. > > Grit your teeth and make the time to get comfortable with each area; take time to get to know the manager and employees in the team, and practice asking for details about the area, so that you can start to learn and develop a sense for what the people in that team are actually doing. ### Debugging dysfunctional organisations The best engineering managers are often great debuggers. They are relentless in their pursuit of the \"why\". Managing teams is a series of complex black boxes interacting with other black boxes. When the outputs aren't as expected, figuring out _why_ requires trying to open them up and see what's going on inside. * **Have a hypothesis**. You need a reasonable hypothesis that explains how the system got into the failed state. * **Check the data**. Look at the team chats and emails, look at the tickets, look at the repository code reviews and check-ins. What do you see? Look at their calendars. * **Observe the team**. Now is the time to start doing some potentially destructive investigations. Sit in their meetings. Boring meeting are a sign. They may be a sign of inefficient planning on the part of the organisers. Good meetings have heavy discussion element, where opinions and ideas are drawn out of the team. Be aware, the act of observing changes the outcome, or rather, causes an outcome to happen. Your presence might provoke hiding the problem you're trying to find. * **Ask questions**. Ask the team what their goals are. If they don't understand the goals of their work, their leaders aren't doing a good job engaging the team in the purpose of the work. * **Check the team dynamics**. Very professional groups tend to have a degree of personal connection between the members. **A bunch of people who never talk to each other and are always working on independent projects are not really working as a team.** * **Jump into help**. It's OK to jump in and help debug team issues as you see them, particularly when the manager in question is struggling. It can be an opportunity to teach the manager and help him grow. * **Be curious**. We get better at debugging by doing it often. We become better leaders by pushing ourselves and our management teams to really get into the bottom of the organisational issues. ### Setting expectations and delivering on schedule Why something is taking so long? Answering it is significantly harder when you aren't embedded deeply in the details. Hopefully you're being asked this question because something is running over plan by a significant margin. Sadly, we are often asked this questions in times when things are not taking any longer than the estimate. You must always be aggressive about sharing estimates and updates to estimates, even when people don't ask. **You must be aggressive about getting estimates**. **Estimates are often useful even if they aren't perfectly accurate because they help escalate complexity to the rest of the team.** Not every project necessarily has requirements that change frequently, and it's possible to do up-front work to drastically reduce the unknowns that make software estimation difficult. Business want to plan and get ideas of cost for effort. Teaching your teams how to hone their instincts about complexity and opportunity is worthy goal. Don't be afraid to cut scope toward the end of the project in order to make important deadlines. ### Challenging situations: roadmap uncertainty Challenge of changing product and business roadmaps. There are a few strategies about building a roadmap: * **Be realistic about the likelihood of changing plans given the size and stage of the company you work for**. * **Think about how to break down big projects into a series of smaller deliverables so that you can achieve e some of the results, even if you don't necessarily complete the grand vision.** Everything must be repeatedly re-examined with an eye toward what's the most valuable right now. * **Don't overpromise a future of technical projects**. This kind of thinking will get hopes up and then disappoint. * **Dedicate 20% of your team's schedule to \"sustaining engineering\"**. Refactoring, bugs, improving engineering processes, doing minor cleanups, and providing support. * **Understand how important engineering projects really are**. Treat big technical projects the same way as product initiatives. **Gather data to support yourself, and talk about what will be possible when the work is done.** Pick your battles. Teams may even be disbanded or moved around in ways you don't understand or agree with. Push for engineering involvement in the early planning for the new work so that people can get excited about the projects they are moving on to. The calmer you can be, and the better you can show (or fake) enthusiasm for the new direction, the easier the transition will be for your whole team. ### Staying technically relevant * **Oversee technical investment**. You're accountable for making sure the team is placing its technical best in the right places. You can see where the ares of greatest need or opportunity lie. * **Ask informed questions**. Having accountability doesn't mean that you personally do the research to find potential investments. You guide these investments by asking questions. You need to know enough about the work to sniff our misguided efforts and evaluate proposed investments. * **Analise and explain engineering and business tradeoffs**. Understanding the business and customer goals, you offer guidance as to which technical projects can achieve those goals within reasonable time frames. * **Make specific requests**. You still need to have enough understanding of the technology in your organisation to make specific requests without distracting the senior engineers with questions. **Managers who don't stay technical enough relay these questions on the team, and then relay the team's response back up to management. This is not value-add role.** * **Use your experience as a gut check**. Rely on your instincts to guide where you spend your time and attention. So, how should you invest your time to stay technically relevant? * **Read code**. Looking over code reviews and pull requests can give you insight into changes that are happening. * **Pick an unknown area, and ask an engineer to explain it to you.** Have him pair with you on a small change. * **Attend postmortems**. In times of failure you can most clearly see where problems have built up. * **Keep up with industry trends in software development process**. Make time to learn about how other teams deliver software. * **Foster a network of technical people outside your company**. * **Never stop learning**. ## The big leagues Your first job is to be a leader. The company looks to you for guidance on what to do, where to go, how to act, how to think, and what to value. You help set the tone for interactions. You're capable of making hard decisions without perfect information and willing to face the consequences of those decisions. You know how to plan for the months and years ahead. You understand organisational structure and how it impacts the work of teams. You can play politics in a productive way, in order to move the organisation and the business forward. You work well with your peers outside of engineering. You understand how to disagree with a decision and commit to deliver on it. You know how to hold individuals and organisations accountable for their output. The book _High Output Management_, breaks down management tasks into four categories: * Information gathering or information sharing: synthesising large quantities of information quickly, sharing information with 3rd parties in a way they would understand. * Nudging: Reminding people of their commitments by asking questions instead of giving orders. * Decision making: Taking conflicting perspectives and incomplete information and setting direction. * Role modelling: Showing people what the values of the company are. Showing up for your own commitments. Setting the best example for the team even when you don't feel like it. > My job wasn't to be the smartest person in the room. It wasn't to be \"right\", my role was to help the team make the best possible decisions and help them implement them in a sustainable and efficient way. ### What's a VP of Engineering? VP role has one obvious difference from the CTO role. **The VP is usually at the top of the management career for engineers.** This role means that the person has a solid handle on processes and details. This person is capable of dropping into the details and making things happen at a low level. The VP is usually the one pushing the execution of ideas, while the CTO focuses on larger strategy and the position of technology within the company. She aligns the development roadmap to the hiring plan, planning out how teams need to evolve. The job for an VP of Engineering is both a big one and a detail-oriented one. This position must be able to gain people's trust and show wisdom in management and leadership. Most engineers are reluctant to trust people without technical credibility. The VP of Engineering have some stake in organisational strategy, she sets goals to achieve business deliverables, closely aligned with the product team. She should have a strong business and product instinct and a track record of getting teams to deliver big projects, including the ability to negotiate deliverables. They want their teams to be happy, but it's important to tie that happiness to a sense of accomplishment. They cultivate a healthy and collaborative culture. ### What's a CTO A CTO is not an engineering role. **CTO is not at the top of the technical ladder, and it is not the natural progression engineers should strive to achieve over the course of their careers.** It's not a role most people who love coding, architecture, and deep technical design would enjoy doing. It follows that the tCTO is not necessarily the best engineer in the company. **The CTO should be a strategic technical executive the company needs in its current stage of evolution.** A CTO must care about and understand the business and she should be able to shape business strategy through the lens of technology. **She is an executive first, a technologist second.** The CTO may identify areas where technology can be used to create new or bigger lines of business. Strong CTOs also have significant management responsibility and influence. **The CTO must protect the technology team from becoming a pure execution arm for ideas without tending to its own needs and its own ideas.** CTOs that don't have management oversight over teams, technology becomes an execution arm and they cannot have much strategic influence. **You can't give up the responsibility of management without giving up the power that comes with it.** **If you give up management, you're giving up the most important power you ever had over the business strategy, and you effectively have nothing but your organisational goodwill and your own two hands.** Being a CTO it's a business strategy job first and foremost. It's also a management job. If you don't care about the business your company is running, then the CTO is not the job for you. ### Changing priorities Priority changes from senior management can sometimes happen without warning. When leaders see an opportunity or feel that the priorities of the organisation need to change, they often expect the change to happen immediately. This rarely happens.. **You must first go though the list of things in flight and kill or postpone work in order to make room for new priorities.** You need to do that, if it's truly urgent. Not focusing on the right priority it's a sign that you and the CEO have a misaligned understanding of reality, and you need to get on the same page. The more senior the management and leadership position you take in the company, the more the job becomes making sure the organisation moves in the direction it needs to move in, and that includes changing direction when needed. **You do this by clearly communicating the direction to your teams and making sure they understand it and are taking the necessary steps to change course.** Most people need to hear something at least three times before it really sinks in. The larger the organisation, the harder it'll be to change priorities quickly. Keep the CEO informed about what's happening and why. Do your best to show that you understand his priorities and tell him about the concrete steps you're taking to meet those priorities. ### Setting the strategy Strategy as a critical element, setting strategy looks like * **Do a lot of research.*** The technology we had currently built, the pain points. Where the expected growth will come from the future. What are the scaling challenges right now, and where they might be in the future. What are the current productivity bottlenecks. * **Combine your research and your ideas.** Draw out the systems in place at your company, slice and dice the systems and teams across various common attributes. Get insights on the way data flows and changes, and possible axes for evolution. * **Draft a strategy.** Actionable ideas to improve operational efficiency, expand features and grow the business. Consider the structure of the business, the needs of the customer and possible future evolutions. Develop a technology strategy that supports these factors into the future. * **Consider your board's communication style.** * In an underdeveloped strategic plan, the system and architectural details offer very few forward-thinking ideas beyond the next 6 to 12 months. It's not uncommon for company boards to read through the slide deck before a meeting, so that the meetings can be focused more on the details on presentations. * A good technology strategy means good technology architecture, team structure, underpinnings of the business and the directions in which it was header. Potential futures of the business. Anticipates and enables future growth. ### Challenging bad situations: delivering bad news You'll need to excel at communicating sensitive information to large groups * **Don't blast an impersonal message to a large group.** The worst way to communicate bad news is via impersonal mediums like email and chat. Your team deserves to hear the message coming from your mouth directly. The second-worst way to deliver a message is with all them in a room at once. * **Do talk to individuals as much as possible.** Try your best to talk to people individually about the news. * **Don't force yourself to deliver a message you can't stand behind.** You might need to have someone else help you deliver it. * **Do be honest about the likely outcomes.** Being forthright with people will help them trust you and tolerate unhappy news well. * **Do think about how you would like to be told.** > #### I have a nontechnical boss > Reporting to a nontechnical manager can be a total culture clash. > * Don't hide information behind jargon, and be careful with details > * Expect that you will need to run your 1-1s with your new boss, so come prepared with a list of topics. > * Try to bring solutions, not problems to be solved. Don't shy away from delivering bad news. > * Ask for advice. Nothing shows respect like asking for someone's advice. > * Don't be afraid to repeat yourself. > * Be supportive. Show that you are there to support. > * Actively look for coaching and skill development in other places. Get a coach, ask for training, and create a peer group outside of the company. ### Senior peers in other functions Senior leaders must actively practice first-team focus. They should dedicate first and foremost to the business and its success, and secondly to the success of their departments. Having few to no peers on your first team who are fellow engineers can feel very isolating. You let them own their ideas, and they let you own yours. If you disagree, try to stay out of it, always choose to discuss with kindness. The place for disagreement is either one-on-ones or in your leadership team meetings. Expect to defend your own technical decisions and roadmap in this settings. You have to put aside the idea that they're acting in irrational and self-interested ways when they disagree with you or do things you don't like. Establishing this fundamental trust is really difficult. A very common clash occurs between people who are extremely analytically driven and those who are more creatively or intuitively focused. Another is between the people who prefer to embrace agility and change (and, yes, sometimes disorder) and those who push for more long-term planning, deadlines and budgets. You have to figure out how to understand and trust everyone's styles across the spectrum. Your peers who are not analytically driven are not stupid. Throwing out jargon to people make _us_ look stupid to _them_. Disagreements that happen in the context of leadership team don't exist to the wider team. Once a decision is made, we commit to that decision. ### The echo You'll be watched more closely than you've ever been watched in your life. **Your first team is compromised of your peers at the leadership/executive level, and your reporting structure has now become your second team.** Socialising heavily with your team outside working hours is a thing of the past. If you don't detach, you're likely to be accused of playing favourites. In fact, you probably _will_ play favourites if you maintain very strong social ties with people who report up to you on the team. You need to detach because you need to learn how to lead effectively, with a throwaway comment, you can cause people to change their whole focus. Your reports are going to have a hard time distinguishing between their buddy and their boss. Your mere presence will change the tone and structure of meetings you attend. As you choose which behaviours to model in front of the team, they will learn those behaviours and copy them. You're going to be part of hard decisions. It won't be appropriate to discuss these decisions with other people at the company. It is tempting to rant to those you consider friends and your reporting team, but this is a bad idea. **As their leader, you can easily undermine their confidence by sharing worries that they can't do anything to mitigate.** _Care more_ about people as individuals. Take time to get to know as many people as you can as humans. It cab be easy to start to dehumanise people and treat them like cogs. People can tell when their leader stop caring about the individuals. Nurture that kind of connection reinforce that you _do_ care. ### Ruling with fear, guiding with trust #### Correcting a culture of fear * **Practice relatedness.** Getting to know your team as people or let them know you as a person. If you want a team that feels comfortable taking risks and making mistakes, one of the core requirements is a sense of belonging and safety. This means you need to take little time to small talk. * **Apologise.** When you screw up, apologise. You're showing the team that apologising doesn't make you weaker, it makes the whole team stronger. * **Get curious.** When you disagree with something, stop to ask why. Attacking something we disagree with makes that harder. Learn how to turn that disagreement into honest questioning. * **Learn how to hold people accountable without making them bad.** How do you measure success? Does the team have the capabilities needed to succeed? Are you providing feedback along the way? ### True north The role played by a senior leader of a functional area (CTO, CFO, etc.), sets the baseline of what excellence looks like in this function (_\"True North\"_). A way of exploring the True North in technology is by looking through the lenses of risk analysis. Things like: * Having a single point of failure. * Having known bugs and issues. * Being unable to tolerate high load. * Losing data. * Putting out code that is undertested. * Having slow performance. True North helps us understand that all these issues must be carefully considered. Just because these rules have exceptions doesn't mean we forget that they exist. True North is the guiding instinct that we as leaders have developed over time **You must spend enough time in your career to hone these instincts in order to be comfortable making fast judgment calls.** --- At this level your coaching and mentoring are likely to come from people outside of your company. Do you know other senior managers at companies in your area? Do you admire any technology senior managers? What is it about them that you admire? Are you behaving like a role model for your team? ## Bootstraping culture As part of your role of senior engineering leader, part of your job is to set the culture of your function. Neglecting the team culture is a sure-fire way to make your job harder. In startup culture, the ideas of \"structure\" and \"process\" are seen as pointless at best and harmful at worst. Startups believe that structure is the reason large companies move slowly. Instead of talking about structure, is better to talk about learning. Instead of talking about process, it's better to talk about transparency. We don't setup systems because structure and process have inherent value. We do it because we ant to learn from our successes and our mistakes, and to share those successes and encode the lessons so we learn from failures in a transparent way. Consider not only what you care about, but also how you can scale that knowledge and effort effectively. The important thing for leaders to be willing to do in those early days is to pic a strategy and run with it. You don't need to find the perfect solution; you need to find something that will get your through to the next milestone. Sometimes companies decide to limit the decisions themselves, as in organisation that forgoes titles. Deciding not to decide right now is a popular option for new companies, because it really doesn't matter at the scale of a few people. **Pretending to lack structure tends to create hidden power structures.** In [The Tyranny of Structurelessness](http://www.jofreeman.com/joreen/tyranny.htm) Jo Freeman describes a set of circumstances in which the unstructured group can work: * It is task oriented. It is the task that basically structures the group. * It is relatively small and homogeneous. Homogeneity is necessary to insure that participants have a \"common language\" for interaction. * There is high degree of communication. Information must be passed on to everyone. * There is a low degree of skill specialisation. Everything must be able to be done by more than one person. People become interchangeable parts. **When work is done to satisfy an immediate task, in a unified code base worked on by a team of interchangeables, the result is not usually a larger thoughtful structure, but a tweak here, a hack there.** We usually end up refactoring the code to make it scalable, this involves identifying and explicitly drawing out structure in order to make code base easier to read and work in. That is the value of structure. Structure is how we scale, diversify, and take on more complex long-term tasks. In the same way that strong technical system designers are capable of identifying and shaping underlying system structures, strong leaders are capable of identifying and shaping underlying team structures and dynamics, and doing so in a way that supports the long-term goals. The earliest startup is like a driving car. As you grow, you graduate to a commercial flight. So you need to consider your movements more carefully. Finally you graduate to a spaceship, where you can't make quick moves and the course is set long in advance, but you are capable of going very far and taking tons of people along the ride. ### Assessing your role * **People.** Leaders who want a high degree of control over their organisation tend to need more structure in place to make sure their wishes are enacted. * **Age.** The longer a company is around, the more habits become entrenched. The longer the company has been around, the more likely it is to continue to survive. * **Size of existing infrastructure.** If you have few established business rules and little code or physical infrastructure, there is less need for structure. The more existing business rules and infrastructure you have, the more you'll need clarity on how to handle them. * **Risk tolerance.** Your structures and process should reflect this. When failures occur, examine all aspects of reality that are contributing to those failures. The patterns you see are opportunities to evolve your structure, either by creating more or different structure or removing it. **Using failure to guide evolution lets you apply structure at the right level. Success is often a poor teacher.** If you want to learn from success, make sure you can identify the actually improvement you're seeking. When every new hire slows down the team down for months because there is no onboarding process, that is a failure due to lack of structure. When people regularly leave the company because they have no path to advancement or career growth, that is a failure due to lack of structure. The third time you have a production outage because someone logged directly into the database and accidentally dropped a critical table, that is a failure due to lack of structure. Better to talk about learning and transparency rather than using the word _structure_. ### Creating your culture > Culture is how the things get done, without people having to think about it. Consciously guiding the culture of your team is part of a leader's job. Culture is the generally unspoken shared rules of a community. Culture doesn't mean that every single person holds exactly the same values, but it tends to guide a general overlap. In complex environments where the needs of the group must override the needs of the individual, cultural values are the glue that enables us to work as a team. ### Applying core values Define your culture. If you have a set of company values, map those values onto your team. **Reinforce culture by rewarding people for exhibiting its values in positive ways. The stories that we tell as a community bond us together.** One of the most important uses of performance reviews is to evaluate the alignment between team members' values and the company's values. Learn to spot people who have value conflicts with the company or team. Using the core values to coach people in areas where they are misaligned can help you articulate what otherwise may feel like just ambiguous friction. Finally, use this as part of your interview process. Look out explicitly for places where the interviewee seems to match or collide with these values. **Cultural fit is not about hiring friends. Cultural fit as determined by friendship tests is almost certain to be discriminatory in some way.** Write the values down if they aren't already written, and try to be explicit. Use this explicit list to evaluate candidates, praise team members, and inform your performance review process. ### Creating cultural policy Getting started on these documents from scratch is hard. Fortunately there are fewer and fewer documents that you need to start from scratch to create, as more people are sharing publicly their policies and processes. Be mindful however, what works for one company, will not always translate well to another company, even if the companies have a lot of things in common. ### Writing a career ladder * **Solicit participation from your team.** Enlist the support for senior managers and engineers on the team to provide feedback and details. * **Look for examples.** Examples of ladders from friends at other companies to help provide some ideas for the details. * **Be detailed.** You want something that is inspirational and descriptive but that matches your company. * **Use both long-form descriptions and summaries.** * **Consider how the ladder relates to salary.** * **Provide many early opportunities for advancement.** You may want to be able to promote someone every year for the first two to three years of her career. Create several levels that encompass the role of _software engineer_. * **Use narrow salary bands for early-career stages.** Lots of levels and narrow salary bands mean that you can promote people quickly. * **Use wide salary bands when and where you have fewer levels.** You want those salary bands to overlap. Software engineer band may go $50-100K, and a senior software engineer band might go $80-150K. This is meant to retain talent who are performing well at their current levels. This also also allows you to hire people who are on the fence into the lower level with the expectation that they will be promoted quickly. * **Consider your breakpoint levels.** Lack of advancement means that the person has not achieved the maturity of independence needed to remain at the company. For many companies, it's somewhere around senior engineer. Someone who's made it this far is a solid team member. You may even want to use it as a the point at which your ladder levels get harder to achieve. * **Recognise achievement.** Celebrate and share keystone promotions. * **Split management and technical tracks.** You do not want people to feel that the only path to advancement is by managing people, because not everyone is suited to that role. * **Consider making people management skills a mid-career requirement.** Consider making leadership experience (tech lead) a requirement for promotion to senior individual contribution levels. * **Years of experience.** Distinguish the keystone levels by an expectation of maturity increase, and these tend to correspond with years of experience. * **Don't be afraid to evolve over time.** A ladder is a living document that will need to evolve as your company grows. (Rent the Runaway Ladder](https://dresscode.renttherunway.com/blog/ladder). ### Cross-functional teams Who do you work with? Who do you report to? Who do you collaborate with? Cross-functional product development is putting everyone who is needed to make a project successful together in one group. We are acknowledging that the most important communication is that which leads to effective product development and iteration. It will probably produce systems that have some inefficiencies compared to companies that have a more engineering-centered team structure. You'll have to decide where you're willing to take some system design hits in order to most effectively create products. #### Structuring cross-functional teams Engineers are managed by engineering managers that report to the CTO. Product managers report to the head of product. The determination of who was working on what is done largely by the team itself. **Someone in engineering needs to oversee critical core systems, and you probably need a few specialists. These functions can be kept in a small infrastructure organisation that is not generally assigned to product development.** Engineers assigned to product teams still need some time to account for engineering-specific tasks like on-call, interviewing, and technical debt (advise to reserve 20% of the time for these tasks). The infrastructure team supports both fundamental systems as well as large frameworks and technologies that will be used by many teams across the company. In technology-focused structures, the focus is on being the best engineer. In product-focused structures, the engineers who have the best product sense will start to emerge as the leaders of the team. In the areas where the technology must be rock-solid or exceptionally innovative and cutting-edge, you probably want teams that have more of an engineering focus and that are led by people who can design complex systems. ### Developing engineering process Without any process, your teams will struggle to scale. With the wrong process, they will be slowed down. > #### Engineering process > > Think of process as risk management. > > This has two important implications. The first is that you should not put a complicated process on any activity where you want people to move quickly or that the risks themselves are obvious to the whole team. > > The second implication is that you need to be on the lookout for places where there is hidden risk, and draw those hidden risks out into the open. Processes should have value even when they are not followed perfectly, and that value should largely lie in the act of socialising change or risk to the team as a whole. ### Practical advice: depersonalise decision making #### Code review You want the process to be straightforward and efficient: * **Be clear about code review expectations.** Code review is largely a socialisation exercise, so that multiple team members have seen and are aware of the changed code. * **Use a linter for style issues.** * **Keep an eye on the review backlog.** Limit how many outstanding review requests a person can have assigned to him. #### The outage postmortem * **Resist the urge to point fingers and blame.** * **Look at the circumstances around the incident and understand the context of the events.** Understand and identify the factors that contributed to this incident. * **Be realistic about which takeaways are important and which are worth dropping.** Pick the one or two that are truly high-risk and highly likely to cause future problems, and acknowledge the ones that you are going to let go for now. #### Architecture review The goal of architecture review is to help socialise big changes to the appropriate group, and to make the risks for those changes clear. * How many people on the team are comfortable using this new system/writing this new language? * Do you have production standards in place for this new thing? * What is the process for rolling this out and training people to use it? * Are there new operational considerations for using this? Some guidelines: * **Be specific about the kinds of changes that need architecture review.** Usually these include new languages, new frameworks, new storage systems, and new developer tooling. * **The value of architecture review is in preparing for the review.** It forces people to think about why they want to make these changes. It make people aware of the risks that they may not have considered. * **Choose the review board wisely.** The scope of the deciding group is best kept to the people who will be closely impacted by the decision. ",
    "url": "/learning-notes/books/the-managers-path/",
    "relUrl": "/books/the-managers-path/"
  },"25": {
    "doc": "The Manual: A Philosopher’s Guide to Life",
    "title": "The Manual: A Philosopher’s Guide to Life",
    "content": "# [The Manual: A Philosopher's Guide to Life](https://www.goodreads.com/book/show/34946912-the-manual) Stoicism doesn't mean repressing emotion and shunning pleasure, in essence is about focusing on what is in our power and letting go of everything we can't control. --- Within our power are our own thoughts and actions. Outside our power is everything else. Within our sphere of control, we are naturally free, independent, and strong. Beyond that sphere, we are weak, limited and dependent. **If you wish to have peace and contentment, release your attachment to all things outside your control.** Whenever distress or displeasure arises in your mind, remind yourself \"This is only my interpretation not reality itself\". Then ask whether it falls within our outside your sphere of power. And, if it is beyond your power to control, let it go. --- If you try to avoid what you cannot control, sickness, poverty, death, you will inflict useless mental suffering upon yourself. End the habit of despising things that are not within your power, and apply your aversion to things that are within your power. **As for desire, for now it is best ot avoid it altogether.** --- **Remind yourself of the true nature of things, objects, and beings that delight your mind.** For your favourite cup, remember that is only a cup. When embracing your wife or child, remember that they are mortal beings. --- For any action, remind yourself of the nature of the action. Remind yourself of the usual incidents that happen. You will not be disturbed if you are prepared for such things. > Remain in harmony with the nature of things. I cannot get upset by things beyond my control. --- People are not disturbed by things themselves, but by the views they take of those things. The fear of death stems form the view that it is fearful. **People who are ignorant of philosophy blame others for their own misfortunes. Those who are beginning to learn philosophy blame themselves. Those who have mastered philosophy blame no one.** --- **Do not take satisfaction in possessions and achievements that are not your own.** What, then, is your own? The way you live your life. --- **Remain steadfast in pursuing your mission, always willing to shed distractions.** --- **Do not wish that all things will go well with you, but that you will go well with all things.** --- The only thing that can impede your will is your will itself. --- **Whenever a challenge arises, turn inward and ask what power can you exercise in the situation.** **If you meet temptation, use self-control; if you meet pain, use fortitude; if you meet revulsion, use patience.** --- Do not say of anything \"I have lost it\", but rather \"I have given it back\". **For as long as the Source entrusts something to your hands, treat it as something borrowed, like a traveller at an inn.** --- **It is better to die poor, while free from fear and grief, than to live surrounded by riches and filled with anxiety.** --- Do not strive to be celebrated for anything. It is no easy feat to hold onto your inner harmony while collecting accolades. --- **If you wish to be free, do not desire anything that depends on another. lest you make them your master.** --- Think of life as a banquet. If a dish is handed to you, sample it with gratitude. If you're waiting for a particular platter to come around, be patient. If it passes you by, don't complain. --- When you see a person weeping with grief, remind yourself \"What upsets this person is their opinion on what has happened\". Do not share these thoughts, sympathise with them, even cry with them. Your tears ill be outward, not inward. --- **Think of life as a play, and yourself as an actor. Play your part to the best of your ability.** You cannot choose but to act well in your given role. --- Distinguish between reality and interpretation. All signs point to good luck, if I interpret them that way. --- **If you make peace with all things that are beyond your power, refusing to fight them, you will be invincible.** There is no room for envy or aping others. Desire to be free, let go of anything that is not within your control. --- **When anyone provokes you, remember that it is actually your opinion provoking you.** --- Remind yourself that you are a mortal being, and someday will die. Do not waste precious time, like stewing over grievances and striving after possessions. --- If you intend to follow this path, do not adopt any air of superiority. Mind your own business. --- **If you find yourself acting to impress others, or avoiding action out of fear of what they might think, you have left the path.** Find satisfaction in following your philosophy. If you want to be respected, start respecting yourself. --- Find significance within yourself. Within your own sphere of power. **If you want to acquire riches without losing your honour and self-respect, then do it.** Work to acquire the character of a person who attracts good friends, rather than losing your character to gain riches. Your position in society is one best suited for your talents, which you can hold with honour. Each person has a vital role in society; you are important right where you are. If you lose your honour in striving for greater (perceived) significance, you become useless. --- If you envy people that attract love because of positive character qualities, be glad for them. If they attract attention because of negative character qualities, be glad you don't share with them. **Everything has its price.** If you have not been invited to a party, it is because you haven't paid the price of the invitation. It costs social engagement, conversation, encouragement, and praise. If you are not willing to pay this price, do not be upset when you don't receive an invitation. You may have something good instead of the invitation, the pleasure of not making small talk within people you don't really like, not praising someone you don't admire, etc. --- Whenever misfortune befalls you, ask yourself how you would react if it were someone else in the same situation. --- If someone tried to take control of your body and make you a slave, you would fight for freedom. **Yet how easily you hand over your mind to anyone who insults you. When you dwell on their words and let them dominate your thoughts, you make them your master.** --- In every situation consider what comes before, during and after an event. If you have considered all, and you still want to do it, by all means, begin. Otherwise, you are like a child who daydreams about and acts out to shows with his friends. If you have fully considered the consequences, and you still wish to make sacrifices, begin. **Whatever your vocation, pursue it wholeheartedly. Consider, choose, and commit.** --- Duties are determined by relations. Fulfil your duties to the best of your ability. **If someone insults or rejects you, do not stoop to his level and retaliate in anger. Hold securely to your clear conscience and inner harmony.** **No one can steal your peace of mind unless you let them.** --- **Stop judging things that fate brings you as \"good\" or \"evil\"; only judge your own thoughts, desires and actions as good or evil.** Be mindful of this when you participate in religious services and prayers. In devotion, be neither careless nor extravagant. --- When you hear predictions of the future, do not become fearful or excited, remember that future events are beyond your control. Outside events do not touch your deepest self, what matters is your interpretation and reaction, you an use any circumstance to your benefit. --- **Be the same person in public as in private.** Speak only what is useful and beneficial. In conversation, avoid idle chatter about horse races, athletes, celebrities, food, and drink. Refuse to participate in gossip, tearing down, inflating, and judging other people. **Among friends, shift the conversation to worthy topics; among strangers, stay silent.** Avoid vulgar entertainments. Take care of your bodily needs, food, drink, clothes, shelter, but avoid luxury and indulgence. Enjoy sex only within the bounds of your marriage; but do not assume a posture of moral superiority and look down upon those who behave otherwise. When you attend the games, do not get emotionally invested in the rivalry. Wish only the best team or athlete wins. When you attend parties, be a polite guest. Do not revel to the point of losing your dignity. **In company, do not prattle on about your own adventures and misadventures. Keep your language free of obscenities. Do not dip into search of cheap laughs.** --- When you feel burning desire for something, think about yourself as someone under a spell. Instead of acting on impulse, take a step back, wait until the enchantment fades and then see things as they are. **Consider the cost pursuing pleasure, every consequence that may follow, how you will feel about it the next day.** If reason tells you a pleasure is wholesome and harmless, you may enjoy it in moderation. But take care not to let your happiness gradually become dependent on it. Gaining in self-possession is more satisfying than bodily pleasure. --- **If you know you are in the right, why fear those who misjudge you?** --- It is rarely a question of good versus bad, but of weighing greater and lesser goods on a scale of values. --- **When thinking, watch for obstacles and errors in your line of thought.** You don't want to stumble into illogic and unreason. --- **Once you let your appetite exceed what is necessary and useful, desire knows no bounds.** --- Virtue, honour, and self-respect are the marks of true beauty. --- **Care for your body as needed, but put your main energies and efforts into cultivating your mind.** --- Gently turn away any insult or injury. \"It seems right to them, though they are mistaken\" --- **If a friend treats you unfairly, do not fall for retribution. Instead, reach for the opposite, for reconciliation.** The relationship is worth keeping. --- A person's worth, after all, is not found in possessions or style. --- Do not mistake your impressions for the whole truth. --- **Do not proclaim yourself a philosopher, or go around preaching your principles. Show them by example.** At a feast, do not give a speech about how everyone should eat. Only eat as you should. --- Do not make a spectacle of self-deprivation. When you fast, tell no one. --- An ignorant person is one who is tossed about between elation and despair by external forces and events. A philosopher is one whose thoughts and emotions are internally anchored. --- Those who focus on interpretations are grammarians, not philosophers. --- Follow your principles as though they were laws. --- You know the necessary principles of philosophy. Choose to act like the worthy and capable person you are. Follow unwaveringly what reason tells you is best course. --- The first topic of philosophy is the practical application of principles, like \"Do not lie\". The second topic is understanding the reasons behind principles \"Reasons why we should not lie\". THe third topic is verifying the principles through logic. \"What are the consequences of lying? Do we confirm our reasons for not lying? Are our reasons contradictory?\" In most schools of philosophy, they spend all their time on the second and third subjects, arguments and proofs, and neglect entirely the first. They can explain in academic and scholarly terms exactly why lying is wrong, yet they routinely lie. **Philosophy is for living, not just learning.** --- > Lead me, Fate, wherever you will and I will cheerfully follow – Cleanthes > If it pleases the gods, let it be – Socrates > They may kill me, but they cannot hurt me – Socrates ",
    "url": "/learning-notes/books/the-manual/",
    "relUrl": "/books/the-manual/"
  },"26": {
    "doc": "The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win",
    "title": "The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win",
    "content": "# [The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win](https://www.goodreads.com/book/show/17255186-the-phoenix-project) - [About the story](#about-the-story) - [About DevOps](#about-devops) - [The three ways](#the-three-ways) - [The four types of work](#the-four-types-of-work) _The Phoenix Project_ frames how a core, chronic conflict between Development and IT Operations preordains failure for the entire IT organisation. ## About the story ### Change definition A \"change\" is any activity that is physical, logical, or virtual to applications, databases, operating systems, networks, or hardware that could impact services being delivered. ### Bottlenecks Work shouldn't be released based on the availability of the first station. Instead, it should be based on the tempo on how quickly the bottleneck resource can consume the work. On the _Theory of Constraints_, any improvements made _anywhere besides the bottleneck_ are an illusion. Any improvement made after the bottleneck is useless, because it will always remain starved, waiting for work from the bottleneck. And any improvements made before the bottleneck merely results in more inventory pilling up at the bottleneck. ### Indispensable people Maybe some part of them are reluctant to share the knowledge they have. So they made themselves virtually impossible to replace. Indispensable people work as constraints to the business. Every time you let these people fix something that nobody else can replicate, these people get smarter and the entire system gets dumber. A possible solution is to isolate these profiles and have a pool of people to work on the same problems and be the only ones accessing the indispensable with supervision of managers. > It's not the upfront capital that kills you, it's the operations and maintenance on the back end. ### The goal In most plants, there are a very small number of resources, whether it's men, machines, or materials, that dictates the output of the entire system. We call this the constraint or the bottleneck. Focusing steps: 1. **Identify the constraint.** Any improvement not made at the constraint is just an illusion. 2. **Exploit the constraint.** Make sure that the constraint is not allowed to waste any time. Ever. It should never be waiting on any other resource for anything. 3. **Subordinate the constraint.** In the _Theory of Constraints_, this is typically implemented by the Drum-Buffer-Rope. The slowest Boy Scout in the troop, actually dictates the entire group's marching pace. So you move the the slowest boy to the front of the line to prevent kids from going too far ahead. In a plant, you should release work in accordance to the rate it could be consumed by the bottleneck. Using a _Kanban_ board to release work and control _work in progress_ for Development and IT Operations might help. ### Outcomes Being able to take needless work out of the system is more important than being able to put more work into the system. You need to know what matters to the achievement of the business objectives. **Outcomes are what matter, not the process, not the controls or what work you complete.** ### Team dynamics A great team doesn't mean that they had the smartest people. What made those teams great is that everyone trusted one another. The book _Five Dysfunctions of a Team_ shows that in order to have mutual trust, you need to be vulnerable. IT is not just a department. IT is a competency that we need to gain as an entire company. People need to reforge into great teams where all can trust one another in order to succeed. ### Unplanned work Being always scrambling, having to take shortcuts, leading to fragile applications in production to end into more unplanned work and firefighting is the the consequence of _Technical Debt_ not being paid down. _Technical Debt_ comes from taking shortcuts, which may make sense in the short-term. But like financial debt, the compounding interest costs grow over time. If an organisation doesn't pay down its technical debt, every calorie in the organisation can be spent just paying interest, in the form of unplanned work. Unplanned is very expensive because unplanned work comes at the expense of planned work. Technical debt will ensure that the only work that gets done is unplanned work. Unplanned work has another side effect. When you spend all your time firefighting, there's little time or energy left for planning. When you do is react, there's not enough time to do the hard mental work of figuring out wether you can accept new work. Leadership needs to say _no_. Companies cannot afford to have their leadership teams to be order takers. Leaders are paid to think, not just do. ### Prioritising Prioritising better is not going to fix the problem. You need to think about the constraints. The goal is to increase throughput of the entire system, not just increase the number of tasks being done. In the case of competing priorities, you freeze the rest, no multitasking allowed unless you control all your constraints and you are actually able to parallelise. ### Capacity and demand Gathering prerequisites of what you need before you can complete work allows you to build a bill of resources. A bill of materials along with the list of the required work centers and the routing, along with the work orders and resources you'll finally be able to get handle on what your capacity and demand is. This will enable you to finally know whether you can accept new work and then actually be able to schedule the work. ### Resilience The Third Way is all about ensuring that we're continually putting tension into the system, so that we're continually reinforcing habits and improving something. Resilience engineering is tells us that routinely injecting faults into the system, doing them frequently, to make them less painful. ### Improvement kata It almost doesn't matter what you improve, as long as you're improving something. Why? Because if you are not improving, entropy guarantees that you are actually getting worse, which ensures that there is no path to zero errors, zero work-related accidents, and zero loss. A _kata_ is a repetition that creates habits, and habits are what enable mastery. Whether you're talking about sports training, learning a musical instrument, or training in the Special Forces, nothing is more to mastery than practice and drills. If you want to create a genuine culture of improvement, you must create those habits. ### Timing handoffs Just as important as throttling the release of work is managing the handoffs. A critical part of the Second Way is making wait times visible, so you know when your work spends days sitting in someone's queue, or worse, the work has go backward because it doesn't have the parts or requires rework. ### Scoping error > This guy is like the QA manager who has his group writing millions of new tests for a product we don't even ship anymore and then files millions of bug reports for features that no longer exist. Anything that is not about helping the business survive is just irrelevant irrelevant technical minutia. ### Kanban board A Kanban board is one of the primary ways manufacturing plants schedule and pull work through the system. It makes demand and WIP visible, and is used to signal upstream and downstream stations. You can take most frequent service requests, documented exactly what the steps are and what resources can execute them, and timed how long each operation takes. Getting executives and workers the tools they need to do their jobs is one of our primary responsibilities. ### Managing handoffs The wait time is the \"percentage of time busy\" divided by the \"percentage of time idle\". If a resource is fifty percent busy, then it's fifty percent idle. The wait time is fifty percent divided by percent, so one unit of time. Let's call it one hour. So, on average our task would wait in the queue of one hour before it gets worked. If a resource is ninety percent busy, the wait time is \"ninety percent divided by ten percent\", or nine hours. A task would wait in the queue nine times longer than if the resource were fifty percent idle. WAIT TIME = %BUSY / %IDLE WAIT TIME 100 ┤ ■ 90 ┤ ■ 80 ┤ ■ 70 ┤ ■ 60 ┤ ■ 50 ┤ ■ 40 ┤ ■ 30 ┤ ■ 20 ┤ ■■ 10 ┤ ■■■■ ┤■■■■■■■■■■■■■■■■■■■■■ └──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬── % RESOURCE BUSY 0 10 20 30 40 50 60 70 80 90 100 Creating and prioritising work inside a department is hard. Managing work among departments must be at least ten times more difficult. Everyone needs idle time, or slack time. If no one has slack time, WIP gets stuck in the system. Or more specifically, stuck in queues, just waiting. Sometimes some tasks look like a single-person task but they are not. Sometimes they involve multiple steps with multiple handoffs among multiple people. If those people are always busy, tasks will get stuck in queues indefinitely. ### The First Way It's about systems thinking, always confirming that the entire organisation achieves its goal, not just one part of it. * Are we competitive? * Understanding customer needs and wants: Do we know what to build? * Product portfolio: Do we have the right products? * R&D effectiveness: Can we build it effectively? * Time to market: Can we ship it soon enough to matter? * Sales pipeline: Can we convert products to interested prospects? * Are we effective? * Customer on-time delivery: Are customers getting what we promised them? * Customer retention: Are we gaining or losing customers? * Sales forecast accuracy: Can we factor this into our sales planning process? You must leave the realm of IT to discover where the business relies on IT to achieve its goals. You must find where you've under-scoped IT, where certain portions of the processes and technology you manage actively jeopardises the achievement of business goals. And secondly, where IT might be over-scoped, where you focused on the problems that bring little value to the business. ### The Second Way Create constant feedback loops from IT Operations back into Development, designing quality into the product at the earliest stages. To do that, you can have nine-month-long releases. You need much faster feedback. You need to reduce the size of your batches. The flow of work should ideally go in one direction only: forward. When work goes backward it's called _waste_. An inevitable consequence of long release cycles is that you'll never hit internal rate of return targets, once you factor in the cost of labor. ### _Tack_ time The cycle time needed in order to keep up with customer demand. If any operation in the flow of work takes longer than the takt time, you will not be able to keep up with customer demand. ### Automated infrastructure Until code is in production, no value is actually being generated. In order to decrease changeover time and enable faster deployment cycle time you need to automate the build and deployment process, recognising that infrastructure could be treated as code. That enables to create one-step environment creation and deploy procedure. This has been defined by _Continuous Delivery_ and the _Lean Startup_ books. You need to get everything in version control. Not just the code but the everything required to build the environment. Then you need to automate the entire environment creation process. You need a deployment pipeline where you can create test and production environments, and then deploy code into them, entirely on-demand. That's how you reduce your setup times and eliminate errors, so you can finally match whatever rate of change Development sets the tempo at. Get humans out of the deployment business. If you can't out-experiment and beat your competitors in time to market and agility, you are sunk. Features are always a gamble. If you're lucky, ten percent will get the desired benefits. So the faster you can get those features to market and test the, the better of you'll be. Incidentally, you also pay back the business faster for the use of the capital, which means the business starts making money faster, too. A big number of deployments per day allows you to fix bugs faster, get performance enhancements sooner, scaling when needed and run A/B tests as soon as possible. ### Sprint interval By reducing the sprint interval, you reduce your planning horizon to make and execute decisions more frequently, as opposed to sticking to a plan made almost a month ago. ### DevOps A super-tribe that is bigger than just Dev or Ops or Security. It's Product Management, Development, IT Operations, and even Information Security all working together and supporting one another. ## About DevOps ### Why? Faster feature time to market, increased customer satisfaction, market share, employee productivity, and happiness as well as allowing organisations to win in the marketplace. Technology has become the dominant value creation process and increasingly important means of customer acquisition. High performers are deploying features more quickly, while providing world-class levels of reliability, stability, and security, enabling them to out-experiment their competitors in the marketplace. Delivering these high levels of reliability actually _requires_ that changes be made frequently. High performers have significantly better organisational performance as well: they are two times more likely to exceed profitability, market share and productivity goals, and there are hints that they have significantly better performance in the capital markets as well. Everyone values nonfuncitonal requirements (e.g., quality, scalability, manageability, security, operability) as much as features. Why? Because nonfunctional requirements are just as important in achieving business objectives, too. We have High-trust, collaboration culture, where everyone is responsible of the quality of their work. Instead of approval and compliance processes, the hallmark of a low-trust command-and-control management culture, we rely on peer review to ensure that everyone has confidence int he quality of their code. Furthermore, there is a hypothesis-driven culture, requiring everyone to be a scientist, taking no assumptions for granted and doing nothing without measuring. Why? Because we know that our time is valuable. We don't spend years building features that our customers don't actually want, deploying code that doesn't work or fixing something that isn't actually the problem. These factors contribute to our ability to release exciting features to the marketplace that delight our customer and help our organisation win. ### Where does it came from? DevOps is the outcome of applying Lean principles to IT value stream. These principles are based on more than a century of sound management practices. ## The three ways ### The First Way Is about the left-to-right flow of work from Development to IT Operations to the customer. In order to maximise flow, we need small batch sizes and intervals of work, never passing defects to downstream work centres, and to constantly optimise the global goals. The practices include: Continuous build, integration, and deployment, creating environments on demand, limiting work in progress, and building safe systems and organisations that are safe to change. ### The Second Way Is about constant flow of fast feedback from right-to-left at all stages of the value stream, amplifying it to ensure that we can prevent problems from happening again or enable faster detection and recovery. By doing this, we create quality at the source, creating or embedding knowledge where we need it. The practices include: \"Stopping the production line\" when our builds and tests fail, elevating the improvement of daily work over daily work; creating fast automated test suites to ensure that code is always potentially deployable creating shared goals and shared pain between Development and IT Operations; and creating pervasive production telemetry so that everyone can see whether code and environments are operating as designed and the customer goals are being met. ### The Third Way Is about creating a culture that fosters two things: continual experimentation, which requires taking risks and learning from success and failure, and understanding that repetition and practice is the prerequisite to mastery. Experimentation and risk taking are what enable us to relentlessly improve our system of work. The practices include: Creating a culture of innovation and risk taking (as opposed to fear or mindless order taking) and high-trust (as opposed to low-trust, command-and-control), allocating at least twenty percent of Development and IT Operations towards nonfunctional requirements, and constant reinforcement that improvements are encouraged and celebrated. ## The four types of work * **Business projects**. These are business initiatives, of which most Development projects encompass. * **Internal IT projects**. Infrastructure or IT Operations projects that business projects might create, as well as internally generated improvement projects. Often these are not centrally tracked anywhere, instead residing with the budget owners. This creates a problem when IT Operations is a bottleneck, because there is no easy way to find out who much of capacity is already committed to internal projects. * **Changes**. Generated from the previous two types of work and are typically tracked in a ticketing system. * **Unplanned word or recovery work**. Operational incidents and problems. Always come at the expense of other planned work commitments. ",
    "url": "/learning-notes/books/the-phoenix-project/",
    "relUrl": "/books/the-phoenix-project/"
  },"27": {
    "doc": "The Subtle Art of Not Giving a Fuck: A Counterintuitive Approach to Living a Good Life",
    "title": "The Subtle Art of Not Giving a Fuck: A Counterintuitive Approach to Living a Good Life",
    "content": "# [The Subtle Art of Not Giving a Fuck: A Counterintuitive Approach to Living a Good Life](https://www.goodreads.com/book/show/28257707-the-subtle-art-of-not-giving-a-f-ck) - [Don't try](#dont-try) - [Happiness is a problem](#happiness-is-a-problem) - [You are not special](#you-are-not-special) - [The value of suffering](#the-value-of-suffering) - [You are always choosing](#you-are-always-choosing) - [You're wrong about everything (but so am I)](#youre-wrong-about-everything-but-so-am-i) - [Failure is the way forward](#failure-is-the-way-forward) - [The importance of saying no](#the-importance-of-saying-no) ## Don't try > The real story of Bukowski's success: his comfort with himself as a failure. Bukowski didn't give a fuck about success. Fixation on the positive only serves to remind us over and over again of what we are not. **No truly happy person feels the need to stand in front of a mirror and recite that she's happy.** If you're dreaming of something all the time, then you're reinforcing the same unconscious reality over and over: that you are _not that_. Giving a fuck about more stuff is good for business. Giving too many fucks is bad for your mental health. It causes you to become overly attached to the superficial and fake. **The key to a good life is not giving a fuck, giving a fuck about only what is true and immediate and important.** ### The feedback loop from hell We feel bad about feeling bad. We feel guilty of feeling guilty. We get angry about getting angry. We get anxious about feeling anxious. Not giving a fuck is key, to accept that the world is totally fucked and that's all right, because it's always been that way and always will be. _**Wanting a positive experience is a negative experience; accepting negative experience is a positive experience**_ > **You'll never be happy if you continue to search for what happiness consists of.** – Albert Camus Everything in life is won through surmounting the associated negative experience. Any attempt to escape the negative, to avoid it or quash it or silence it, only backfires. How to pick and choose what matters to you and what does not matter to you is based on finely honed personal values. ### The subtle art of not giving a fuck **Not giving a fuck does not mean being indifferent; it means being comfortable with being different.** Indifferent people are afraid of the world and the repercussions of their own choices. That's why they don't make any meaningful choices. **You must give a fuck about something**. \"Fuck it,\" not to everything in life, but rather to everything _unimportant_ to life. They reserve their fucks for what truly matters. Friends. Family. Purpose. People give a fuck about them in return. **To not give a fuck about adversity, you must first give a fuck about something more important than adversity.** The problem with people who hand out fucks like ice cream at a goddamn summer camp is that they don't have anything more fuck-worthy to dedicate their fucks to. If a person has no problems. the mind automatically finds a way to invent some. If you don't find that meaningful something, your fucks will be given to meaningless and frivolous causes. **Whether you realise it or not, you are always choosing what to give a fuck about.** Maturity is what happens when one learns to only give a fuck about what's truly fuckworthy. Basically: our families, our best friends and our golf swing. ### So Mark, what the fuck is the point of this book anyway? When we believe that it's not okay for things to suck sometimes, then we unconsciously start blaming ourselves. **Practical enlightenment is becoming comfortable with the idea that some suffering is always inevitable.** The only way to overcome pain is to first learn how to bear it. ## Happiness is a problem **As with being rich, there is no value in suffering when it's done without purpose.** Pain and loss are inevitable and we should let go of trying to resist them. ### The misadventures of disappointment panda The greatest truths in life are usually the most unpleasant to hear. We suffer for the simple reason that suffering is biologically useful. It is nature's preferred agent for inspiring change. This constant dissatisfaction has kept our species fighting and striving, building and conquering. So no, our own pain and misery aren't a bug of human evolution; they're a feature. Pain is what teaches us what to pay attention. Our psychological pain is not necessarily always bad or even undesirable. And this is what's so dangerous about a society that coddles itself more and more from the inevitable discomforts of life: we lose the benefits of experiencing healthy doses of pain, a loss that disconnects us from reality of the world around us. ### Happiness comes from solving problems To be happy we need something to solve. Happiness is therefore a form of action. Happiness is a constant work-in-progress, because solving problems is a constant work-in-progress. The solutions to today's problems will lay the foundation for tomorrow's problems. For many people, life doesn't feel that simple: 1. *Denial*. Some people deny that their problems exist, they must constantly delude or distract themselves from reality. 2. *Victim Mentality*. Some choose to believe that there is nothing they can do to solve their problems. People deny and blame others for their problems for the simple reason that it's easy and feels good, while solving problems is hard and often feels bad. Remember, nobody who is actually happy has to stand in front of a mirror and tell himself that he's happy. ### Emotions are overrated Negative emotions are a _call to action_. When you feel them, it's because you're supposed to _do something_. Positive emotions, on the other hand, are rewards for taking the proper action. Emotions are merely signposts, _suggestions_, not commandments. We should make a habit of questioning them. To deny one's negative emotions is to deny the feedback mechanisms that help a person solve problems. Decision-making based on emotional, pretty much always sucks. Whatever makes us happy today will no longer make us happy tomorrow, because our biology always needs something more. Everything comes with an inherent sacrifice. What creates our positive experiences will define our negative experiences. ### Choose your struggle Lifelong fulfilment and meaning have to be earned through the choosing and managing of our struggles. You can't win if you don't play. What determines your success isn't \"What do you want to enjoy?\" **The relevant question is \"What pain do you want to sustain?\" The path to happiness is a full path of shitheaps and shame.** > The truth is, I thought I wanted something (become a rockstar), but it turns out I didn't. End of story. > > I wanted the reward and not the struggle. I wanted the result and not the process. I was in love with not the fight but only the victory. Who you are is defined by what you're willing to struggle for. Our struggles determine our successes. If you think at any point you're allowed to stop climbing, I'm afraid you're missing the point. Because the joy is in the climb itself. ## You are not special Sometime in the 1960s, developing \"high self-esteem\", having positive thoughts and feelings about oneself, became all the rage in psychology. People who _thought_ highly about themselves generally performed better and caused fewer problems. Many researchers and policymakers at the time came to believe that raising a population's self-esteem could lead to some tangible social benefits. The paradoxical mantra: every single one of us can be exceptional and massively successful. A generation later and the data is in: we're _not_ all exceptional. Feeling good about yourself doesn't mean anything unless you have a _good reason_ to feel good about yourself. **Adversity and failure are actually useful and even necessary for developing strong-minded and successful adults.** Teaching people to believe they're exceptional and to feel good about themselves no matter what leads to a population full of Jimmys. Jimmy, the delusional start-up founder. The Jimmy who spent so much talking about how good he was that he forgot to, you know, actually do something. People who feel entitled view every occurrence in their life as either an affirmation of, or a threat to, their own greatness. The true measurement of self-worth is not how a person feels about her _positive_ experiences, but rather how she feels about her _negative_ experiences. Entitled people, incapable of acknowledging their own problems openly and honestly, are incapable of improving their lives in any lasting or meaningful way. ### Things fall apart > The pain from my adolescence led me down a road of entitlement that lasted through much of my early adulthood. I was worthy. I felt entitled to say or do whatever I wanted, to break peoples trust, to ignore people's feelings, and then justify it later with shitty, half-assed apologies. The deeper the pain, the more helpless we feel against our problems, and the more entitlement we adopt to compensate those problems. 1. I'm awesome and the rest of you all suck, so I deserve special treatment. 2. I suck and the rest of you are all awesome, so I deserve special treatment. **Construing everything in life so as to make yourself out to be constantly victimised requires just as much selfishness as the opposite.** There is no such thing as a personal problem. Other people have had it in the past, have it now, and are going to have it in the future. **You are not special.** The easier and more problem-free our lives become, the more we seem to feel entitled for them to get even better. ### The tyranny of exceptionalism We're all, for the most part, pretty average people. There's no way we can process the tidal waves of information flowing past us constantly. The pieces that break through and catch our attention are the truly exceptional. The inundation of the exceptional makes people feel worse about themselves, makes them feel that they need to be more extreme, more radical, and more self-assured to get noticed or even matter. ### B-b-b-but, if I'm not going to be special or extraordinary, what's the point? We all _deserve_ greatness. \"Average\" has become the new standard of failure. Better to be at the extreme low end of the bell curve than to be in the middle, because at least there you're still special and deserve attention. **People who become great at something become great because they understand that they're not already great – they are mediocre, they are average – and that they could be so much better.** The pleasures of simple friendship, creating something, helping a person in need, reading a good book, laughing with someone you care about. **Ordinary things. They are ordinary for a reason: because they are what _actually_ matters.** ## The value of suffering Hiroo Onoda, the Japanese soldier who fought for many years refusing to believe the war did not end, choose to suffer for loyalty to a dead empire. Suzuki, the guy who discovered him, chose to suffer for adventure, no matter how ill-advised. Their suffering _meant_ something. And because it meant something, they were able to endure it, or perhaps even enjoy it. The question we should be asking is not \"How do I stop suffering?\" but \"Why am I suffering, for what purpose?\". ### The self-awareness onion 1. First layer of self-awareness onion is a simple understanding of one's emotions. \"This is when I feel happy.\" \"This makes me feel sad.\" \"This gives me hope.\" 2. The second layer of self-awareness onion is an ability to ask _why_ we feel certain emotions. This layer helps us understand the root cause of the emotions that overwhelm us. We can ideally do something to change it. 3. The third level is our personal values: _Why_ do I consider this to be success/failure? How am I choosing to measure myself? **Our values determine the nature of our problems, and the nature of our problems determines the quality of our lives.** Much of the advice out there simply tries to make people feel good in the short term, while the real long-term problems never get solved. Problems may be inevitable, but the _meaning_ of each problem is not. ### Rock star problems The question is not _whether_ we evaluate ourselves against others; rather, the question is by _what standard_ do we measure ourselves? Our values determine the metrics by which we measure ourselves and everyone else. If you want to change how you see your problems, you have to change what you value and/or how you measure failure/success. ### Shitty values 1. **Pleasure**. Ask to a drug addict or an adulterer if they are happy. Pleasure is the most superficial form of life satisfaction and therefore the easiest to obtain and the easiest to lose. **Pleasure is not the cause of happiness; rather it is the effect.** If you get the values and metrics right, then pleasure will naturally occur as a by-product. 2. **Material success**. Research shows that once one is able to provide for basic physical needs (food, shelter, and so on), the correlation between happiness and worldly success quickly approaches zero. Another issue with this value is the danger of prioritising it over other values such as honesty, nonviolence, and compassion. 3. **Always being right**. As humans, we're wrong pretty much constantly, you're going to have a difficult time rationalising all of the bullshit to yourself. People who base their self-worth on being right about everything prevent themselves from learning from their mistakes. It's far more helpful to assume that you're ignorant and don't know a whole lot. 4. **Staying positive**. Sometimes life sucks, and the healthiest thing you can do is admit it. Denying negative emotions leads to experiencing deeper and more prolonged negative emotions and to emotional dysfunction. To deny negativity is to _perpetuate_ problems rather than solve them. The trick with negative emotions is to 1) express them in a socially acceptable and healthy manner and 2) express them in a way that aligns with your values. > One day, in retrospect, the years of struggle will strike you as the most beautiful – Freud ### Defining good and bad values **Good values are 1) reality based, 2) socially constructive, and 3) immediate and controllable.** **Bad values are 1) superstitious, 2) socially destructive, and 3) not immediate or controllable.** People who are terrified of what others think about them are actually terrified of all the shitty things they think about themselves being reflected back. Good, healthy values are achieved internally. Bad values are generally reliant on external events. **What _self-improvement_ is really about: prioritising better values, choosing better thing to give a fuck about.** Because when you give better fucks, you get better problems. And when you get better problems, you get a better life. Five counterintuitive values: 1. **Responsibility**. Taking responsibility for everything that occurs in your life, regardless of who's at fault. 2. **Uncertainty**. The acknowledgement of your own ignorance and the cultivation of constant doubt in your own beliefs. 3. **Failure**. The willingness to discover your own flaws and the mistakes so that they may be improved upon. 4. **Rejection**. The ability to both say and hear no, thus clearly defining what you will and will not accept in your life. 5. **Contemplation of our own mortality**. Paying attention to one's own death keep all our other values in proper perspective. ## You are always choosing The only difference between a problem being painful or being powerful is a sense that we _chose_ it, and that we are responsible for it. ### The choice Individually, we are responsible for everything in our lives, no matter the external circumstances. **We don't always control what happens to us. But we _always_ control how to interpret what happens to us, as well as how we respond.** We are always responsible for our experiences. \"With great responsibility comes great power\". The more we choose to accept responsibility in our lives, the more power we will exercise over our lives. We are responsible for experiences that aren't our fault all the time. Fault is past tense. Responsibility is present tense. Nobody else is ever responsible for your situation but you. Many people may be to blame for your unhappiness, but nobody is ever _responsible_ for your unhappiness but you. This is because _you_ always get to choose how you see things, how you react to things, how you value things. ### Responding to tragedy Pain of one sort of another is inevitable for all of us, but we get to choose what it means to and for us. ### Genetics and the hand we're dealt A lot of people treat being born with a disadvantage as though they were screwed out of something highly valuable. They feel that there's nothing they can do about it, so they avoid responsibility for their situations. The beauty of poker is that while luck is always involved, luck doesn't dictate the long-term success of the game. ### Victimhood chic The responsibility/fault fallacy allows people to pass of the responsibility for solving their problems to others. One side effect of the Internet and social media is that it's become easier than ever to push responsibility onto some other group or person. The public sharing of \"injustices\" garners far more attention and emotional outpouring than most other events on social media, rewarding people who are able to perpetually feel victimised with ever-growing amounts of attention and sympathy. _Anyone_ is offended about _anything_. \"outrage-porn\": rather than report on real stories and real issues, the media find it much easier (and more profitable) to find something mildly offensive. The biggest problem with victimhood chic is that it sucks attention away from _actual_ victims. People get addicted to feeling offended all the time because it gives them a high; being self-righteous and morally superior feels _good_. Part of living in a democracy and a free society is that we all have to deal with views and people we don't necessary like. We should pick our battles carefully. We should prioritise values of being honest, fostering transparent, and welcoming doubt over the values of being right, feeling good, and getting revenge. ### There is no \"how\" You are _already choosing_, in every moment of every day, what to give a fuck about, so change is a simple as choosing to give a fuck about something else. It's just not easy. You're going to feel uncertain. Giving up a value you've depended on for years is going to feel disorienting. You'll feel like a failure. When you change your priorities, change your metrics, and stop behaving in the same way, you'll fail to meet that old, trusted metric and this immediately feel like some sort of fraud or nobody. You'll weather rejections. Many of the relationships in your life were built around values you've been keeping. Many of them will blow up in your face. ## You're wrong about everything (but so am I) I'm always wrong about everything, over and over and over again, and that's why my life improves. Growth is an endlessly _iterative_ process. **We are always in the process of approaching the truth and perfection without actually ever reaching truth or perfection.** We should seek to chip away at the ways that we're wrong today so that we can be a little less wrong tomorrow. Many people become so obsessed with being \"right\" about their life that they never end up actually _living it_. It's easier to sit in a painful certainty than to actually _test_ those beliefs to find out for sure. Certainty is the enemy of growth. We should be in constant search of doubt. Being wrong opens us up to the possibility of change. We don't _actually_ know what a positive or negative experience is. Don't trust your conception of positive/negative experiences. ### Architects of our own beliefs > Experiment: you give people a bunch of buttons and if a light turns on you give them points. After 30 minutes every user of the experiment comes up with an \"algorithm\" to determine how the light turns on. Candidates don't realise that the lights turn on randomly. The point of the experiment is to show how quickly the human mind is capable of coming up with and believing in a bunch of bullshit that isn't real. Our minds are constantly whirring, generating more and more associations to help us understand and control the environment around us. There are two problems. We mistake things we see and hear. Once we create meaning for ourselves, our brains are designed to hold on to that meaning. Even if we see evidence that contradicts the meaning we created, we keep believing anyway. ### Be careful what you believe We're in perpetual state of misleading ourselves and others for no other reason than that our brain is designed to be efficient, no accurate. Our brain is always biased toward what we feel to be true in that moment. In an effort to achieve coherence, our mind will sometimes, invent false memories. By linking our present experiences with that imagined past. Our beliefs are malleable and our memories are horribly unreliable. **If our hearts and minds are so unreliable, maybe we should be questioning our own intentions and motivations _more_.** ### The dangers of pure certainty The fact that people do everything \"right\" doesn't make them right. Worst criminals felt pretty damn good about themselves. In spite of the reality around them that gave them the sense of justification for hurting and disrespecting others. **Evil people never believe that _they_ are evil; rather, they believe that everyone else is evil** The more you try to be certain about something, the more uncertain and insecure you will feel. The more you embrace being uncertain and not knowing, the more comfortable you will feel in knowing you don't know. Uncertainty removes our judgments of others. Uncertainty is the root of all progress and growth. Before we can look at our values and prioritisations and change them into better, healthier ones, we must first become _uncertain_ of our current values. ### Manson's law of avoidance > The more something threatens your identity, the more you will avoid it. **I say _don't_ find yourself. I say _never_ know who you are. Because that's what keeps you striving and discovering.** ### Kill yourself There is little that is unique or special about your problems. My recommendation: _don't_ be special; _don't_ be unique. Redefine your metrics in mundane and broad ways. The narrower and rarer the identity you choose for yourself, the more everything will seem to threaten you. For that reason, define yourself in the simplest and most ordinary ways possible. Giving up your sense of entitlement and your belief that you're somehow owed something by this world. ### How to be a little less certain of yourself? * **What if I'm wrong?** We're all the world's worst observers of ourselves. The goal is merely to ask the question and entertain the thought at the moment, not to hate yourself. It's worth remembering that for any change to happen, _you must be wrong about something_. * **What would it mean if I were wrong?** The potential meaning behind our wrongness is often painful. It does call into question or values and forces us to consider what a different, contradictory value could potentially look and feel like. * **Would being wrong create a better or a worse problem than my current problem, for both myself and others?** The goal here is to take a look at which _problem_ is better. If it feels like it's you versus the world, chances are it's really just you versus yourself. ## Failure is the way forward ### The failure/success paradox The magnitude of your success is based on how many times you've failed at something. We can be truly successful only at something we're willing to fail at. My self-worth is based on my own behaviours and happiness. Shitty values involve tangible external goals outside of our control. Better values are process-oriented. \"honesty\", is never completely finished. The value is an ongoing, lifelong process that defies completion. ### Pain is part of the process Our pain often makes us stronger, more resilient, more grounded. Only when we feel intense pain when we're willing to take a look at our values and question why they seem to be failing us. You call it \"hitting bottom\" or \"having an existential crisis\". > VCR (Videocassette Recorder) problem: we sit and stare and shake our heads and say \"But _how_?\" When really, it's as simple as just doing it. VCR questions are funny because the answer appears difficult to anyone who has them and appears easy to anyone who does not. Life is about not knowing and then doing something away. ### The \"do something\" principle > Don't just sit there. _Do_ something. The answers will follow Action isn't just the effect of the motivation; it's also the cause of it. We assume that the steps occur in sort of chain reaction, like: Emotional inspiration -> Motivation -> Desirable action But the thing about motivation is that it's not only a three-part chain, but an endless loop Inspiration -> Motivation -> Action -> Inspiration -> Motivation -> Action -> Etc. So we can reorient the thing to be Action -> Inspiration -> Motivation **If you lack the motivation to make an important change in your life, _do something_ – anything, really – and then harness the reaction to that action as a way to begin motivating yourself.** When inspiration is seen as a reward rather than a prerequisite we propel ourselves ahead. We feel free to fail, and the failure moves us forward. ## The importance of saying no **The only way to achieve meaning and a sense of importance in one's life is through rejection of alternatives, a _narrowing_ of freedom.** In Western cultures people learn to pretend to be friends with people they don't actually like, to buy things they don't actually want. The economic system promotes such deception. ### Rejection makes your life better We all must give a fuck about _something_, in order to _value_ something. And to value something, we must reject what is _not_ that something. We are defined by what we choose to reject. If we reject nothing, we essentially have no identity at all. Having honesty in our lives is becoming comfortable with saying and hearing the word \"no\". ### Boundaries Unhealthy love is based on two people trying to escape their problems through their emotions for each other. Healthy love is based on two people acknowledging and addressing their own problems with each other's support. Unhealthy or toxic relationships have a poor sense of responsibility in both sides, inability to give and/or receive rejection. A healthy relationship have clear boundaries, there will be an open avenue of giving and receiving rejection when necessary. The mark of an unhealthy relationship is two people who try to solve each other's problems in order to feel good about themselves. A healthy relationship is when two people solve their own problems in order to feel good about each other. You both should support each other. But only because you _choose_ to support and be supported. **Acts of love are valid only if they're performed without conditions or expectations.** ### How to build trust When your highest priority is to always make ourselves feel good, or to always make our partner feel good, then nobody ends up feeling good. Without conflict, there can be no trust. Conflict exists to show us who is there for us unconditionally and who is just there for the benefits. No one trusts a yes-man. Without trust, the relationship doesn't actually _mean_ anything. If people cheat, it's because something other than the relationship is more important to them. Other factor in regaining trust after it's been broken is a practical one: a track record. ### Freedom through commitment We are actually often happier with less. Paradox of choice. The more options we're given, the less satisfied we become with whatever we choose. There are some experiences that you can have _only_ when you've lived in the same place for five years, when you've been with the same person for over a decade, when you've been working on the same skill or craft for half your lifetime. **Commitment, in its own way, offers wealth of opportunity and experiences that would otherwise never be available.** There is freedom and liberation in commitment. Commitment allows you to focus intently on a few degree of success that you otherwise would. The rejection of alternatives liberates us. ### ...And then you die Without death, everything would feel inconsequential, all experience arbitrary, all metrics and values suddenly zero. ### Something beyond our selves _The denial of death_ (book) essentially makes two points: 1. Humans are unique in that we're the only animals that can conceptualise and think about ourselves abstractly. We're blessed with the ability to imagine ourselves in hypothetical situations. That we all, at some point, become aware of the inevitability of our own death. 2. We essentially have two \"selves\". The physical self, and the conceptual self. Our identity, or how we see ourselves. **We are all aware on some level that our physical self, so we try to construct a conceptual self that will live forever.** \"Immortality projects\", projects that allow our conceptual self to live on way past the point of our physical death. Human civilisation is basically a result of immortality projects. _All the meaning in our life is shaped by this innate desire to never truly die._ When the prospect of our conceptual self outliving our physical self no longer seems possible or likely, death terror creeps back into our mind. This one is described in the book as mental illness. Our immortality projects are our values. Once we become comfortable with the fact of our own death, we can then choose our values more freely, unrestrained by illogical quest for immortality. ### The sunny side of death > The fear of death follows from the fear of life. A man who lives fully is prepared to die at any time. Confronting the reality of our own mortality is important because it obliterates all the crappy, fragile, superficial values in life. Death confronts an important question: What is your legacy? This is arguably the _only_ truly important question in our life. The only way to be comfortable with death is to understand and see yourself as something bigger than yourself. Happiness comes from the same thing: caring about something greater than yourself, believing that you are a contributing component in some much larger entity. Entitlement strips this away from us. Entitlement sucks attention inward, toward ourselves. Entitlement isolates us. The pampering of modern mind has resulted in a population that feels deserving of something without earning that something. > We're all going to die, all of us. What a circus! That alone should make us love each other, but it doesn't. We are terrorised and flattened by life's trivialities; we are eating up by nothing – Bukowski The primary lesson was this: **there is nothing to be afraid of. Ever.** ",
    "url": "/learning-notes/books/the-subtle-art-of-not-giving-a-fuck/",
    "relUrl": "/books/the-subtle-art-of-not-giving-a-fuck/"
  },"28": {
    "doc": "Understanding the Four Rules of Simple Design",
    "title": "Understanding the Four Rules of Simple Design",
    "content": "# [Understanding the Four Rules of Simple Design](https://www.goodreads.com/book/show/21841698-understanding-the-four-rules-of-simple-design) - [Coderetreats](#coderetreats) - [4 rules of simple design](#4-rules-of-simple-design) The idea of \"Good Design\" can often lead to a feeling that there is a pinnacle. Prefer to talk about \"Better Design\". The one constant that we know for sure in software development is that things are going to change. Simple design, is one that is easy to change, the key to a \"better design\". This does not mean that we should strive to make everything configurable. Quite the opposite. Rather than planning for change points, we build systems that can change at ANY point. ## Coderetreats A day where you are encouraged to try new things. You don't have to live with the mistakes you've made during learning, you can just throw it away. It allows people to relax into the idea of not finishing. Typically Coderetreats is about trying to solve the Conway's Game of Life automaton, deleting the code and changing the constraints by the hour. ## 4 rules of simple design 1. **Test pass.** If you can't verify that your system works, then it doesn't really matter how great your design is. This is about correctness and verification not automation specifically. When looking at your testing strategy, tend towards automated, and then towards making them fast(er). 2. **Express intent.** One of the most imporatnt qualities of a codebase, when it comes time to change, is how quickly you can find the part that should be changed. Paying attention to the names and how our code expresses itself. 3. **No duplication (DRY).** This is about _knowledge_ duplication (concepts). Instead of looking for code duplication, always ask yourself whether or not the duplication you see is an example of core knowledge in the system. 4. **Small.** It is important to look back and make sure that you don't have any extraneous pieces. * Is there any vestigial code that is no longer used * Are there any duplicate abstractions. Maybe there are abstractions with similar characteristics, missing another common abstraction that they can rely on. * Have we over-extracted. Once we do our cleanup, we can merge again extracted code. --- An important thing to realise about these rules is that they iterate over each other. ",
    "url": "/learning-notes/books/understanding-the-four-rules-of-simple-design/",
    "relUrl": "/books/understanding-the-four-rules-of-simple-design/"
  },"29": {
    "doc": "Gender stereotypes about intellectualability emerge early and influence children’s interests",
    "title": "Gender stereotypes about intellectualability emerge early and influence children’s interests",
    "content": "# [Gender stereotypes about intellectualability emerge early and influence children’s interests](https://sci-hub.uno/10.1126/science.aah6524) Common stereotypes associate high-level intellectual ability with men more than women. These stereotypes discourage women's pursuit of many prestigious careers. These stereotypes are endorsed by, and influence the interest of, children's as young as 6. Findings suggest that gendered notions of brilliance are acquired early and have an immediate effect on children's interests. Careers aspirations of young men and women are shaped by societal stereotypes about gender. The stereotype that men are better than women at maths impairs women's performance in this domain, and undermines their interest in mathematics-intensive fields. It is commonly assumed the high-level cognitive ability is present more often in men than in women. The earlier children acquire the notion that brilliance is a male quality. By the age of 6, girls are less likely than boys to believe that members of their gender are \"really, really smart\" (child-friendly way of referring to brilliance). **At the age of 6, girls in these studies begin to shy away from novel activities said to be for children who are \"really, really smart\".** Results on tests suggest that children's ideas about brilliance exhibit rapid changes over the period from ages 5 to 7. At 5, boys and girls associated brilliance with their own gender to a similar extent. Girls aged 6 and 7 were significantly less likely than boys to associate brilliance with their own gender. Older girls were actually more likely to select girls as having top grades than older boys where to select boys. Consistent with the reality that girls get better grades in school than boys at this age. Girls ideas about who is brilliant are not rooted in their perceptions of who performs well in school. In another study, there was an investigation whether children's gendered beliefs about brilliance shape their interests. The 6 and 7 year old girls own-gendered brilliance perceptions where lower than boys. These stereotyped beliefs mediated the relationship between children's gender and their interest for brilliant (versus persistent). **Young children's emerging notions about who is likely to be brilliant are one of the factors that guide their decisions about which activities to pursue.** Women are subject to stronger modesty norms than men, perhaps 6 and 7 year old girl's lower interest in games for brilliant children was due to an increase in concerns about modesty. ",
    "url": "/learning-notes/papers/gender-stereotypes-about-intellectualability/",
    "relUrl": "/papers/gender-stereotypes-about-intellectualability/"
  },"30": {
    "doc": "How Measurable is Success? Behavioural Science in the Computer Age",
    "title": "How Measurable is Success? Behavioural Science in the Computer Age",
    "content": "# [How Measurable is Success? Behavioural Science in the Computer Age](https://sci-hub.se/https://doi.org/10.1177/001112876901500408) The general public wants \"success\" or \"failure\" defined in black or white terms. A prime requisite for evaluating \"success\" is clearly defined and ascertainable goal, aim or design. Achievement would constitute \"success\". ## Lack of yardsticks Correction needs basic scientific research and universally acceptable and applicable standards. Edward W. Soden warns the rehabilitation of alcoholics has no definite yardstick by which success can be measured. Edward M. Taylor and Alexander W. McEachern wonder how effective probation is, so long as delinquency is described neither accurately nor in terms of what is done about it, it remains impossible to evaluate the success of probation services in regard to any particular type of delinquency. Charles L. Newman urges correctional treatment must be more than trial-and-error process, that in corrections, as in law-enforcement, are not allowed errors. Sophia M. Robinson sums up the predicament 1. There is no agreement on the definition of the disease 2. No definitive description of the characteristics of those who are vulnerable 3. No precise methods 4. No agreed-upon criteria for determining the relative success of the cure ## Meaningful criteria We need to develop a clear and meaningful criteria for determining \"success\" or \"failure\" in correction, and disseminate these standards widely. The general public uses seemingly clear-cut criterion for rehabilitating offenders, the effort is a \"failure\" if the offender has reoffended, and a \"success\" if he has not. A more practical criteria must be devised to evaluate \"progress\". We can outline a general behavioural equation without real value since we lack a practical means to measure the factors involved. ## The Algebra correction Even with these equation there will be many factors difficult measure such as genetic predisposition, how the mother context influences the child, what are the postnatal factors… Until we can exactly define and determine mensurable values to replace these symbols in the equation, it remains entirely unusable. ## Conclusion Eventually we'll be able to develop and utilise practical behavioural equations that yield reliable predictions and offer concrete measurements in \"black or white\" terms. In the not too distant future, our courts may look to the computer rather than to the probation officer. The defendant may undergo a series of clinical tests and measurements and the results will be fed into a machine to deliver findings. There will still be need for a live probation, parole, and correctional offices to implement and carry out the prescribed treatment plan. In the interim, **perhaps we might best concentrate on demonstrating whatever \"progress\" we can show**. ",
    "url": "/learning-notes/papers/how-measurable-is-success/",
    "relUrl": "/papers/how-measurable-is-success/"
  },"31": {
    "doc": "Managing The Development of Large Software Systems",
    "title": "Managing The Development of Large Software Systems",
    "content": "# [Managing The Development of Large Software Systems](http://www-scf.usc.edu/~csci201/lectures/Lecture11/royce1970.pdf) The context for this paper was related with the development of software packages for spacecraft mission planning, commanding and post-flight analysis. There are two essential steps of computer programs regardless of the size or complexity: Analysis and coding. However, manufacturing large software projects taking into account only these steps ia doomed to failure. A more grandiose approach precedes two levels of requirement analysis, separated by a design step and followed by a testing step. These steps are separated because they are planned and staffed differently in terms of resources. System Requirements ↘ Software Requirements ↘ Analysis ↘ Program Design ↘ Coding ↘ Testing ↘ Operations The iterative relationship between phases can be reflected in the following scheme. Each step progresses and the design is further detailed. Hopefully the interaction is confined in successive steps: System Requirements ↖ ↘ Software Requirements ↖ ↘ Analysis ↖ ↘ Program Design ↖ ↘ Coding ↖ ↘ Testing ↖ ↘ Operations The problem is that design iterations are never confined to successive steps: System Requirements ↘ ┌► Software Requirements │ ↘ │ Analysis │ ↘ └───────── Program Design ──┐ ↘ │ Coding │ ↘ │ Testing ◄┘ ↘ Operations ## Step 1: Program design come first A first step towards fixing the previous problem is to introduce a preliminary program design step between the software requirements and analysis phase. Critics can argue about the program designer being forced to design in a vacuum of software requirements without any analysis, and that the preliminary design may result in substantial error; and that would be correct, but it would miss the point. This is intended to assure the software won't fail because of storage, timing or data flux reasons. As analysis proceeds, collaboration must happen between the program designer and the analyst, so proper allocation of execution time and storage resources happens. ┌─────┐ ┌─► Document System Overview └─────┘ ├─► Design Database and Processors ↘ ├─► Allocate Subrutine Storage ┌─────┐ ├─► Allocate Subrutine Execution Times └─────┘ ├─► Describe Operating Procedures ↘ │ Preliminary Program Design ↘ ┌─────┐ └─────┘ ↘ ┌─────┐ └─────┘ The procedure is made from these steps: 1. Begin the design with program designers, not analysts or programmers. 2. Design, define and allocate the data processing modes even at risk of being wrong. 3. Write an overview document that is understandable, informative and current. ## Step 2: Document the design The first rule of managing software development is ruthless enforcement of documentation requirements. Management of software is simply impossible without a very high degree of documentation. System Requirements ↘ Software Requirements ─► Doc 1: Software Requirements ↘ Preliminary Software Design ─► Doc 2: Preliminary Design (Spec) ↘ Analysis ┌─► Doc 3: Interface Design (Spec) ↘ ├─► Doc 4: Final Design (Spec) Program Design ─┴─► Doc 5: Test Plan (Spec) ↘ Coding ─► Doc 4: Final Design (As Built) ↘ Testing ─► Doc 5: Test Plan (Spec), Test Results ↘ Operations ─► Doc 6: Operating Instructions Why so much documentation? 1. Each designer must communicate with interfacing designers, so prevents the designer from hiding behind the \"I'm 90 percent finished\" 2. During the early phase of software development the documentation **is** the specification and **is** the design. 3. The real monetary value of good documentation begins downstream 1. **During the testing phase**, the manager can concentrate on personnel on the mistakes in the program. 2. **During the operational phase**, the manager can use operational-oriented personnel to operate the program and to do a better job, cheaper. Without good documentation the software must be operated by those who built it. These people are relatively disinterested in operations and do not do as effective job as operations-oriented personnel. 3. **Following initial operations**, good documentation permits effective redesign, updating, and retrofitting in the field. ## Step 3: Do it twice After documentation, the second most important criterion for success revolves around whether the product is totally original. The final version delivered to the customer for operational deployment should be actually the second version. The point for this is that questions fo timing, storage, etc. which are otherwise matters of judgement, can now be studied with precision. Without this simulation, the project manager is at the mercy of human judgement. ┌─► Preliminary Design ┐ ┌─────┐ ├─► Analysis │ └─────┘ ├─► Program Design │ ↘ ├─► Coding ├────┐ ┌─────┐ ├─► Testing │ │ └─────┘ ├─► Usage ┘ │ ↘ │ │ Preliminary Program Design │ ↘ │ Analysis ◄────────────────────┤ ↘ │ Program Design ◄──────────┤ ↘ │ Coding ◄──────────────┤ ↘ │ Testing ◄─────────┤ ↘ │ Operations ◄──┘ ## Step 4: Plan, control and monitor testing The biggest user of project resources is the test phase. It is also the phase of greatest risk in terms of dollars and schedule. The previous recommendations, to design before analysis and coding, to document and to build a pilot are about uncovering and solving problems before entering the testing phase. Recommendations in order to plan for testing: 1. Many parts of tests process are best handled by test specialists. With good documentation it is feasible to use specialists, which do a better job of testing than the designer. 2. Most errors can be easily spotted by visual inspection, in the nature of proofreading the analysis and code. Do not use the computer to detect this kind of thing, it is too expensive. 3. Test every logic path at least once with some kind of numerical check. 4. Turn over the software to the test area for checkout purposes. ## Step 5: Involve the customer It is important to involve the customer at earlier points before the final delivery. Leaving the contractor free between requirement definition and operation means trouble. Following requirements definition where the insight, judgement, and commitment of the customer can bolster the development effort. System Requirements (Update) ◄─ System Requirements Generation ↘ Software Requirements ↘ Preliminary Software Design ─► PSR: Preliminary Software Review ──┐ │ Analysis ◄────────────────────────────────────────────────────┘ ↘ ┌─► CSR 1: Critical Software Review 1 ┐ Program Design ─┼─► CSR 2: Critical Software Review 2 ├───┐ └─► CSR N: Critical Software Review N ┘ │ Coding ◄──────────────────────────────────────────────┘ ↘ Testing ─► FSAR: Final Software Acceptance Review ┐ │ Operations ◄──────────────────────────────────┘ --- Worth noting that each item cost some additional sum of money. ",
    "url": "/learning-notes/papers/managing-the-development-of-large-software-systems/",
    "relUrl": "/papers/managing-the-development-of-large-software-systems/"
  },"32": {
    "doc": "MapReduce: Simplified Data Processing on Large Clusters",
    "title": "MapReduce: Simplified Data Processing on Large Clusters",
    "content": "# [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a _map_ function that processes a key/value pair to generate a set of intermediate key/value pairs, and a _reduce_ function that merges all intermediate values associated with the same intermediate key. Programs written in this functional style are automatically parallelised and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilise the resources of a large distributed system. ## Introduction The abstraction is inspired by the _map_ and _reduce_ primitives present in Lisp and many other functional languages. Map and reduce operations allows easy parallelisation at the same time that re-execution serves as the primary mechanism for fault-tolerance. ## Programming model The computation takes a set of _input_ key/value pairs, and produces a set of _output_ key/value pairs. The user of the MapReduce library expresses the computations as two functions: _Map_ and _Reduce_. _Map_, written by the user, takes an input pair and produces a set of _intermediate_ key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate Key _I_ and passes them to the _Reduce_ function. The _Reduce_ function, also written by the user, accepts and intermediary key _I_ and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per _Reduce_ invocation. ### Usage exmples * Distributed Grep: The map function emits a line if it matches a supplied pattern. The reduce function is an identity function that just copies the supplied intermediate data to the output. * Count of URL access frequency: The map function processes logs of a web page requests and outputs ``. The reduce function adds together all values for the same URL and emits a `` pair. * Distributed sort: The map functions extracts the key from each record, and emits a `` pair. The reduce function emits all pairs unchanged. ## Implementation One implementation may be suitable for a small shared-memory machine, another for a large NUMA multi-processor, and yet another for an even larger collection of networked machines. ### Execution overview The _Map_ invocations are distributed across multiple machines by automatically partitioning the input data into a set of _M splits_. The input splits can be processed in parallel by different machines. _Reduce_ invocations are distributed by partitioning the intermediate key space into _R_ pieces using a partitioning function. The number of partitions (_R_) and the partitioning function are specified by the user. Overall flow: 1. The MapReduce library in the user program **splits the input files into _M_ pieces** typically 16MB per piece. It then starts up many copies of the program on a cluster of machines. 2. One of the copies of the program is special, the master. The rest are workers that are assigned work by the master. There are _M_ map tasks and _R_ reduce tasks to assign. **The master picks idle workers and assigns each one a map task or reduce task.** 3. A worker who is assigned a map task reads the contents of the corresponding input split. It **parses key/value pairs out of the input data and passes each pair to the user-defined _Map_** function. The intermediate key/value pairs produced by the _Map_ function are buffered in memory. 4. Periodically, the **buffered pairs are written to local disk**, partitioned into _R_ regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers. 5. When a **reduce worker** is notified by the master about these locations, it **uses remote procedure calls to read the buffered data from the local disks of the map workers**. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. 6. The **reduce worker** iterates over the sorted intermediate data and **for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user's _Reduce_ function**. The output of the _Reduce_ function is appended to a final output file for this reduce partition. 7. When all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point the MapReduce call in the user program returns back to the user code. After successful completion, the output of the MapReduce execution is available in the _R_ output files (one per reduce task, with the file names as specified by the user). Users usually don't need to combine these _R_ output files into one file, they can either use another MapReduce call or use a distributed application to process them. ### Master data structures For each map task and reduce task, master stores the state (_idle_, _in-progress_, or _completed_) and the identity of the worker machine. For each completed map task, the master stores the locations and sizes of the _R_ intermediate file regions produced by the map task. Updates to this location and size information are received as map tasks are compelted. The information is pushed incrementally to workers that have _in-progress_ reduce tasks. ### Fault tolerance #### Worker failure The master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. Any map tasks completed by the worker are reset back to their initial _idle_ state, and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to _idle_ and becomes eligible for rescheduling. Completed map tasks are re-executed on failure because their output is stored on the local disks of the failed machine and is therefore inaccessible. Completed reduce tass do not need to be re-executed since their output is stored in a global file system. When a map task is executed first by worker _A_, and later executed by worker _B_ (because _A_ failed), all workers executing reduce tasks are notified of the re-execution. Any reduce task that has not already read the data from worker _A_ will read the data from worker _B_. The MapReduce master simply re-executes the work done by unreachable worker machines and continues to make forward progress. #### Master failure It's easy to make the master write periodic checkpoints of the master data structures. If the master task dies, a new copy can be started from the last checkpointed state. Current implementation aborts the MapReduce computation if the master fails. #### Semantics in the presence of failures The vast majority of _map_ and _reduce_ operators are deterministic, and the fact that the semantics are equivalent to a sequential execution in this case makes it very easy for programmers to reason about their program's behaviour. ### Localty Network bandwith can be conserved by taking advantage of the fact that the input data (managed by GFS) is stored on the local disks of the machine that make up the cluster. Most input data is read locally and consumes no network bandwidth. ### Task granularity The map phase is subdivided into _M_ pieces and reduce phase is divided into _R_ Pieces. Having each worker perform many different tasks improves dynamic load balancing and also speeds up recovery when a worker fails. ### Backup tasks One of the common causes that lengthens the total time taken for a MapReduce operation is a \"straggler\", a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation. There is a general mechanism to alleviate the problem of stragglers. When a MapReduce operation is close to completion, the master schedules backup executions of the remaining _in-progress_ tasks. The task is marked as completed whenever either the primary or the backup execution completes. A distributed sorting program can take up to 44% longer without the backup task mechanism. ## Refinement ### Partitioning function Users of MapReduce specify the number of reduce task/output files that they desire (_R_). A default partitioning function is provided that uses hashing (peg: `hash(key) mod R`), which tends to result in fairly well-balanced partitions. In some cases, it is useful to partition data by some other function of the key, for example sometimes the output keys are URLs, and is convenient to have a single host end up in the same output file. A partitioning function can be provided to MapReduce, for example `hash(Hostname(urlkey)) mod R`. ### Ordering guarantees Within a given partition, the intermediate key/value pairs are guaranteed to be processed in increasing key order, which makes it easy to generate a sorted output file per partition. ### Combiner function MapReduce allows the user to specify a _Combiner_ function that does partial merging of the data before it is sent over the network. The _Combiner_ function is executed on each machine that performs a map task. The difference between a reduce function and a combiner function is how the MapReduce library handles the output of the function. The output of a reduce function is written to the final output file. The output of a combiner function is written to an intermediate field that will be sent to a reduce task. ### Input and output types \"Text\" mode input treats each line as a key/value pair: the key is the offset in the file and the value is the contents of the line. Another common supported format stores a sequence of a key/value pairs sorted by key. Users can add support for a new input type by providing an implementation of a simple _reader_ interface, though most users just use one of a small number of predefined input types. In a similar fashion, a set of output types for producing data in different formats is supported. ### Side-effects Users of MapReduce may find convenient to produce auxiliary files. The application writer can make such side-effects atomic and idempotent. ### Skipping bad records Sometimes there are bugs in user code that can cause the _Map_ or _Reduce_ functions to crash deterministically on certain records, which prevents the whole MapReduce operation from completing. There is an optional mode of execution where the MapReduce library detects which records cause deterministic crashes and skips these records in order to make forward progress. Each worker process installs a signal handler that catches segmentation violations and bus errors. Before invoking a user _Map_ or _Reduce_ operation, the MapReduce library stores the sequence number of the argument in a variable. The signal handler sends a \"last gasp\" UDP packet that contains the sequence number to the master. When the master has seen more than one failure on a particular record, it indicates that the record should be skipped. ### Local execution To help facilitate debugging, profiling, and small-scale testing, there is an alternative implementation of the MapReduce library that sequentially executes all of the work for a MapReduce operation on the local machine. ### Status information The master runs an internal HTTP server and exports a set of status pages for human consumption. ### Counters The MapReduce library provides a counter facility to count occurrences of various events, like the count of total number of words processed, documents indexed, etc. The counter values from individual worker machines are periodically propagated to the master. The master aggregates the counter values from successful map and reduce tasks and returns them to the user code when the MapReduce operation is completed. The master eliminates the effects of duplicate executions of the same map or reduce task to avoid double counting. ## Experience MapReduce has been used at Google for a wide range of domains including: * Large-scale machine learning problems. * Clustering problems for the Google News and Froggle products. * Extraction of data used to produce reports of popular queries. * Extraction of the properties of web pages for new experiments and products. * Large-scale graph computations. MapReduce has been so successful because it makes it possible to write a simple program and run it efficiently on a thousand machines int he course of half an hour, greatly speeding up the development and prototyping cycle. It allows programmers who have no experience with distributed and/or parallel systems to exploit large amounts of resources easily. ### Large-scale indexing Probably the most significant use of MapReduce is the rewrite of the production indexing system that produces the data structures used for the Google web search service. Using MapReduce has provided several benefits: * The indexing code is simpler, smaller, and easier to understand, because the code that deals with fault tolerance, distribution, and parallelisation is hidden within the MapReduce library. * The performance of the MapReduce library is good enough that it can keep conceptually unrelated computations separate, instead of mixing them together to avoid extra passes over the data. * The indexing process has become much easier to operate, because most of the problems caused by machine failures, slow machines, and networking hiccups are dealt with automatically by the MapReduce library without operator intervention. ## Related work MapReduce exploits a restricted programming model to parallelise the user program automatically and to provide transparent fault-tolerance. By restricting the programming model, the MapReduce framework is able to partition the problem into a larger number of fine-grained tasks. These tasks are dynamically scheduled on available workers so that faster worker process more tasks. The restricted programming model also allows to schedule redundant execution of tasks near the end of the job which greatly reduces completion time in the presence of non-uniformities. ## Conclusions The success of MapReduce is attributed to: 1. The model is easy to use, even for programmers without experience with parallel and distributed systems, since it hides the details of parallelisation, fault-tolerance, locality optimisation, and load balancing. 2. A large variety of problems are easily expressible as MapReduce computations. 3. There is an implementation of MapReduce that scales to large clusters of machines compromising thousand of machines. Some learnings: 1. Restricting the programming model makes it easy to parallelise and distribute computations and to make such computations fault-tolerance. 2. Network bandwidth is a scarce resource. A number of optimisations in the system have been done towards reducing the amount of data sent across the network. 3. Redundant execution can be used to reduce the impact of slow machines, and to handle machine failures and data loss. ",
    "url": "/learning-notes/papers/map-reduce/",
    "relUrl": "/papers/map-reduce/"
  },"33": {
    "doc": "Out of the Tar Pit",
    "title": "Out of the Tar Pit",
    "content": "# [Out of the Tar Pit](http://curtclifton.net/papers/MoseleyMarks06a.pdf) - [Complexity](#complexity) - [Simplicity is hard](#simplicity-is-hard) - [Approaches to understanding](#approaches-to-understanding) - [Causes of complexity](#causes-of-complexity) - [Complexity caused by state](#complexity-caused-by-state) - [Complexity caused by control](#complexity-caused-by-control) - [Complexity caused by code volume](#complexity-caused-by-code-volume) - [Other causes of complexity](#other-causes-of-complexity) - [Classical approaches to managing complexity](#classical-approaches-to-managing-complexity) - [Object-orientation](#object-orientation) - [Functional programming](#functional-programming) - [Logic programming](#logic-programming) - [Accidents and essence](#accidents-and-essence) - [Recommended general approach](#recommended-general-approach) - [Ideal world](#ideal-world) - [Theoretical and practical limitations](#theoretical-and-practical-limitations) - [The relational model](#the-relational-model) - [Structure](#structure) - [Manipulation](#manipulation) - [Integrity](#integrity) - [Data independence](#data-independence) - [Extensions](#extensions) - [Functional relational programming](#functional-relational-programming) - [Architecture](#architecture) - [Benefits of this approach](#benefits-of-this-approach) - [Types](#types) The biggest problem in the development and maintenance of large-scale software systems is complexity. The major contributor to this complexity is the handling of _state_. Other closely related are _code volume_, and explicit _flow of control_. Object-oriented programming tightly couples state with related behaviour, and functional programming eschews state and side-effects all together. ## Complexity In the [\"No Silver Bullet\" paper](http://worrydream.com/refs/Brooks-NoSilverBullet.pdf) Brooks identified four properties of software which make software hard: Complexity, Conformity, Changeability and Invisibility. Complexity is _the_ root of the vast majority of problems with software today, like the ones about unreliability, late delivery or lack of security. ### Simplicity is hard The type of complexity we are discussing in this paper is the one which makes large systems _hard to understand_. The one that causes huge resources in _creating and maintaining_ such systems. This type complexity has nothing to do with the complexity related with a _machine executing a program_. ## Approaches to understanding * **Testing**: Attempting to understand the system from the outside with observations about how it behaves. Performed either by a human or by a machine. * **Informal reasoning**: Understand the system by examining it from the inside. Reasoning is the most important by far, mainly because there are limits to what can be achieved by testing, and because informal reasoning is _always_ used. The key problem with testing is that tells you _nothing at all_ when it is given a different set of inputs. > Testing can be used very effectively to show the presence of bugs but never to show their absence – Dijkstra _Informal reasoning_ is limited in scope, imprecise and hence prone to error, as well as _formal reasoning, which is dependant upon the accuracy of specification. It may often be prudent to employ testing _and_ reasoning together. Is because of the limitations of these approaches that _simplicity_ is vital. ## Causes of complexity ### Complexity caused by state #### Impact of state in testing State affects all types of testing. The common approach to testing a stateful system is to start up such that it is in some kind of \"clean\" or \"initial\" state, perform the desired tests upon the assumption that the system would perform in the same way every time the test is run. Sweeping the problem of state under the carpet. It's not _always_ possible to \"get away with it\", some sequence of events can cause the system to \"get into a bad _state_\", then thing can go wrong. Even though the number of _inputs_ may be very large, the number of possible _states_ the system can be is often even larger. #### Impact of state in informal reasoning The mental processes which are used to do this informal reasoning often revolve around a case-by-case mental simulation of behaviour. As the number of states grows, the effectiveness of this mental approach buckles quickly. Another issue for informal reasoning is _contamination_. Consider a system made up of procedures, some of which are stateful and others which aren't. If the procedure makes use of any procedure which _is_ stateful, _even indirectly_, the procedure becomes _contaminated_. The more we can do to _limit_ and _manage_ state, the better. ### Complexity caused by control Control is about _order_ in which things happen. We do not want to have to concerned about this. When control is an implicit part of the language, every single piece of program must be understood in that context. When a programmer is forced to specify the control, he or she is being forced to specify an aspect of _how_ the system should work rather than simply _what_ is desired. They are being forced to _over-specify_ the problem. A control-related problem is concurrency, which affects _testing_ as well. Concurrency is normally specified _explicitly_ in most languages. The most common model is \"shared-state concurrency\" in which explicit synchronisation is provided. The difficulty for informal reasoning comes from adding further the _number of scenarios_ that must mentally be considered as the program is read. Running a test in the presence of concurrency tells you _nothing at all_ about what will happen the next time you run that very same test. ### Complexity caused by code volume This is a secondary effect. Much code is simply concerned with managing _state_ or specifying _control_. It is the easiest form of complexity to _measure_, and it interacts badly with the other causes of complexity. Complexity definitively _does_ exhibit nonlinear increase with size (of the code) so it's vital to reduce the amount of code to an absolute minimum. ### Other causes of complexity There are other causes like: duplicated code, dead code, unnecessary abstraction, missed abstraction, poor modularity, poor documentation... All these come down to the following principles * **Complexity breeds complexity**. Complexity introduced as a _result of_ not being able to clearly understand a system, like _Duplication_. This is particularly common in the presence of time pressures. * **Simplicity is hard**. Simplicity can only be attained if it is recognised, sought and priced. * **Power corrupts**. In the absence of language-enforced guarantees mistakes (and abuses) _will_ happen. We need to be very wary of any language that even _permits_ state, regardless of how much it discourages its use, the more _powerful_ a language, the harder it is to _understand_ systems constructed in it. ## Classical approaches to managing complexity ### Object-orientation Imperative approach to programming. #### State An object is seen as consisting of some state together with a set of procedures for accessing and manipulating that state. _Encapsulation_ allows the enforcement of integrity constraints over an object's state by regulating access to that state through access procedures (\"methods\"). One problem is that if several of the access procedures access or manipulate the same bit of state, then there may be several places where a given constraint must be enforced. Another major problem is that encapsulation-based integrity constraint enforcement is strongly biased toward single-object constraints, and it's awkward to enforce more complicated constraints involving multiple objects. ##### Identity and state Each object is seen as being uniquely identifiable entity regardless of its attributes. This is known as _intentional_ identity (in contrast with _extensional_ identity in which things are considered the same if their attributes are the same). Object identity _does_ make sense when objects are used to provide a (mutable) stateful abstraction. However, where mutability is _not_ required, the OOP approach is the creation of \"Value Objects\". It is common to start using custom access procedures to determine whether two objects are equivalent. There is no guarantee that such domain-specific equivalence concepts conform to the standard idea of an equivalence relation (peg: no guarantee of transitivity). The concept of _object identity_ adds complexity to the task of reasoning about systems developed in OOP. ##### State in OOP All forms of OOP rely on state, and general behaviour is affected by this state. OOP does not provide an adequate foundation for avoiding complexity. #### Control Standard sequentual control flow and explicit classical \"shared-state concurrency\" cause standard complexity errors. A slight variation of \"message-passing\" Actor model canis not lead to easier informal reasoning but is not widespread. #### Summary Conventional imperative and object-oriented programs suffer greatly from both state-derived and control-derived complexity. ### Functional programming It has its roots in the completely stateless lambda calculus of Church. #### State Functional programming languages are often classified as \"pure\" which shun state and side-effects completely, and \"impure\", whilst advocating the avoidance of state and side-effects in general, do permit their use. By avoiding state (and side-effects) the entire system gains the property of _referential transparency_, a function will _always_ return exactly the same result. Because of this, testing does become far more effective. By avoiding state, informal reasoning becomes much more effective. #### Control Functional languages do derive one slight benefit when it comes to control because they encourage a more abstract use of control functionals (such as `fold` / `map`) rather than explicit looping. There are also concurrent versions. #### Kinds of state By \"state\" what is really meant is _mutable state_. Language which do not support or discourage mutable state it is common to achieve somewhat similar effects by means of passing extra parameters to procedures (functions). There is no reason why the functional style of programming cannot be adopted in stateful languages. Whatever the language being used, there are large benefits to be had from avoiding, implicit, mutable state. #### State and modularity State permits a particular kind of modularity, within a stateful framework it is possible to add state to any component without adjusting the components which invoke it. Within a functional framework the same effect can only be achieved by adjusting every single component that invokes it to carry the additional information around. In a functional approach you are forced to make changes to every part of the program that could be affected, in the stateful you are not. In a functional program _you can always tell exactly what will control the outcome of a procedure_ simply by looking at the arguments supplied where it is invoked. In a stateful program you can never tell what will control the outcome, and _potentially_ have a look at every single piece of code in the _entire system_ to determine this information. The trade-off is between _complexity_ and _simplicity_. The main weakness of functional programming is that problem arise when the system to be built must maintain state of some kind. #### Summary Functional programming goes along way towards voiding the problems of state-derived complexity. ### Logic programming Together with functional programming _declarative_ style of programming places emphasis on _what_ needs to be done rather than exactly _how_ to do it. #### State Logic programming makes no use of mutable state, and for this reason profits from the same advantages as functional programming. #### Control Operational commitment ot _process_ the program is the same order as it read textually (depth first). Particular ways of writing down can lead to non-termination, which leads inevitably to the standard difficulty for informal reasoning caused by control flow. #### Summary Despite the limitations it offers the ability to escape from the complexity caused by control. ## Accidents and essence Brooks defined difficulties of \"essence\" as those inherent in the nature of software and classified the rest as \"accidents\". * **Essential complexity** is inherent in the _users problem_. * **Accidental complexity** is complexity with which the development team would not have to deal in the ideal world. Any real development _will_ need to contend with _some_ accidental complexity. Complexity itself is not inherent (or essential) property of software. Much complexity that we do see in existing software is not essential (to the problem). The goal of software engineering must be both to eliminate as much of accidental complexity as possible and to assist with the essential complexity part of it. ## Recommended general approach Complexity could not be _possibly_ avoided even in the ideal world. ### Ideal world In the ideal world we would not be concerned about performance. Even in the ideal world, we would need to start with a set of _informal requirements_ from the prospective users. We would ultimately need something to _happen_, and we are going to need some _formality_.We are going to need to derive formal requirements from the informal ones. The next step is simply to _execute_ these formal requirements directly. Effectively what we have just described is in fact the very _essence_ of _declarative programming_, specify _what_ you require, not _how_ it must be achieved. #### State in the ideal world The aim for state is to get rid of it hoping that most will be _accidental state_. All data will be provided (_input_) or _derived_. **Input data** Included in the informal requirements and as such is deemed _essential_. * If the system may be required to refer to the data in the future then it is _essential state_. * If there is no such possibility and the data is designed to have some side-effect, the data need not to be maintained at all. **Essential derived data (immutable)** Data can always be re-derived, we do _not_ need to store it in the ideal world (accidental state). **Essential derived data (mutable)** This can be excluded and hence corresponds to _accidental state_. **Accidental derived data** State that is _derived_ but _not_ in the users' requirements is _accidental state_. As a summary, mutable state can be avoided, even in the ideal world we _are_ going to have _some_ essential state. _Accidental state_ can be excluded from the ideal world (by re-deriving the data as required). The vast majority of state isn't needed. One effect of this is that _all_ the state int he system is _visible_ to the user. #### Control in the ideal world Control generally can be completely omitted, so is considered _accidental_. We should not have to worry about the control flow. _Results_ should be independent of the actual control mechanism. This is what logic programming taught us. #### Summary It is clear that a lot of complexity is _accidental_. ### Theoretical and practical limitations #### Formal specification languages _Executable specifications_ would be ideal. Declarative programming paradigm proposed approaches have been proposed as approaches for executable specifications. In the ideal world, specifications derived _directly_ from the users' informal requirement is critical. Formal specification has been categorised in two main camps: * **Property-based** approaches focus on _what_ is required rather than _how_ the requirements should be achieved (_algebraic_ approaches such as Larch and OBJ). * **Model-based (or state-based)** approaches construct a model and specify how that model must behave. These approaches specify how a stateful, imperative language solution must behave to satisfy the requirements. There have been arguments against the concept of executable specifications. The main objection is that requiring a specification language to be executable can directly restrict its expresiveness. In response, a requirement for this kind of expressivity does not seem to be common in many problem domains. Secondly where such specifications _do_ occur they should be maintained in their natural form but supplemented with a _separate_ operational component. _Property-based_ approaches have the greatest similarity to _executable specifications_ in the ideal world. A second problem is that even when specifications _are_ directly executable, this can be impractical for efficiency reasons. It may require some accidental components. ##### Ease of expression Immutable, derived data would correspond to _accidental state_ and could be omitted (you could derive data on-demand). There are occasionally situations where (using on-demand derivation) does not give rise to the most natural modelling of the problem. An example of this is the derived data representing the position state of an opponent in an interactive game. It is at all times _derivable_ by a function of both all prior movements and the initial starting positions, but this is not the way it is most naturally expressed. ##### Required accidental complexity * **Performance** making use of accidental state and control can be required for efficiency. * **Ease of expression** making use of accidental state can be the most natural way to express logic in some cases. #### Recommendations _Avoid_ state and control where they are not absolutely and truly essential. When needed, such complexity must be _separated_ out from the rest of the system. ##### Required accidental complexity * **Performance** _avoid_ explicit management of the accidental state, restrict ourselves to simply _declaring_ what accidental state should be used, and leave it to a completely separate infrastructure to maintain. We can effectively forget that the _accidental state_ even exists. * **Ease of expression** this problem arises when derived (_accidental_) state offers the most natural way to express part of the logic of the system. ##### Separation and the relationship between components First separate _all_ complexity of any kind from the pure logic (_logic_ / _state_ split). Second, divide complexity between _accidental_ and _essential_. The essential bits correspond to the requirements. \"Separate\" is basically advocating clean distinction between all three of these components. Additionally, it advocates for a split between the state and control components of the \"Useful\" Accidental Complexity, but this is less important. It may be ideal to use _different languages_ for each. The weaker the language, the more simple it is to reason about. Separation is important because it \"restricts the power\" of each of the components independently. It facilitates reasoning about them as a whole. Recommended architecture (arrows show static references) ┌───────────────┬────────────────────┐ ┌──│ │ │ │ │ ─┼─► Essential Logic │ │ │ Accidental │ │ │ │ State and │──────────┼─────────┤ │ │ Control │ ▼ │ │ │ ─┼─► Essential State │ │ │ │ │ │ └───────────────┴──────────────────┬─┘ │ Language and Infrastructure │ └─────────────────────────────────────┘ * **Essential state** This is the foundation of the system. It can _make no reference_ to _either_ of the other parts. Changes in either of the other specifications may never require changes to the specification of essential state. * **Essential logic** This is referred as \"business\" logic, it's expressed, in terms of the state, what must be true. It does _not_ say anything about how. Changes to the essential state specification may require changes to the logic specification, and changes to the logic specification may require changes to the specification for accidental state and control. It should _make no reference_ to _any_ part of the accidental specification. * **Accidental state and control** The least important of the system. Changes to it can _never_ affect the other specifications. ## The relational model This has nothing _intrinsically_ to do with databases. Relational features as _structuring_ and _manipulating_ data, and maintaining _integrity_ and consistency of state, are applicable to state and data in any context. Not only that, it does also allow clear separation between the logical and physical layers of the system (_data independence_). ### Structure #### Relations A relation is a homogeneous _set of records_, each one of them composed by a set of _attributes_. Relations can be: * **Base relations** stored directly * **Derived relations** also known as _Views_, defined in terms of other relations. A relation can be considered a _value_, and consider mutable state not as a \"mutable relation\" but rather a _variable_ which at any time can contain a particular relation _value_ (_relation variables_ or _relvars_). #### Structuring benefits of relations, access path independence Structuring data using relations is appealing because there is no need for up-front decisions need to be made about the _access paths_ to be queried later. The ability of the relational model to _avoid_ access paths completely was one of the primary reasons for its success. There are disturbing similarities between the data structuring approaches of OOP and XML. ### Manipulation There are two different mechanisms for expressing the manipulation aspects of the relational model, the relational calculus and the relational algebra. Relational algebra consists of the following operations: * **Restrict** unary operation, select of a subset. * **Project** unary operation, creates a new relation corresponding to the old relation. * **Product** binary operation, cartesian product. * **Union** binary operation, all records in either argument relation. * **Intersection** binary operation, all records in both argument relations. * **Difference** binary operation, all records in the first but no the second argument relation. * **Join** binary operation, all possible records that result from matching identical attributes. * **Divide** ternary operation, all records of the first argument which occur in the second argument associated with _each_ record of the third argument. One significant benefit of this manipulation language is that it has the property of _closure_, all operations can be nested in arbitrary ways. ### Integrity Simply by specifying in a purely declarative way a set of constraints must hold. The most common types of constraint are _primary_ keys and _foreign_ keys. DBMSs provide _imperative_ mechanisms such as triggers for maintaining integrity. ### Data independence Is the principle of separating the logical model from the physical storage representation. ### Extensions Relational algebra is a restrictive language in computational terms, and is normally augmented. Common extensions include: * **General computation capabilities** for example simple arithmetical operations. * **Aggregate operators** `MAX`, `MIN`, `COUNT`, `SUM`, etc. * **Grouping and summarisation capabilities** * **Renaming capabilities** ## Functional relational programming FRP is purely hypothetical, it has not in any way been proven in practice. In FRP all _essential state_ takes the form of relations, and the _essential logic_ is expressed using relational algebra extended with (pure) user-defined functions. The goal behind the FRP is the _elimination of complexity_. The components of an FRP system. ┌───────────────┬────────────────────┬─────────┐ │ │▒▒▒▒▒▒▒▒▒░░░░░░░░░░░│ │ │ │▒ Essential Logic ─┼─► │ │ Accidental │▒▒▒▒▒▒▒▒▒░░░░░░░░░░░│ │ │ State and │────────────────────┤ Other │ │ Control │░░░░░░░░░░░░░░░░░░░░│ │ │ │░ Essential State ◄─┼─ │ │ │░░░░░░░░░░░░░░░░░░░░│ │ └───────────────┴────────────────────┴─────────┘ ▒ Functional ░ Relational ### Architecture _Separate_ specifications for each of the following components: * **Essential state** A relational definition of the stateful components of the system. * **Essential logic** Derived-relation definitions, integrity constraints and (pure) functions. * **Accidental state and control** A declarative specification of a set of performance optimisations for the system. * **Other** Interfaces o the outside world. In contrast with the object-oriented approach, FRP emphasises a clear _separation_ of state and behaviour. #### Essential state (\"state\") Essential state for the system in terms of base relvars (in FRP state store solely in terms of relations). FRP strongly encourages data to be treated as essential state _only_ when it has been _input directly by a user_. #### Essential logic (\"behaviour\") Compromises both functional and algebraic relational parts. The definitions can make use of an arbitrary set of pure user-defined functions. Logic specifies a set of _integrity constraints_, boolean expressions which must hold at all times. #### Accidental state and control (\"performance\") Consists of a series of isolated performance \"hints\". They should be declarative in nature and intended to provide guidance to the infrastructure. It provides a means to specify _what state_ (accidental) should exist. amd it provides means to specify _what physical storage mechanisms_ should be used. #### Other (interfacing) Basically _interfacing with the outside world_. All input must be converted into relational assignments (replace the old relvar values in the _essential state_ with new ones), and all output (and side-effects) must be driven from changes to the values of relvars. There will probably be a requirement for a series of _feeder_ (or _input_) and _observer_ (or _output_) components. These components will be of a _minimal_ nature, performing only the necessary translations to and from relations. * **Feeders** convert input into relational assignments (causes changes to the _essential state_). _Feeders_ will need to specify them in some form of _state manipulation language_ provided by the infrastructure. * **Observers** generate output in response to changes which they observe in the values of the (derived) relvars. At the minimum, they only need to specify the _name_ of the relvar which they wish to observe. Infrastructure will ensure that the observer is invoked. _Observers_ act both as _live-queries_ and also _triggers_. #### Infrastructure The FRP _system_ is the specification. _Infrastructure_ is what is needed to execute this specification. The requirements are: **Infrastructure for essential state** 1. Store and retrieve data in the form of relations to named relvars. 2. A state manipulation language which allows the stored relvars to be updated. 3. Optionally secondary storage in addition to the primary one. 4. A base set of generally useful types. **Infrastructure for essential logic** 1. Evaluates relational expressions. 2. Provides a base set of useful functions. 3. Language to allow specification of the user-defined functions in the FRP system. 4. Optionally a means of type inference. 5. Means to express and enforce integrity constraints. **Infrastructure for accidental state and control** 1. Specify which _derived_ relvars should actually be stored. Along with the ability to store such relvars _and ensure_ they are _up-to-date at all times_. 2. _Flexible_ physical storage mechanisms to be used by a relvar. **Infrastructure for feeders and observers** For _feeders_ to be able to process relational assignment commands. It is useful to include the ability to accept commands which specify new relvar values in terms of their previous values (`INSERT`, `UPDATE`, `DELETE`) . For _observers_ to be able to supply new value of a relvar whenever it changes. Extensions that could be useful are the ability to provide deltas, throttling and coalescing capabilities. The ability to access arbitrary _historical_ relvar values would obviously be a useful extension in some scenarios too. --- It is possible to develop an FRP infrastructure in _any_ general purpose language, object-oriented, functional or logic. ### Benefits of this approach #### Benefits for state It _avoids_ useless accidental state, and to avoid ever getting into a \"bad state\". From the point of view of the logic, the _essential state_ is seen as a _constant_. The _functional_ component (of the logic) has _no access_ to any state at all, it is referentially transparent. There are major advantages from adopting relational representation of data such no concern with data access paths. Finally, integrity of constraints provide big benefits for maintaining consistency of state in a _declarative_ manner. #### Benefits of control Control is _avoided_ completely. In FRP the logic consist simply of a set of equations with no intrinsic ordering or control flow at all. An _infrastructure_ which supports FRP may well make use of _implicit_ parallelism to improve its performance. It is also much easier to create distributed implementations. #### Benefits of code volume It _avoids_ useless accidental complexity and that leads to less code. It reduces the harm that large volumes of code through its use of _separation_. #### Benefits of data abstraction Un-needed data abstraction actually represents another common cause of complexity via: * **Subjectivity** Pre-existing data abstractions are too easily lead to _inappropriate_ reuse. * **Data hiding** Erodes the benefits of referential transparency. Data which _does_ get used is _hidden_ at the function call site. This leads to problems for testing as well as informal reasoning in ways very similar to state. One of the primary strengths of the relational model involves only minimal commitment to any subjective groupings. #### Other benefits Potential benefits include performance and development teams themselves could be organised around different components. ### Types FRP provides a limited ability to define new user types for use in the _essential state_ and _essential logic_ components. It _does not_ permit the creation of new product types, this is in order to _avoid_ any unnecessary data abstraction. ",
    "url": "/learning-notes/papers/out-of-the-tar-pit/",
    "relUrl": "/papers/out-of-the-tar-pit/"
  },"34": {
    "doc": "Reflections on Trusting Trust",
    "title": "Reflections on Trusting Trust",
    "content": "# [Reflections on Trusting Trust](https://www.archive.ece.cmu.edu/~ganger/712.fall02/papers/p761-thompson.pdf) To what extent should one trust a statement of a program is free of Trojan horses? Perhaps it is more important to trust the people who wrote the software. ## Stage I The problem is to write the shortest source program that, when compiled and executed, will produce as output an exact copy of its source. It has to have two important properties: 1. This program can be easily written by another program 2. This program can contain an arbitrary amount of excess baggage that will be reproduced along with the main algorithm ```c char s[] = { '\\t', '0', '\\n', '}', ';', '\\n', '\\n', '/', '*', '\\n', (213 lines deleted) 0 } /* * The string s is a * representation of the body * of this program from '0' * to the end. */ main() { int i; printf(\"char\\ts[] = {\\n\"); for(i = 0; s[i]; i++) printf(\"\\t%d, \\n\", s[i]); printf(\"%s\", s); } ``` ## Stage II The C compiler is written in C. C allows individual characters in the string to be escaped to represent unprintable characters. For example ```c \"Hello world\\n\" ``` It \"knows\" in a completely portable way what character code is compiled for a new line in a character set. ```c c = next(); if(c != '\\\\') return(c); c = next(); if(c == '\\\\') return('\\\\'); if(c == 'n') return('\\n'); ``` Suppose we wish to alter the C compiler to include `'\\v'` to represent the vertical tab character. We must \"train\" the compiler. After it \"knows\" what `'\\v'` means, then our new change will become legal C. ```c c = next(); if(c != '\\\\') return(c); c = next(); if(c == '\\\\') return('\\\\'); if(c == 'n') return('\\n'); if(c == 'v') return('\\v'); ``` Now, the old compiler won't accept the new source. We look up on an ASCII chart that a vertical tab is decimal 11, we can alter our source code so the old compiler accepts the new source. ```c c = next(); if(c != '\\\\') return(c); c = next(); if(c == '\\\\') return('\\\\'); if(c == 'n') return('\\n'); if(c == 'v') return(11); ``` You simply tell it once, then you can use this self-referencing definition. ## Stage III We can do a simple modification to the compiler that will deliberately miscompile source. If this were not deliberate, it would be called compiler \"bug\". Since it is deliberate it should be called a \"Trojan horse\". ```c compile(s); char *s; { if(match(s, \"pattern\")) { compile(\"bug\"); return; } } ``` First we compile the modified source with the normal C compiler to produce a bugged binary. We install this binary as the official C. We can now remove the bugs from the source of the compiler and the new binary will reinsert the bugs whenever it is compiled. Programs will remain bugged with no trace in source anywhere. ## Moral You can't trust code that yu did not totally create yourself. No amount of source-level verification or scrutiny will protect you from using untrusted code. The act of breaking into a computer system has to have the same social stigma as breaking into a neighbour's house. It should not matter that the neighbour's door is unlocked. The press must earn that misguided use of a computer is no more amazing than a drunk driving of an automobile. ",
    "url": "/learning-notes/papers/reflections-on-trusting-trust/",
    "relUrl": "/papers/reflections-on-trusting-trust/"
  },"35": {
    "doc": "8 Lines of Code",
    "title": "8 Lines of Code",
    "content": "# [8 Lines of Code](https://www.infoq.com/presentations/8-lines-code-refactoring) Simplicity is good because > I am stupid to work otherwise. Fancy code befuddles me. The problem with magic: Things happen that I don't understand. Sometimes magic doesn't work, so I don't know how it works. Annotations is an example of magic. Here is where simplicity goes out of window. This is not simple. This _simple_ thing is actually increasing the complexity massively. How do you explain all these magic to a person without much experience? How do you explain a _Dynamic Proxy_? With Dynamic Proxy you need to do _special things_ to make it work, like adding `virtual` to every method or avoiding returning `this`. People grow with these things and will start spreading this kind of behaviour without really understanding why they do it. This is actually cargo cult. Some teams use frameworks and tools that have so much magic that they start looking for people that know about those things because teaching all that magic takes a lot of time. With annotations those simple lines of code become way more complex than it should be. Love platforms, hate frameworks. Frameworks have a tendency to introduce magic in your system. > If you find you need an extension to your IDE to understand what's going on. It's probably not simple. What's the core problem behind a problem (like wanting a _Dynamic Proxy_). **Can I change my problem so I no longer need those things?** We can avoid the proxy entirely if we unify the interface for all of our methods (refactor parameters to objects, Commands), changing our problem to simplify the solution. Now that we have a common interface, we can do basic composition instead of proxying stuff at runtime. From ```c# public void Deactivate(Guid id, string reason) { var item = repository.GetById(id); item.Deactivate(); } public void Reactivate(Guid id, DateTime effective, string reason) { var item = repository.GetById(c.id); item.Reactivate(); } ``` To ```c# interface Handles where T:Command { void Handle(T command); } ``` ```c# public static void Handle(DeactivatedCommand c) { var item = repository.GetById(c.id); item.Deactivate(); } public static void Handle(ReactivateCommand c) { var item = repository.GetById(c.id); item.Reactivate(); } ``` Now that we have a common interface, would have Loggers, Transactions, etc. in the same way we had annotations. ```c# class LoggingHandler : Handles where T:Command { private readonly Handles next; public void LoggingHandler(Handles next) { this.next = next; } public void Handle(T command) { myLoggingFramework.Log(command); next.Handle(command); } } ``` The less magic I have, the faster I get onboard. IoC container. Before having an injection container, we used to pass dependencies as parameters to constructors. Now we have all this boilerplate for injecting and passing parameters everywhere. You can pass dependencies to handlers so we avoid having constructors. ```c# public static void Deactivate(ItemRepository repository, DeactivatedCommand c) { var item = repository.GetById(c.id); item.Deactivate(); } ``` Now the problem we have is that we lost the common interface! How can we do the equivalent of dependency injection inside a functional dependency ```c# public static int Add(int a, int b) { return a + b; } ``` We can close one of the parameters with a lambda (partial application). ```c# var add5 => c => Add(5, x); ``` Now we can recover our common interface for our commands ```c# public static void Deactivate(ItemRepository repository, DeactivatedCommand c) { var item = repository.GetById(c.id); item.Deactivate(); } ``` ```c# var nodepends => x => Deactivate(new ItemRepository(), x); ``` IoC containers solve a problem that maybe you don't have. If you had a UI program with lots of complex nested dependencies then maybe is the right tool. But for most cases IoC containers are too much. ```c# void Bootstrap() { handlers.Add(x => Deactivate(new ItemRepository(), x)); handlers.Add(x => Reactivate(new ItemRepository(), x)); handlers.Add(x => CheckIn(new ItemRepository(), new BarService(), x)); } ``` How many use cases do you have? 15 lines of code could be completely fine. A junior person would totally understand. Feel the pain of passing dependencies and setting up complex structures. A problem with IoC containers is that they make it very easy to do things that you shouldn't be doing. What about adding a fresh repository per request? (Dependencies life-cycle). ```c# void Bootstrap() { handlers.Add(x => Deactivate(() => new ItemRepository(), x)); handlers.Add(x => Reactivate(() => new ItemRepository(), x)); handlers.Add(x => CheckIn(() => new ItemRepository(), new BarService(), x)); } ``` Abstract Factory is too complex. You can write the same thing simply and without any tools. Because a common interface I can do a lot of stuff, our previous Logger handler works the same, probably we don't need the class. ```c# public static void Log(T command, Action next) where T:Command { myLoggingFramework.Log(command); next(command); } ``` We are not usign any of the tools, we can just pass this function around. **Understand the problem a tool or idea solves well.** Make your tools redundant. Can I change my problem to avoid having that? You should feel pain while passing tons of dependencies across many levels, tools hide problems. They mask problems. > If you need to add stuff to your IDE you are probably on the wrong path. A good system shouldn't need a tool. When we talk about simplicity, we are also talking about tools. > You own all code in your project. > > Your boss doesn't care if the bug happened in someone else's library! Sometimes they bring magic to your software, and at some point in time it will break and you won't know how to fix it. Simplicity is about minimising dependencies, understanding the problems we solve. Try to minimise external dependencies! ",
    "url": "/learning-notes/talks/8-lines-of-code/",
    "relUrl": "/talks/8-lines-of-code/"
  },"36": {
    "doc": "Am I senior yet?",
    "title": "Am I senior yet?",
    "content": "# [Am I senior yet?](https://www.youtube.com/watch?v=jcTmoOHhG9A) Grow your career by teaching your peers. People tend to gravitate towards technical aspects, which is important, but you'll need to learn other areas. > A senior developer understands that you cannot do everything yourself. Their primary role is to **help their team get better**. - Matt Brigs Software generally is built by teams of people, not individuals. Your primary role should be about leveling the team up. ## Teaching Teaching will help your career move forward. Teaching allows you to maximise your impact. Employers wil hire, promote and reward **high impact** engineers. How do I become a better teacher? ### 8 lessons * **A question is an opportunity for growth (for who asks it)** Situation: Someone may jump into your desk and ask you something. Standard approach: Just answer a fix. Better approach: Use the opportunity to teach someone something so they can solve their problems in the future. * Do they undertand the problem? * Do they understand the context? * Can they identify the root cause? * **Learn to tailor your response to your audience.** Take the time to consider who you are taling to. * Consider things like * Background * Experience level * Comfort level * Motivations * Goals * Skills vs will matrix. * **Junior new hire** ↑ Will ↓ Skill * Highly motivated to learn * Hasn't had time to builde their skills yet * **Guide** * Provide tools, training, and guidance * Remove obstacles * Reduce risk * **Junior 1 month in** ↓ Will ↓ Skill * Starts to realise how much they don't know * Hasn't had time to build their skills yet * **Direct** * Provide clear explanations * Identify motives * Develop a shared vision of success * **Mid level engineer** ↑ Will ↑ Skill * Loves what they are doing * Highly skilled * **Delegate** * Provide freedom * Communicate trust * Develop stretch goals * Broaden responsibilities * **Senior engineer** ↓ Will ↑ Skill * Highly skilled * A little bored * They've done all of this before * **Excite** * Point out challenges * Identify their interests * Align interest with their work * **Resist the urge to teach by doing**. If you are doing most of the typing, 9/10 times this person will not understand everything you just did. Are there exceptions to this rule? * If you are trying to replicate something * **Keep an eye out of mimicry**. Aka: Cargo cult. Mimikin code, copy and pasting code. Big red flag. Is not okay to do that in production code. If people do that it means that they are really not understanding what they are doing. * **Teaching is about communicating effectively**. Teaching is all about communication skills. Human interactions are plagued by packet loss (background, context, emotion, intention, energy...). > The single biggest problem in communication is the illusion that it has taken place - George Bernard Shaw **How effective you are at conveying information?** How much is your audience receiving? 50%? 20%? Ask people to replay what you just communicated. Good opportunity on meetings, standups... Communicate what you need to communicate in the less time as possible. * **Have realistic expectations**. Teaching is a marathon is not a sprint. People need time, be realistic. * You cannot expect them to absorb everything at once. * Maybe they are progressing in ways that you haven't noticed. If it something you really need to come across, repeat it. * **Teaching also benefits the teacher**. Everyone has something to teach. Teaching helps with: * Building confidence * Solidifying your own knowledge * Practicing your ability to communicate effectively * Creating opportunities for feedback * **Talk to your manager about how can you learn more effectively**. Managers are just trying to help you become a better engineer. Talk about ways that your manager can help you * Providing more opportunities to teach * Providing challenges * Adopting teaching focused practices * Paired programming * Show and tell * Helping you with conference submissions --- > True learning involves a permanent change int he way you see and act in the world. > The accumulation of information isn't learning. > - Benjamin Hardy References * [The TAO of Coaching](https://www.goodreads.com/book/show/265454.The_Tao_of_Coaching) by Max Landsberg ",
    "url": "/learning-notes/talks/am-i-senior-yet/",
    "relUrl": "/talks/am-i-senior-yet/"
  },"37": {
    "doc": "Clean Architecture and Design",
    "title": "Clean Architecture and Design",
    "content": "# [Clean Architecture and Design](https://www.youtube.com/watch?v=Nsjsiz2A9mg) A typical Ruby applications structure * Controllers * Logs * Views * ... Why is not telling me what it does? How it behave? **Frameworks have a tendency of structuring your code around web details.** The web is a delivery mechanism. Why the application is formed up around on IO channel? If we look at a picture of a library, we can understand that is a library. We don't see hammers and saws. **Architecture is about intent!** **Use-cases should drive the architecture.** Example of a use case: ``` Create Order Data: Customer ID, Shipment destination, Payment information, Customer contact info, Shipment mechanism Primary course: 1. Order clerk issues \"Create Order\" command with above data. 2. System validates all data. 3. System creates order and determines order-id. 4. System delivers order-id to clerk. Exception course: Validation error 1. System deliver error message to clerk. ``` We don't care about details at use case level. It doesn't say _how_. Nothing is not about the IO channel, nothing about the use case to be web. ``` Request/Response that don't know anything about web ───────────────────┐ ├───► Boundary ◄──┐ Delivery mechanism │ ├─ Interactor ──► Entity ├──|► Boundary ◄|─┘ ───────────────────┘ ▲ Request/response translation for the delivery mechanism │ │ User (request/response for specific delivery mechanism ─────────────────── REQUEST ───────────────────► ◄────────────────── RESPONSE ─────────────────── ``` ## Interactors These are the use cases. Objects that encodes the processing rules, and application specific business rules. Probably exposing an `execute` method to the outside world (Command Pattern). The interactor tells the _Entities_ what to do. ## Entity This are the objects that have the business logic. ## Boundary These are interfaces. The interactor implements one of the interfaces, interactors implemented outputs and inputs. --- Then the Web happened. After the 2.0 bubble crash, people were looking for anything that would help. Rails was invented, you can build a website iterarively really fast. The Rails ways of thinking carries a lot of baggage with it ## MVC is not an architecture ``` View ═══► Model ◄──── Controller ``` * **Model**: small objects with mutable data * **View**: has an observable behaviuour over the model, that changes the view * **Controller**: would observe clicks and user input and modify the model * **MVC goes wrong as a web architecture**. Business objects becomes this thing, half controller half view, etc. Boundaries are not very well defined to do it properly ## Model-view presenter ``` ║ ┌───► Entity ║ │ Presenter ─────║─────► Boundary ◄────── Interactor ────┼───► Entity │ │ ║ | │ │ │ │ ║ └──────────┐ ▼ └───► Entity │ │ ║ ├ Response Model │ └───────║─────────────────────┘ View Model ║ ▲ ║ │ ║ View ║ ``` Look about the direction of dependencies. Your delivery mechanism depends on your application. The web (the delivery mechanism) is a plugin. Your application should not know about it. ``` ┌──────────║───► Request model ◄───────────┐ │ ║ ▲ │ │ ║ │ │ Controller ────║───► Boundary ◄|───┐ │ ║ ├─── Interactor Presenter ─────║──|► Boundary ◄────┘ │ │ ║ │ │ │ ║ ▼ │ └──────────║───► Response model ◄──────────┘ ``` All this is already testable, and decoupled. Plug-in architecture. ## Databases They defined our industry for the last 30 years. The database have become the center of our applications. We have given too much importance to databases. The reason we have databases is a hardware issue, if we could store data in memory, we wouldn't use databases. If you don't mutate state, we can increasingly store events, there is no need for transactions. **The database is a detail!** Is just the thing that holds the data, we should treat it like a plugin. ``` ┌───► Entity ◄───┐ │ │ Interactor ────┼───► Entity ◄───┤ │ │ │ │ └───► Entity ◄───┤ ▼ │ Entity Gateway │ ▲ ┌─────────┘ │ │ ══════════════════════════════════════ │ │ Entity Gateway ├──────┘ Implementation ├───────► Database API ``` An object is not a bunch of data, an object is a bunch of methods. Data should be encapsulated. Your entities shouldn't expose their data for the database. Below the line the main program, is a plugin. Where you have your factories, dependency injection container, etc. **A good architecture allows major decisions to be deferred!** Decisions about the UI, the database, the framework, etc. **A good architecture maximises the number of decisions not made.** References: * [Object-Oriented Software Engineering](https://www.goodreads.com/book/show/296981.Object_Oriented_Software_Engineering) by Ivar Jacobson. ",
    "url": "/learning-notes/talks/clean-architecture-and-design/",
    "relUrl": "/talks/clean-architecture-and-design/"
  },"38": {
    "doc": "Everything You Wanted to Know About Distributed Tracing",
    "title": "Everything You Wanted to Know About Distributed Tracing",
    "content": "# [Everything You Wanted to Know About Distributed Tracing](https://www.youtube.com/watch?v=HSgb7gOO1Ig) ## Software evolution From * Monolith * On premise * Single language * Single stack * Virtual machines To * Microservices * Containers * Multi cloud/hybrid * Polyglot * Serverless/Cloud functions Better scalability or team performance and autonomy. ## New microservice architecture, new challenges * **Observability**. A single instance does not give enough context to dedbug or understand stuff. * Deployment / packaging * Configuration management * Debugging * Secrets management ## Distributed tracing Method used to profile and monitor applications, especially those built using microservices. It helps pinpoint where failures occur and what causes poor performance. **Trace**: A trace is a tree of spans that follows the course of a request or system from it source to its ultimate destination. Each trace is a narrative that tells the request story as it travels throught he system. **Span**: Are logical units of work in a distributed system. They have a name, a start time, and a duration. Each span captures important data points specific to the current process handling request. **Context propagation**: Metadata information passed form one service to another so it can be collected later. Usually passed by headers Incoming request ┌───────────┐ Outbound request ┌───────────┐ ─────────────────────>│ Service 1 │─────────────────────>│ Service 2 │ trace-id = 123 └───────────┘ trace-id = 123 └───────────┘ parent-id = nil parent-id = 1 span-id = 1 spain-id = 2 **Tags & Logs**: Both annotate the span with some contextual information. * Tags typically apply to the whole span, while logs represent some events that happened during the span execution. * A log always has a timestamp that falls within the span's start-end time interval. * The tracing system does not explicitly track causality between logged events the way it keeps track of causality relationship between spans, because it can be inferred from the timestamps. ### What questions help distributed tracing answer? * What services did a **request pass through**? * What ocurred in **each service** for a given request? Root-cause analysis * Where did the **error** happen? We are able to pinpoint where the error originated * Where are the **bottlenecks**? * What is the **critical path** for a request? * Who should I **page**? ### Why isn't everybody using it? * Not much education or not many publicised case studies on the benefits. * Vendor Lock in is unacceptable: Instrumentation must be decoupled from vendors. * Inconsistent APIs: Tracing semantics must not be language dependent. * Handoff woes: Tracing libs in Project X do not handoff to tracing libs in Project Y. ### Meet OpenTelemetry (opentelemetry.io) [OpenTelemetry](https://opentelemetry.io/) is a match between [OpenTracing](https://opentracing.io/) and [OpenCensus](https://opencensus.io/) projects * **Single set of APIs** for tracing and metrics collection. * **Standarised Context Propagation**. * **Exporters** for sending data to backend of choice. * **Collector** for smart traces & metrics aggregation. * **Integrations** with popular web, RPC and storage frameworks. #### Options With agent * It does bytecode instrumentation and sends the data automatically Manually instrumenting your application * You have to manually instrument to collect information ### Tracers Open source * [Jaeger](https://www.jaegertracing.io/) * [Zipkin](https://zipkin.io/) * [Lightstep](https://lightstep.com/products/tracing/) * [Apache Skywalking](https://skywalking.apache.org/) Hosted * Honeycomb * New Relic * Data Dog ",
    "url": "/learning-notes/talks/everything-you-wanted-to-know-about-distributed-tracing/",
    "relUrl": "/talks/everything-you-wanted-to-know-about-distributed-tracing/"
  },"39": {
    "doc": "Grinding the monolith",
    "title": "Grinding the monolith",
    "content": "# [Grinding the monolith](https://www.youtube.com/watch?v=xxW_c_8AHiE) ## Monolith Pros: * Easier to deploy * Easier to place and find stuff in ## Microservices Cons: * Operational complexity * Partial failure Pros: * Independence of development * Independence of scalability * Building in smaller and simpler pieces ### We have heard the story before Java Beans, COMM, COBRA * Independent components, you can just buy and connectcomponents * Decoupled and simple pieces We can have software that works like Lego. But this hypothesis does not work in reality as there are many more dimensions (connections between components). Components may have hundreds or thousands of collaborators, we would need to have to have nth dimensional Lego bricks! Maybe a better model would be Mycelium, a fungus where fractal roots communicate to each other in a way more complex way. We can try to make a clear horizontal layered separation: UI, Application, Domain, Persistence. Try to repeat the pattern in vertical separations, but soon enough they will start coupling each other. We can make the thing even worse with operational monitoring, logging, etc. Maybe the Layers architecture referred more to protocol stacks than to web applications, precisely because of the standarisation of interfaces between layers does not generally apply. ## Microservices The key issue is around **safety** for different teams to operate independently. Issues from other teams should affect yours. Without this, you'll start adding checks, reviews, etc. \"The Fear Cycle\". Safety failure in Monoliths * Bad code has an unlimited span of effect (SQL leak will affect all code) * Modifying shared code will affect other teams * Semantic coupling, different concepts mixed under the same name. What works for one context, may not work for the other All those make code changes slow. Many teams tried transformations towards microservices, 2/3s failed because impatience, unrealistic expectations, or hubris. ## Strategies to move towards microservices ### Clean then separate Refactor to create interfaces between subdomains, then turn those into HTTP interfaces. **Failed** * You have to stop feature development * You have the clean team is going slower than the feature teams that will make compromises to go faster (their work is throwaway) ### Entity services You turn your key domain objects into services **Failed** * Transactionality problems. Microservices need to have behaviour * Deployments are painful as redeploying a core entity service will affect a lot of clients * Distributed monolith ### Project teams Have a pool of people (contractors) do create services and throw them back to the pool once they finish. **Failed** * Favour deadlines (short term) over high quality code and architecture initiatives (long term) * If you done your hard work, you should enjoy the fruit of the result. If not, you should leave with it and learn from it * Better to have product teams (which can have projects) ### Clean-sheet design of a new services Create new services from scratch **Failed** * We make the same mistakes over and over * This system will always lag behind * Optimism not supported by past evidence. What happened last time (unexpected issues) is most likely to happen this time ### The strangler Intercept traffic to the existing system and replace pieces with new services. It is a transitional pattern. **Partial success** ### Clone and slash Clone the monolith for each team. Remove the code the team does not need. Transitional state. You will be sharing the DB for a long time. **Success** ### Continual redesign/rebuild Keep the presure on to keep redesigning, microservices are never done. Remove code and microservices, which is one of the main benefits of removing microservices. **Success** ### Microservices by lifecycle Each stage of the process has its own microservice, and each process/microservice passes the state to the next. **Success** ### Transmit domain objects on the wire Domain objects as the representation to transfer in APIs. **Failure** * Domain object coupled with the API, will be very difficult to evolve and change * Makes it very difficult to handle multiple versions of APIs * Leaks implementation details and internals of the service Turn domain objects into hashmaps, lists, etc. ### Refactor to hexagons Express the domain independently from integration and persistence implementation details. **Success** --- Think long-term, act short-term. Solve the problem in front of you. --- References - \"Layers architecture\": [Pattern oriented software architecture, vol 1](https://www.goodreads.com/book/show/85039.Pattern_Oriented_Software_Architecture_Volume_1) ",
    "url": "/learning-notes/talks/grinding-the-monolith/",
    "relUrl": "/talks/grinding-the-monolith/"
  },"40": {
    "doc": "How to make a sandwitch by Dan North",
    "title": "How to make a sandwitch by Dan North",
    "content": "# [How to make a sandwitch by Dan North](https://www.youtube.com/watch?v=P8sNSNkWFpc) ## What is feedback * Tech lead role * Conways law ### Systems theory `input -> simple design -> output` Feedback new inputs into the system with a delay Adaptive system changes its behaviour when feeded with feedback * **Accelerating loop**: amplifies behaviour [positive] amplifier for a guitar [negative] financial aid, you make adiction [negative] meds * **Diminishing loop**: suppreses behaviour * **Balancing loop**: fedback tends to towards table goal - You help me I help you - The outlet of the factory has to be upstream (so factories won't contaminate their own water) * **Oscilating/trashing loop**: feedbackflips between states - Like a shower, too hot, too cold, bad experience. If you introduce a delay you can control it a bit. Timing in feedback is everything. **Small and frequent** is better than **large and infrequent** to people. Anual reviews, I'm gonna tell you something bad that you did 11 months ago any delay on receiving feedback means that you are basically going blind - Reduces responsiveness - Limits the options (to react to that) - Increases processing effort for the feedback (compared to TDD, continuous integrations, small frequent batches, much more controls) ### Why do we ask for feedback? * To **improve** or modify our behaviour * For **help**, validation when we are stuck * _For **recognition**, when we think we are doing okay_ ### Why do we offer feedback? * To **improve** the system of work (improving the team) * To **model a culture** of encouring feedback * _To **control** others_ * _To demonstrate our **superior knowledge**_ You have to question your motives, on why are you really offering feedback. ### Feedback as a system 1. You offer feedback or you get some feedback 2. Feedback has to be heard 3. Needs to be actioned. Go to 1. All those 3 things need to happen, if not feedback is broken. ## Delivering feedback Feedback is about behaviour and is usually understood as a personal level. \"Your work is sloppy\" heard as \"you are a sloppy worker\" Better: Your work has been sloppy recently Much better: **On this specific occasion** your work was substandard Great, actionable: If you did these things your work quality **would improve** Awesome, give ownership on that feedback relationship: If you did these things it would **make me happier** ### SBI model * **Situation**, specific: an example of **when** * **Behaviour**, what you did, observable, concrete. Avoid judging * **Impact**, is me, how I felt #### Example: * Situation: In the team meeting on Friday * Behaviour: you spoke across me several times (other people also observed, no judging) * Impact: so I felt like I wasn't being allowed to share my opinion with the team ``` ↑ I take **actions** based on my beliefs ┌─↑ I adopt **beliefs** about the world REFLEXIVE LOOP │ ↑ I draw **conclusions** our beliefs │ ↑ I make **assumptions** based on the meanings I added affect what data │ ↑ I add **meanings** (cultural and personal) we select next └→↑ I select **\"data\"** from what I observe time ↑ Observable **\"data\" and experiences** (as a videotype recorder might capture it) ``` _Author Chris Argyris \"Ladder of Inference\", extracted from the book [The Fifth Discipline](https://www.goodreads.com/book/show/255127.The_Fifth_Discipline) We always we have different layers of filters to talk to each other subconciously. ## Structuring feedback It depends on the level of trust, or level of safety, or level of vulnerability. ### Porpoise feedback Trained based on rewards. When you give feedback with just goodness. People are really kind to please you beyond what you asked for. This is really useful when you are starting out and probably everything else is being done wrong, is important to start with something. - Offer specific positive regard - Assume everything else will self-correct - Everything else will self-correct **This is the best way you can interact with a new group, new team, new work mates. Or someone that just changed role. Just possive regard.** ### Sandwich feedback - Offer **specific positive regard** - Offer **a growing edge** (corrective) - End with **general positive regard** (everything was fine and continuous to be fine) You need to genuily care. ### \"Atkings\" feedback When there is high trust (all safety) - Offer **a growing edge** ## Receiving feedback You just say \"thank you\". No step 2. All feedback has good intentions. ",
    "url": "/learning-notes/talks/how-to-make-a-sandwich/",
    "relUrl": "/talks/how-to-make-a-sandwich/"
  },"41": {
    "doc": "Mastering Chaos: A Netflix Guide to Microservices",
    "title": "Mastering Chaos: A Netflix Guide to Microservices",
    "content": "# [Mastering Chaos: A Netflix Guide to Microservices](https://www.youtube.com/watch?v=CZ3wIuvmHeM) ## Microservices basics What microservices are not: * **Monolithic code base.** Everyone was contributing to a single code base that was released once a week, when change introduced a problem it was difficult and slow to debug. Because so many changes were released on a single deploy. * **Monolithic database.** When this went down, everything went down. Scaling it vertically was very expensive. * **Tightly coupled architecture.** One of the most painful points was the lack of agility, everything was deeply interconnected. What is a microservice: An evolutionary response * Separation of concerns. Modularity, encapsulation * Scalability. Horizontally scaling, workload partitioning * Virtualisation and elasticity. Automated operations, on demand provisioning Microservices as organs: * Each organ has a purpose * Organs form systems * Systems form an organism Microservices are an abstraction: * You have a **service** that provides some functionality * The service may need to access some persistence mechanism like a **database** * You may provide **service client** for accessing data operations * You may provide an **cache for your client** (peg: [EVCache](https://github.com/Netflix/EVCache)) * You may need to provide some orchestration between the client and the cached client, so you may need to really provide a **client library** that will go to the cache first, and if it fails will go for the client that will hit the microservice and persistence layer and then backfill the cache for the next call * All this will have to be embedded within the **client application** that wants to use the From the consuming application the client library, that includes the service client cache, the service client, the service and the database is the microservice. Is not a simple state thing. ## Challenges and solutions ### Dependency #### Infra-service request Everything is great until something breaks. * Network latency, congestion, failure. * Logical or scaling failure. Cascading failure, one service fails without defences it can cascade and take down your entire network. Solution: [Hystrix](https://github.com/Netflix/Hystrix) * Structured way of handling timeouts and retries * Fallbacks, if I cannot call a service, can I return some static response instead (degraded service) to allow the customer to continue using the product * Isolated threadpools and the concept of circuits. If you keep hammering the service and it keeps failing, maybe you should stop calling it and fail fast returning the fallback How do you know if it works at scale? Netflix created [FIT](https://medium.com/netflix-techblog/fit-failure-injection-testing-35d8e2a9bb2) (Fault Injection Testing) to test this. It inject failure information metadata to [Zuul](https://github.com/Netflix/zuul), and this is carried through the network. * Synthetic transactions * Override by device or account * % of live traffic up to 100% * Enforced throughout the call path How do we constraint testing scope? (So you are not testing millions of permutations, or downstream dependencies of the services you test). To address this Netflix defined the critical microservices to have basic functionality to work, which is not all of them, and test only those (by blacklisting all of the other services that are not critical). This has worked great to make sure that the service actually functions when all those dependencies go away. This is a much simpler approach than doing point-to-point interactions. Client libraries * Many clients * Common business logic * Common access patterns Trade-offs for client libraries * Heap consumption * Logical defects * Transitive dependencies We can limit the client libraries, try to simplify them as much as possible. Persistence * CAP theorem In the presence of a network partition, you must choose between between consistency and availability. Netflix chose availability via Cassandra, systems are eventually consistent. Infrastructure Do not put all your eggs into one basket. [You can go multi-region](https://www.infoq.com/presentations/netflix-failure-multiple-regions/). ### Scale #### Stateless services * Not a cache or a database * Frequently accessed metadata * No instance affinity * Loss a node is non-event, you can replace a node at no cost Auto scaling groups is fundamental for microservices, advantages * Compute efficiency, on-demand capacity * Node failure, nodes gets replaced easily * Traffic spikes, DDoS attack, etc. * Performance bugs, auto-scaling allows you to absorb damage while figuring out what happened Surviving instance failure, thanks to Chaos Monkey (losing individual nodes). #### Stateful services * Databases and caches * Custom apps which hold large amounts of data * Loss of a node is a notable event, it could take hours to recover Redundancy is fundamental, EVCache similar to memcache but it writes to several zones for redundancy #### Hybrid services It's easy to take EVCache for granted * 30 million requests/sec * 2 trillion requests per day globally * Hundreds of billions of objects * Tens of thousands of memcached instances * It consistency scales in a linear way, no matter the load. Milliseconds of latency per requests Problem is you may rely too much on EVCache. Solutions * Workload partitioning, split cache by workload (real-time vs batch processes) * Request-level caching, so you are not repeat hitting the service over and over. Make the first hit expensive and the rest of them free through the lifecycle of the application * Secure token fallback, embed a secure token through the requests. If the subscriber service is unavailable, fallback to a datastore with that encrypted token so you have enough information to identify the customer and provide basic operation * Chaos under load, use tools to test your architecture ### Variance within your architecture The more variety you have in your system, the more complex and difficult to manage it becomes. #### Operational drift, which happens over time Unintentional, it happens eventually. Over time * Alert thresholds * Timeouts, retries, fallbacks * Throughput (RPS) Across microservices * Reliability best practices The first time you talk with teams about this, they will be very enthusiastic; however, as this is tedious and repetitive, and is not related to product work, people will tend to avoid it. They solved the cycle with continuous learning and automation. An incident gets a resolution, then a review happens, a remediation plan, some analysis, then extract some learning into best practice, then you automate wherever possible, then you drive adoption. ##### Production ready checklist * Alerts * Apache and Tomcat * Automated canary analysis * Autoscaling * Chaos * Consistent naming * ELB config * Healthcheck * Immutable machine images * Squeeze testing * Staged, red/black deployments * Timeouts, retries, fallbacks #### Polyglot (new languages) and containers Intentional variance. The paved road, focused on Java and EC2 * Stash * Nebula/Gradle * BaseAMI/Ubuntu * Jenkins * Spinnaker * Runtime platform Some engineers went off road, building their own roads, doing stuff in Python, Ruby, NodeJS, each one of them provided value in some sense. When Docker was introduced, things went a bit wild. #### Cost of variance * Productivity tooling * Insight and triage capabilities * Base image fragmentation * Node management * Library/platform duplication * Learning curve, production expertise Instead of a single paved road, now we have multiple paved roads that makes life difficult for the teams that support engineering infrastructure. The strategic stance to cost was to * Raise awareness of costs * Constraint centralised support, focus specially into JVM, and of course for Docker * Prioritise by impact * Seek reusable solutions How do we achieve velocity with worrying about breaking things all the time? Global cloud management and delivery, Spinnaker an automated delivery system. * Conformity checks * Red/black pipelines * Automated canaries * Staged deployments (one region at a time) * Squeeze tests ## Organisation and architecture Electronic Delivery, NRDP 1.x. Wasn't called Streaming yet. * Simple UI, \"Queue Reader\" * Collaborative design * XML payloads * Custom responses * Versioned firmware releases * Long cycles In parallel the Netflix API, let a 1000 flowers bloom. It wasn't very successful but after that, it started to be used privately * Content Metadata * General REST API * JSON schema * HTTP response codes * OAuth security model (it was important for 3rd party apps) Hybrid architecture, now we have these two edge services functioning in very different ways. Distinct in * Services * Protocols * Schemas * Security There was a lot of friction between teams, as client developer you would have to change context all the time. Josh: What is the right long term architecture? Peter: Do you care about the organisational implications? ### Conway's law Any piece of software reflects the organisational structure that produce it. If you have four teams working on a compiler you will end up with a four pass compiler. **This is not solutions first, this is organisation first.** ### Outcomes and lessons By unifying these things around the client. Outcomes * Productivity and new capabilities * Refactored organisation Lessons * Solutions first, team second * Reconfigure teams to best support your architecture --- Microservice architecture as complex and organic. Health depends on discipline and injecting chaos. Dependency * Circuit breakers, fallbacks, chaos * Simple clients * Eventual consistency Scale * Auto-scaling * Redundancy, avoid SPoF * Partioned workloads * Failure-driven design * Chaos under load Variance * Engineered operations * Understood cost of variance * Prioritised support by impact Change * Automated delivery * Integrated practices Organisation and architecture * Solutions first, team second ",
    "url": "/learning-notes/talks/mastering-chaos-a-netflix-guide-to-microservices/",
    "relUrl": "/talks/mastering-chaos-a-netflix-guide-to-microservices/"
  },"42": {
    "doc": "Probabilistic Data Structures",
    "title": "Probabilistic Data Structures",
    "content": "# [Probabilistic Data Structures](https://www.youtube.com/watch?v=F7EhDBfsTA8) They allow you to do things in memory at scale for things you couldn't do before with a very tiny memory footprint. Something you have to know is that you as a developer is that you have to accept a predictable level of inaccuracy. ## Bloom filters For working out if something is in a set. With Java, a naive implementation could be using a `Set`. The problem is if we start adding loads of stuff into a `Set`. Insertion time gets slower, heap space in JVM get massive. [Space/Time Trade-offs in Hash Coding with Allowable Errors paper](http://crystal.uta.edu/~mcguigan/cse6350/papers/Bloom.pdf) Parts of a Bloom filter: * A bit array of size _n_ * _k_ hash functions A Bloom filter can tell you if something is not there, and _maybe_ if is there. Available in Google Guava library `com.google.guava:guava:19.0` as `BloomFilter`. There is an error rate of checking existence in the set. Adjusting this error size, affects insertion performance. The more accurate you want to be, the more hash functions you have. Use cases: * \"One hit wonders\" If you visit a page once, content for you will be served from a Bloom filter. For any other case, we can serve you something more computationally expensive. * Avoiding lookups (HBase, Cassandra) * Real-time matching, real-time searches. ## Count-min sketch Slightly newer than Bloom filters. Similar use case, for checking membership. More used for count tracking (how many times something exists). If you were using a naive Java implementation for the problem, you can use a `HashMap`, or a `HashMultiset` (a `HashMap` with a key for the element, and an integer as a value counter). Same issues as before, insertion time and heap size gets bigger. [Approximating Data with the Count-Min Data Structure paper](http://dimacs.rutgers.edu/~graham/pubs/papers/cmsoft.pdf). Initialising a sketch: * Epsilon: accepted error added to counts with each item. * Delta: Probability that estimate is outside accepted error. You can use `com.clearspring.analytics:stream:2.9.2` and `CountMinSketch`. Use cases: * Any kind of frequency tracking! * Natural Language Processing * Extension: Heavy-hitters * Extension: Range-query ## HyperLogLog Is used for cardinality, \"in my list of things, how many unique items are there\". \"What are my unique visitors to my site today\". Naively in Java we can do it with a Set again. Just retrieving back the unique elements inserted (not-repeated). `mySet.size()`. This is an implementation of a compound of papers: * Linear counting, similar to Bloom filter: [A Linear-Time Probabilistic Counting Algorithm for Database Applications paper](http://dblab.kaist.ac.kr/Publication/pdf/ACM90_TODS_v15n2.pdf) * LogLog counting: [Loglog Counting of Large Cardinalities paper](http://algo.inria.fr/flajolet/Publications/DuFl03-LNCS.pdf) * HyperLogLog: [HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm paper](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf) You can use `com.clearspring.analytics:stream:2.9.2` and `HyperLogLog`. Use cases: * Anywhere you need cardinality in O(n)! * Unique site visitors * Estimates of massive tables * Streams of data ",
    "url": "/learning-notes/talks/probabilistic-data-structures/",
    "relUrl": "/talks/probabilistic-data-structures/"
  },"43": {
    "doc": "Programming Across Paradigms",
    "title": "Programming Across Paradigms",
    "content": "# [Programming Across Paradigms](https://www.youtube.com/watch?v=Pg3UeB-5FdA) The paradigms of Programming, Robert W. Floyd Stanford University Turing Award Lecture > I believe the chance we have to improve the general practice programming is to attend our paradigms – Robert W. Floyd ## What is a paradigm? Philosophy, \"The Structure of Scientific Revolutions\". A paradigm is a worldview, how do we observe the universe, a model. A paradigm enables progress. **Unless we agree onto something is very difficult to progress as a community.** > In learning a paradigm the scientists acquires theory, methods, and standards together, usually in an inextricable mixture – Tomas S. Kuhn What entities make up the universe how they behave and interact. What makes up a program? Which problems are worth solving which solutions are legitimate. Which problems we need to define as programmers? Which programs are the good ones? > All models are wrong – George E. P. Box `paradigm -> anomaly -> crisis -> shift -> paradigm...` Ptolomeo model on the universe (earth at the center) was superseeded with Copernicus model (sun at the center) which was superseeded by the Newton model, and then Albert Einstein, etc. ## Major paradigms? ### Imperative programming **Follow my commands, in the order I give them. Remember state.** _It's like a complex clock, you need lots of precision._ This paradigm requires so much precision that a single issue can bring the entire thing down. ### Object-Oriented programming **Keep your state to yourself. Receive messages. Respond as you see fit.** It is still imperative, but at least these are handled in smaller chunks. Objects have little pieces of state. Everything is about the messages and relationships between them. _It's like biological stem cells in a body. A cell in a larger tissue. Every stell have their own organs and pieces. They have receptors in the membrane, receiving messages from the outside_ ### Functional programing The solution to the problem with rigidity of imperative programming. **Mutable state is dangerous. Pure functions are safe. Data goes in, data comes out.** _This is like a factory. Where we have like an assembly line were we transform materials into different stages until we get a car._ ### Declarative programming Functional programming falls under a broader umbrella called Declarative Programming. **These are the facts. This is what I want. I don't care how you do it.** _This is like a puzzle, a sudoku, where you have these rules about the game. It doesn't matter how you place numbers in it._ SQL is a declarative language. A subparadigm of declarative programing is Logic Programming and Prolog for example. ## What do they have in common? It looks like there is massive rivalry between Object-Oriented Programming and Fucntional programming? Is there anything in common? They are both trying to solve the rigidity and the complexity of imperative style programming. They try to tackle shared mutable state. Functional programming just removes the problem by making data immutable, OOP solved it creating small chunks of data. > I'm sorry that I long ago coined the term \"objects\" for this topic because it gets many people to focus on the lesser idea. The big idea is \"messaging\" – Alan Kay `thing.do(some, stuff)` `thing` is the recipient `do` is the message with `some` and `stuff` as arguments. ```python buddy.is_friend_of('guy') buddy.send('is_friend_of', 'guy') buddy('is_friend_of', 'guy') ``` We can do this with closures instead of Classes so it becomes functional. So we will have the same behaviour than with OOP. Is it fuctional or object-oriented? Depends on your worldview or mindset! ## Which paradigm is the best? > All models are wrong **but some are useful** – George E. P. Box Paradigms are useful in different ways. Newton model still works and Albert Einstein also works for other things. > Each paradigm supports a set of concepts that makes it the best for a certain kind of problem – Peter Van Roy Maybe we can match our problem with the best paradigm to solve it. > We shouldn't ask if the model is **true**, but more on if the model is **illuminating and useful** – George E. P. Box Maybe changing a paradigm will make our problem easier to solve. ## What can a paradigm teach me? **Imperative programing**: Be explicit, understand the implementation. Forces you to think how to optimise these little details. **Declarative programming**: Be abstract, understand the domain. To think in the big picture ideas, and not how to implement them. We can change the implementation but the big picture remains the same. **Object Oriented Programming and Functional Programming**: Encapsulate and communicate. Sometimes we can take advantage of Object Oriented Programming to encapsualte behaviour. **Functional Programming**: Specialise, transform data. Functional programming can make a convoluted situation an easy task to solve. --- **No paradigm is best absolutely, each is best for a certain case.** A set concepts that match very well with a set of problems. > If the advancement of the general art of programming requires the continuing invention and elaboration of paradigms, advancement of the art of the individual programmer requires that they expand their repertory of paradigms – Robert W. Floyd As individuals we need to get comfortable with as many paradigms as possible, to have more tools in our toolbox. **Learn new paradadigms, try multi-paradigm languages.** ## What's the point? Paradigms enable programming, as they paradigms enable scientific progress. Not only about the entities that defines the problems, but also the problems we can solve. Don't fight your paradigm, embrace it. Be often to shift when anomalies arise. Attend to your paradigms. ",
    "url": "/learning-notes/talks/programming-accross-paradigms/",
    "relUrl": "/talks/programming-accross-paradigms/"
  },"44": {
    "doc": "Refactoring from good to great",
    "title": "Refactoring from good to great",
    "content": "# [Refactoring from good to great](https://www.youtube.com/watch?v=L1G--mPscQM) 1) Temporal variables to private methods * Reduces the number of methods * We can reuse them * Easier to read * Hides implementation details 2) Feature envy (a class inspects other classes). Tell don't ask * Move logic to the object * Behaviour closer to the data 3) Data clump (passing a lot of parameters) * Better to pass a minimum number parameters as possible 4) Invert control. Instead of passing params we pass the data clump * Reduces coupling * Easiert to change 5) Parameter coupling. Passing and inspecting objects in methods increases coupling, less parameters is better. 6) Behaviour magnet. Now that we have the appropriate classes, we have a place to put behaviour. 7) Refactor and improve performance of the classes we have probably with native behaviour 8) Extract behavior into private methods 9) Collection as first class citisens so we have a place to put collection behaviour 10) We do not have all data in objects (null data). Pej: a Job may not have a contact name. We have to check it all over the place. NullContact (Null-Object pattern). NullContact (name and phone). No need for conditions or obfuscation. 11) Adapter pattern. A user managing payments (Depending on Braintree). Depend upon abstractions. If I want to change the payment provider I have to change a lot of places. Extract it to a PaymentGateway and make the user depend on that abstraction. Easy to mock and test. When to refactor: - All the time. Refactor on green. On code review - God objects (like user, the main class of the business journey) - High churn files, a file that changes all the time. Do not waste a lot of effort on files that don't change often - Bug Check out for number of lines, there are better ways of measuring complexity References: - [Clean Code](https://www.goodreads.com/book/show/3735293-clean-code) - [GOOS](https://www.goodreads.com/book/show/4268826-growing-object-oriented-software-guided-by-tests) ",
    "url": "/learning-notes/talks/refactoring-from-good-to-great/",
    "relUrl": "/talks/refactoring-from-good-to-great/"
  },"45": {
    "doc": "Rethinking the Developer Career Path",
    "title": "Rethinking the Developer Career Path",
    "content": "# [Rethinking the Developer Career Path](https://www.youtube.com/watch?v=yIPbE7BssOs) > I picked the wrong career. We had a team lunch, none of them planned to stay in the Software Engineering path. **The only way to progress in your career is to step into management** Getting into management should be mandatory. Implementers, Solvers and Finders. Rethinking the developer career path. ## What does _senior_ mean? What people understand * Expensive. Consultancy companies offer \"senior\" positions to charge more to their clients. * Bored. * Opinionated. * From Minnesota. Entry level for people from Minnesota, because they have such a convoluted system. **We don't really know**. What people really convey what a _senior_ is it depends on a single thing * Years of experience. ## What *should* _senior_ mean? Metrics * Quantifiable. Where we are? * Give engineers a way forward. How do we continue? * Describe the work we do. ### The process We start with a really big problem. Pej: \"When people get back to their homes, how do we make them choose Netflix instead of Amazon Premium, HBO or broadcast TV?\" Then we split that big problem into smaller chunks. Like \"How do we make our systems reliable. Improving monitoring, alerting, etc.\" We can break the system even further. Like \"Building a UI for make people at netflix aware about the alerting, monitoring, etc.\" We can still break things down, 'til we get to the code. The problems are always different. Happy engineers are the ones that get different problems all the time. Now we can understand the \"X years of experience, or the same year of experience repeated X times\". Maybe that's the right career path for engineers: 1. Solution implementer (insetad of _junior_) * Given solutions. Leads will help them break things and build solutions someone else though. They have to be exposed to many kind of problems from your company. * Produces code. Leads will review the code 2. Problem solver (instead of _mid level_) * Given problems. At the beginning you'll need to provide small problems, increasing complexity. You as a lead will need to start backing off, you need to trust your peers. They are given autonomy incrementally * Produces solutions 3. Problem finder (instead of _senior_) * Given context. Full autonomy. Instead of give them tasks, you provide them context. They will break down problems, and they will participate implementing them. * Discovers and prioritises new problems. You can see these path as a series of steps. Each step the base for the next one. ## Where we go wrong? Anti-patterns * Inverted piramid. Like finding problems that you never get to solve, break or implement (Software Architect). * Not mentoring implementers. Implementers need help to become problem solvers. You need to mentor juniors, it is your fault they are failing. * UI squeeze. Product managers deciding what to do, introducing lots of tickets, handing them all to the designers, then handling them off to UI engineers. Having _just_ implementers, there is no growth for these people. Bring the engineers back to the front. Engineers should have autonomy, Product Managers should provide problems, not solutions. A lot of tech debt will accumulate becuase nobody listened to engineers. * Impostor syndrome. Someone is put into a position of seniority or leadership. People start wondering what a senior is. We can solve this by talking about the previous career path. --- Problem when companies have silos. Departments not talking to each other. When you break things into problems to be solved, barriers can be broken down. Break down to each level. ",
    "url": "/learning-notes/talks/rethinking-the-developer-career-path/",
    "relUrl": "/talks/rethinking-the-developer-career-path/"
  },"46": {
    "doc": "Simplicity is Complicated",
    "title": "Simplicity is Complicated",
    "content": "# [Simplicity is Complicated](https://www.youtube.com/watch?v=rFejpH_tAHM) ## Success What makes Go successful? * Speed of compilation * Speed of execution * Deployment * Tools * Libraries Which are not language features, these are superficial reasons. All are important but not really the answer. ## Simplicity Go is simple, at least compared to established languages. Simplicity has many facets. Simplicity is complicated. ## Convergence Java, JavaScript, TypeScript, C#, C++, PHP actively borrow features from one another, they are converging into a single huge language. ## Language relativity _Sapir-Whorf hypothesis_: Language influences thought. Controversial with regard to human languages. Close to a fact for computer languages. Consider: * Logic programming * Procedural programming * Functional programming * Object-oriented programming * Concurrent programming Like disciplines in mathematics. Like, you don't solve Calculus with Algebra even though they share semantics. ## Convergence and relativity If the languages all converge, we will all think the same. But different ways of thinking are good. Need different languages for different problems. We don't want just one tool, we want a set of tools, each best at one task. ## Convergence and features Language evolve and compete by adding features. The languages grow in complexity while becoming more similar. Bloat without distinction. ## Features in Go Go is different. Go does not try to be like the other languages. Go does not compete on features. As of Go 1, the language is fixed. Many newcomers to Go ask for features from other languages they know. But those features do not belong in Go, and the language is fixed. Adding features to Go would not make it better, just bigger. That would make Go less interesting by being less different. ## But you need features! Of course, there must be _some_ features. But which ones? The right ones! Design by consensus. ## Readability If a language has too many features, you waste time choosing which one to use. Then implement, refine, possibly rethink and redo. Later, \"Why does the code work this way?\" _The code is harder to understand simply because it is using a more complex language._ Preferable to have just one way, or at least fewer, simpler ways. Features add complexity. We want simplicity. Features hurt readability. We want readability. Readability is paramount. ## Readable means reliable Readable code is reliable code. It's easier to understand. It's easier to work on. If it breaks, it's easier to fix. If the language is complicated: * You must understand more things to read and work on the code. * You must understand more things to debug and fix it. A key tradeoff: More fun to write, or less work to maintain? Go has been thought to make programs more maintainable in the long term. ## Expressiveness Features are often suggested to aid \"expressiveness\". Conciseness can be expressive but not always readable (imagine mathematical symbols). Can also be expensive, implementing simple ideas with too-powerful primitives. Performance can also be unpredictable. On the other hand, verbosity can inhibit readability by obscuring intent. Build on, but do not be limited by, familiar ideas. Be concise while remaining expressive. ## The right set of features Not features for features' sake. Features that \"cover the space\", like vector basis covering solution space. Orthogonal features that interact predictably. Simple features that interact in simple ways. Simplicity comes from orthogonality and predictability. Keep the language's goals in mind. ## Go's goals Clean procedural language designed for scalable cloud software (infrastructure/ server infrastructure). Composable distinct elements, including: * Concrete data types * Functions and methods * Interfaces * Packages * Concurrency Plus: Good tools, fast builds. All the pieces feel simple in practice. ## Simplicity Go is actually complex, but it _feels_ simple. Interacting elements must mesh seamlessly, without surprises. Requires a lot of design, implementation work, refinement. Simplicity is the art of hiding complexity. ## A few simple things in Go * Garbage collection * Goroutines * Constants * Interfaces * Packages Each hides complexity behind a simple facade. ## Garbage collection Perhaps the best example of simplicity hiding complexity. Very difficult to implement well, but worth it. Yet so simple the language specification mentions it only in the introduction. Nonetheless, parts of the language strongly influenced by it. Code is simpler because GC is there. Design does not need to include \"ownership\". Go: There is no `free`. There is only garbage collection. As simple as it can be. (But complex behind the facade.) ## Concurrency Concurrency is the ability to write your program as independently executing pieces. In Go, concurrency has three elements: * goroutines (execution) * channels (communication) * `select` (coordination) ## Goroutines Start a goroutine with the `go` keyword: ```go go function(args) ``` Like garbage collection, eliminate considerations form the programmer's concern: * no stack size * no return or completion status * no mechanism for management * no \"ID\" These are things other systems would provide. Go instead has a minimalist design. Implementation complex, dependent on garbage collection for stack management. ## Constants In Go, constants are just numbers, even though (because) it is strongly typed. ```go var nanosecond = time.Second/ie9 ``` Simple idea took about a year to work out. Difficulties: * \"infinite\" precision integers * \"ifinite\" precision floating point (tried and failed with rationals) * promotion rules (`i := 2; f := 2.0; g := 1/2; h := 1/2.0`) * corner cases like shift ```go fmt.Printf(\"%T %T\", 2.0, 2.0<<0) ``` Still not totally satisfied, but the effect is that constants feel like numbers, contribute to ease of using Go. But complicated behind the scenes. ## Interfaces Just a set of methods. No data. Simple idea, but more complex than expected ```go type Reader interface { Read([]byte) (int, error) } ``` Also need variables of that type (`var reader io.Reader`). These variables add dynamic typing to a statically typed language. ```go var r Reader = os.Stdin // statically checked. var x interface{} = os.Stdin // statically checked. r = x.(Reader) // dynamically checked. Must be explicit here, design decision. ``` Requires careful design. Interface assignment must be implemented at run time (not a v-table). What if it fails? Led to type assertions and the \"comma, ok\" idiom. More complecity crept in. Type assertions and type switches were not in the original plan. Go's most distintive and powerful feature. Profound effect on library design. Enables true component architectures. Prime examples are `io.Reader` and `io.Writer`, generalisations of the Unix pipe idea. Feel simple, worth the complexity. ## Packages A design for structuring programs and libraries. ```go package big ... import \"math/big\" ``` Took a long time to design. Enable componentisation, scalability, sharing, data hiding and isolation, ... Affect program design, syntax, naming, building, linking, testing, ... Separation of package path (`\"math/big\"`) from package name (`big`). Enabled the `go get` mechanism. Intrincate to implement yet natural to use. After garbage collection, perhaps the highest ratio of true complexity to apparent simplicity, and therefore of the power of simplicity to hide complexity. ## Summary Simplicity is complicated but the clarity is worth the fight. ## Conclusion Simplicity is hard – to design. Simplicity is complicated – to build. But if you get it right... Simplicity is easy – to use. The success of Go proves it. ",
    "url": "/learning-notes/talks/simplicity-is-complicated/",
    "relUrl": "/talks/simplicity-is-complicated/"
  },"47": {
    "doc": "TDD, where did it all go wrong",
    "title": "TDD, where did it all go wrong",
    "content": "# [TDD, where did it all go wrong](https://vimeo.com/68375232) ### Frustrations with TDD Didn't refactoring promise change without breaking tests? If we change the implementation details, non of our tests should break. More test code than implementation code. Programming Anarchy and Lean Software Development drop tests first. Duct tape programmer, delivers functionality faster, quickly abandoning tests as \"slowing them down' Difficult to understand test intent, why tests fail. ATDD suites: - much of their life in red - Customers don't engange with the suite - slow to run and increasing cost. - devs ignoring red results - we need tools to fix them - devs don't want to write them ## Where did it all go wrong? - Writing tests against operations instead of writing them against behaviour (BDD). - Coupling our tests to implementation details. ### TDD rebooted #### The Zen of TDD * Avoid testing implementation details, test behaviours. Test intent has to be explicit about this, high-level details. - Adding a new class is not the trigger for writing tests. The trigger is implementing a requirement. - Test outside-in, writing tests to cover then the use cases, scenarios. You have to test the domain, if you test through forms (MVC), the moment you change your delivery mechanism you break all the tests. You should test the surface. Don't test the internals, you'd be coupling your test to internal details. - Only writing test to cover the implementation details as a way to understand the refactoring of the simple implementation we start with. BDD (Dan North), we should test behaviours, not tests. People misunderstand what TDD is all about. Kent explicitly talks about behaviours, with examples. Testing the perimeter surface, not the implementation details. #### What is a unit test * For Kent Beck, it is a test that _runs in isolation_ from other tests. - Nothing more, nothing less. - It is NOT to be confused with the classical unit test definition of targeting a module. - We don't touch file system, database, because these _shared fixture_ elements prevent us running in isolation from other tests (side-effects.) * Explicitly writing tests that target a method on a clas, is not a TDD unit tests - TDD unit tests focus on a story. Use-case, scenario... * Focusing on methods creates tests that are hard to maintain. We don't capture the behaviour we want to preserve. A lot of issues with TDD is people misunderstanding _isolation_ as _class isolation_, replacing collaborators with test doubles. ### Red-green-refactor 1. **Red** Write a little test that doesn't work, and perhaps doesn't even compile at first. 2. **Green** Make the test work quickly, commiting whatever sins necessary in the process. 3. **Refactor** Eliminate all of the duplication, created in merely getting the test to work. #### The simplest thing (green) * Make it run. Quickly getting that bar to go green domintates everything else. * The shift in aesthetics is hard for some experienced software engineers. - The only know how to follow the rules of good engineering. - Quick green excuses all sins. - This is not about accepting sin, its about being sinful. - Write sinful code! This is difficult fir experienced engineers, we tend to think on how to engineer the problem, not how to solve it fast. > We can commit any number of sins to get there, because speed trumps design, just for a brief moment. - Kent Beck > Good design at good times. Make it run, make it right. - Kent Beck #### Clean code now (refactor) * The refactoring step is when we produce clean code. - It's when you apply patterns - It's when you remove duplication - It's when you sanitise the code smells * **You do not write new unit tests here** - You are not introducing public classes - It is likely if you feel you need, you need collaborators to fulfill a role If you add more tests at this point, you'll be coupling your tests to implementation details, making your tests fragile. Your API is your contract, your tests should test the API, not the implementation details. Coupling is the first problem in software. * We need to eliminate dependency between our tests and our code - Tests should not depend on details, because then changing implementation breaks tests. Tests should depend on contracts or public interfaces. - This allows us to refactor implementations without changing tests. - Don't bake implementation details into tests * Test behaviours not implementations Counter argument: The setup of this may be too complex as we testing super high-level. * Don't test internals * Don't make everything public in order to test it * Preserve implementation hiding by keeping a thin public API * Refactor implementation details out, so that they do not need their own tests * Continue to refactor implementation details over time, as you want * Have expressive tests that you can read in the future ### Refactoring to patterns Patterns emerge in refactoring step. You don't think on patterns first an implementations later. References: TDD by Example (Kent Beck), Refactoring to Patterns (Joshua) ### Port and adapters Testing ice-cream cone anti-pattern. Testing piramid. UI: We are not testing the domain logic, we are testing the widgets. Integration: Tests cannot run in isolation if we have shared state fixtures like databases, etc. Here we do test that kind of stuff. Unit: To test the business logic. The tests are more valuable and less costly are unit tests. #### Hexagonal architecture We unit test the port, the use case. We test the behaviour, the contract to the world. We don't test the implementation details (peg: the database). Integraiton tests: Don't test things you don't own. Just test that you are using them correctly. System tests: Tests that verify that everything works. ### Gears We can be in the motorway driving in the fifth gear, that's may be the operational mode while developing, but sometimes we need to start changing down the gears. Fifth gear is \"obvious implementation\", we can go straight to green. You may write unnecessary code. Fourth gear is \"we are writing code nicely\", we start to think we don't know how to get to the solution so write some tests, some implementation details (drilling tests) to see how it may look. You need to think about behaviour. Maybe you want to remove those implementation tests once you finish with the behaviour. You should test the ports, not implementation details. You should be feel free to remove tests if necessary. The tests we need to keep are the tests that express the behaviour, the API of your system. ### Acceptance test-driven development Customers should be available to write scenarios for tests. Getting the customer chain together for this is problematic. Unit tests evolve into \"programmer tests\". ATDD is more about high-level communication. **If customers do not participate, it does not make any sense**. Expensive to write, expensive to run (Fitness, Cucumber). Maybe we should involve customers while writting unit tests and focus on unit tests. The problem is programmers making programmer driven tests instead of behaviour driven tests, so let's solve that. ### Behaviour-driven development BDD starts with a similiar inisght, that we misunderstood TDD. * It creates tools like RSpec and Jbehave * It realises that specifying scenarios is the key to driving test-driven development * It evolves into a methodology for facilitating the transmission of requirements from customer to developer through scenarios that can be automated. ### Mocks * Classic example is database - Long time to start - Difficult to keep \"clean\" - Tie tests to physical location on network - Write tests against something that acts like a DB * Mocks can improve readability by making contents of read rows explicit (evident data) * Mocks hinder the use of singletons - Which may be a good thing * Mocks make us consider issues around coupling - We learn to inject dependencies * Mocks risk that the real system will not perform in the correct way Two forces * We want to remove \"hard-to-test\" dependencies - Database, UI, network, file system, configuration files, etc. - Depending on these makes our tests fragile * We want to use Responsibility Driven Design and mock collaborators - Tell don't ask - But don't mock internals You do not mock the internals or implementation details, so couple to the implementation details with the mocks, mock other ports and other publics (APIs). Don't mock adapters either, do not mock things you don't own. ### Object Mother and Test Data Builder This is how you control complex setups for larger testing clusters (big setup). * An early solution was objet mother - A class that contains static factory methods used to create obejcts for use in tests - The name of the method helps identify the sterotypical object being created - The concept was that devevelopers would become faimiliar with the objects and use them in scenarios Problems with Object Mother * Variations in testing mean we end up with many different factory methods. Object mother soon becomes bloated * The variables affecting the test are not evident, but hidden behind the creation mechanism. If you don't know the Object Mother test data, tests are obscure. * The factory methods become shared fixture, which means that changes to construction ripple out A solution is using **Test Data Builders**. For each class you want to use in a test, create a Builder for that class that * Has an instance variable for each constructor parameter * Initialises its instance variables to commonly used or safe values * Has a `build` method that creats a new object using the values in its instance variables. We can do a neat trick with conversion operators too * Has a \"chainable\" public methods for overriding the values in its instance variables ## Summary * The reason to test is a new behaviour, not a method on a class * Write dirty code to get green, then refactor * No new tests for refactored itnernals and privates (methods, classes) * Both develop and accept against tests written on a port * Add integration tests for coverage of ports to adapters * Add system tests for end-to-end confidence * Don't mock internals, privates, or adapters ",
    "url": "/learning-notes/talks/tdd-where-did-it-all-go-wrong/",
    "relUrl": "/talks/tdd-where-did-it-all-go-wrong/"
  },"48": {
    "doc": "The Art of Destroying Software",
    "title": "The Art of Destroying Software",
    "content": "# [The Art of Destroying Software](https://vimeo.com/108441214) One of the beautiful things about deleting code is that it allows you to change your mind. [Waterfall paper](http://www-scf.usc.edu/~csci201/lectures/Lecture11/royce1970.pdf), one of the biggest ironies in our industry; it basically describes agile. [Big ball of mud paper](https://joeyoder.com/PDFs/mud.pdf). It argues that the big ball of mud is inevitable. The way to tackle it is to create pockets of smaller ball of muds. You have to write code for the purpose of deleting it. What if you developed software with the purpose of deleting it. Apply the lessons of Erlang to the rest of the software. Microservices help on this vision, if we have many services, I can delete and rewrite them. The definition of refactor is to either change my test or my code. Refactor changing the code and the test at the same time is called *refucktor*. The whole point of TDD is to make one of the sides stable so you can modify the other. It's a measurement, I'm predicting what will happen. Many refactoring tools change both sides at the same time. Your tools affect the code you are building. There is no way of having a 12 months project to refactor the `ls` program. The Unix way of doing things is the same as microservices, the same as the Erlang way. When we have small programs, we are not afraid anymore of deleting and rewriting them from scratch. We should optimise for their delete ability. Imagine how wonderful to know you could delete code without side-effects. The best way to optimise for deletability is to run away from monoliths, from decoupling. You should run coupling analysis in your software, there are great tools for it like Sonar. Microservices shouldn't be big or small, shouldn't be bigger than a week worth of rewrite. My personal risk if I had to do a rewrite is a week. If I later I get a better understanding of the model, it's just a week. Business people won't understand 3 months of tackling \"technical debt\" without providing value to customers. When you find that your models are wrong, just delete the code. Debt is not bad, debt allows you to buy your house! Same is true for technical debt. Sometimes you can ship crap code, would you care that much about crap if you were able to delete it easily? If technical debt start accumulating, delete it. Working in month-length projects is waste, try to work in 1 week projects. It is also more manageable in your head. Imagine onboarding and understanding the system as a whole. Get things small, manageable and understandable. The same for services is the same for objects or actors. Divide bigger systems into 1 week projects. Refactoring becomes just _deleting_ code. Alan Kay said that the biggest mistake he made naming the thing Object-Orientation was to call it Object-Orientation instead of Message-Orientation. Bigger programs contain smaller programs that communicate to each other. People nowadays think about the objects and not the messages that go between them. In Erlang, each Object is basically a process. These programs should be 1 week to rewrite. The secret for consulting is to never build big programs! The difference between good code and bad code is the size of the programs. You cannot keep 100.000 lines of code in your head, 3000 unit tests won't help. Refuse also to deploy more than a thousand lines of code to production. Smaller chunks are easier to deploy, test, rollback and debug. Teams want to build programs that are inherently too big and too complex. Developers usually have the habit of solving problems nobody has. > I could do it in a simple way, but if I did it won't be up to my level of intellect, so I am going to make it more complicated so it is worthy of me. You won't be able to predict the use case of the future, make your system easier to delete than to change. Don't plan for future changes, focus on the ability to rewrite from scratch when changes happen. Make decisions in the last responsible moment. The identifying trait of good code is small isolated programs that can be deleted on the fly. ",
    "url": "/learning-notes/talks/the-art-of-destroying-software/",
    "relUrl": "/talks/the-art-of-destroying-software/"
  },"49": {
    "doc": "The Do’s and Don’ts of Error Handling",
    "title": "The Do’s and Don’ts of Error Handling",
    "content": "# [The Do's and Don'ts of Error Handling](https://www.youtube.com/watch?v=TTM_b7EJg5E) > A system is fault-tolerant if it continues working even if something is wrong > Work like this is never finished, it's always in-progress * Hardware can fail (relatively uncommon) * Software can fail (common) ## Overview * Fault-tolerance cannot be achieved using a single computer – it may fail * We have to use several computers * Concurrency * Parallel programming * Distributed programming * Physics – time to propagate messages * Engineering * **Message passing is inevitable** – the basis of Object-Oriented Programming * Programming languages should make this ~easy~ doable We are never going to eliminate failure, systems will be inconsistent. We have to deal with it. * How individual computers work is the smaller problem. How the system works as a whole is important. * How the computers are interconnected and the protocols used between computers is the significant problem. * We want the same way to program large and small scale systems. Why do we have to program differently for local programs to communicate (memory) versus distributed systems (message bus)? In Object-Oriented Programming, Alan Kay said that the big idea behind Object-Oriented Programming was \"messaging\" ## Erlang * Derived from Smalltalk and Prolog (influenced by ideas from CSP) * Unifies ideas on concurrent and functional programming * Follows laws of physics (asynchronous messaging) * Designed for programming fault-tolerant systems Invented Erlang to solve a problem, not the language to find a problem to be solved with it. Building fault-tolerant software boils down to detecting errors and doing something when errors are detected. > We can't fix our own errors in the same way if I'm having a heart attack, I'll need someone to help me out ## Types of errors * Errors that can be detected at compile time * Errors that can be detected at run-time * Errors that can be inferred * Reproducible errors * Non-reproducible errors ## Philosophy * Find methods to prove Software correct at compile-time * Assume software is incorrect and will fail at run-time then do something about it at run-time ## Evidence for software failure is all around us Proving the self-consistency of small programs will not help. Testing a system will tell us that is self-consistent, not that it is correct. ## Conclusion * Some small things can be proven to be self-consistent. * Large assemblies of small things are possible to prove correct. --- 1980 - Erlang model of computation rejected. Shared memory systems ruled the world. 1985 - Ericsson started working on \"a replacement of PLEX\", started thinking about errors. \"errors must be corrected somewhere else\", \"shared memory is evil\", \"pure message passing\" 1986 - Erlang, unification of OO with FP 1998 - Several products in Erlang, Erlang gets banned and moved to Open Source 2004 - Erlang model of computation widely adopted in many different languages. ## Types of system * Highly reliable (nuclear power plant control, air-traffic), satellite (very expensive if they fail) * Reliable (driverless cars) (moderately expensive if they fail. Kills people if they fail) * Reliable (annoys people if they fail), banks, telephone * Dodgy (cross if they fail), Internet, HBO, Netflix * Crap (very cross if they fail), free apps Different technologies are used to build and validate the systems. How can we make software that works reasonably well even if there are errors in the software? \"Making reliable distributed systems in the presence of software errors\" book by Joe Armstrong ## Requirements * Concurrency * Error encapsulation * Fault detection * Fault identification * Code upgrade * Stable storage ## The \"method\" * Detect all errors * If you can't do what you want to do try to do something simpler (relax requirements) * Handle errors \"remotely\" (detect errors and ensure that the system is put into a safe state defined by an invariant) * Identify the \"Error Kernel\" (the part that must be correct) ## Supervision trees The node can be on different machine. Akka is \"Erlang supervision for Java and Scala\" ## It works * Ericsson smart phone data setup * WhatsApp * CouchDB (CERN) * Cisco (netconf) * Spine2 (NHS - uk - riak (basho) replaces Oracle) * RabbitMQ --- ## What is an error? * An undesirable property of a program * Something that crashes a program * A deviation between desired and observed behaviour ## Who finds the error? * The program (run-time) finds the error * Errors * Arithmetic errors: divide by zero, overflow, underflow, ... * Array bounds violated * System routine called with nonsense arguments * Null pointer * Switch option not provisioned * An incorrect value is observed * What can we do * Ignore it (no) * Try to fix it (no) * Crash immediately (yes) * Don't make matters worse * Assume somebody else will fix the problem (someone up in the supervision tree, this is like rebooting the system) * What should the programmer do when they don't know what to do? * Ignore it (no) * Log it (yes) * Try to fix it (possibly, but don't make maters worse) * Crash immediately (yes) * In sequential languages with single threads, crashing is not widely * The programmer finds the error * The compiler finds the error ## What's the big deal about concurrency? A sequential program when crashes it turns everything down (single thread). When in a parallel program something crashes, the rest should keep working. There are linked process, if a process dies, these other processes are going to tell the process has died; these processes are going to get error messages. ## Why concurrency? * Fault-tolerance is impossible with one computer * Scalable is impossible with one computer (yes, you can scale things vertically up to a limit; up to the capacity of the computer) * Security is very difficult with one computer (if the system gets compromised, everything gets compromised) * I want one way to program not two ways. One for local systems, the other for distributed systems (rules out shared memory) * The world is concurrent ## Detecting errors ### Where do errors come from * Arithmetic errors * Silent **and deadly** errors, errors where the program does not crash but delivers an incorrect result * Noisy errors, errors which cause the program to crash * Very difficult to get right * The same answer in a single and double precision does not mean the answer is right * If it matters, you must prove every line containing arithmetic is correct * Real arithmetic is not associative * Unexpected inputs * Wrong values * Programs does not crash, but the values computed are incorrect or inaccurate * How do we know if a program/value is incorrect if we do not have a specification * Many programs have no specifications or specs that are so imprecise to be useless * The specification might be incorrect _and the tests and the program_ * Wrong assumptions about the environment * Sequencing errors * Concurrency errors * Breaking laws of maths or physics --- The programmer does not know what to do: CRASH > I call this \"let it crash\" > Somebody else will fix the error > Needs concurrency and links What do you do when you receive an error? * Maintain an invariant * Try to do something simpler ## Is that all? What's a message? * A program is a black box * There are thousands of programming languages * What language used is irrelevant * The only important thing is what happens at the interface, their behaviour * Two systems are the same if they obey observational equivalence We shouldn't care about what happen inside these boxes, we just take care about the boundaries. * Interaction between components involves message passing * There are very few ways to describe messages (JSON, XML) * There are very very few formal ways to describe the valid sequences of messages (= protocols) between components * We need a way to describe protocols. Protocols are contracts. Contracts assign blame. We need an architecture that describes what we see on the wire: the contract checker * How do we describe contracts? ",
    "url": "/learning-notes/talks/the-dos-and-donts-of-error-handling/",
    "relUrl": "/talks/the-dos-and-donts-of-error-handling/"
  },"50": {
    "doc": "The Mess We Are In",
    "title": "The Mess We Are In",
    "content": "# [The Mess We Are In](https://www.youtube.com/watch?v=lKXe3HUG2l4) - Broken images in Open-Office - Slides on Keynote, async different versions not working - Slides to PDF, requires Grunt. Grunt not found locally * June 1948. Tom Kilburn, the first _modern_ programmer. 32bits. * Year 1985. OS boots in 60 seconds. * Typical laptop 2014. 1000x faster than previous generation. Should boot in 60ms, what happened? Now we don't know why programs stop working, we used to know. We are the job creators in the future. ## Seven deadly sins 1. Code even you cannot understand a week after you wrote it. **No comments** - We should write really big comments. A book! 2. Code with **no specifications**. 3. Code that is shipped as soon as it runs and before it is beautiful 4. Code with added features 5. Code that is very very fast very very **very obscure** and incorrect 6. Code that is **not beautiful** 7. Code that you wrote withut understanding the problem ## Legacy code * Programmers who wrote the code are dead * No specification * Written in archaic languages which nobody understands * \"It works\" * Management thinks modifying legacy code is cheaper than a total re-write --- * A program with 6 32bit integers can have more states than the number of atoms on the earth. * Don't ask about Javascript. A computer is a state machine. `State x Event -> New state`. You need 2^7633587786 universes as the same number of atoms as states in my machine. **We do never have the same state as other machines.** ## Failure We have to deal with failure, we can't ignore it. To handle you need to understand. Computer 1 fails, Computer 2 detects this * Distributed computing * Parallel computing * Concurrent programming Systems should self-repair self-configure and evolve with time, like biological systems. ## Languages Notation does not matter? Ask rommans to represent numbers! In 1985 everybody* knew `sh`, `make`, `c`. Now programmers have no common language and cannot talk to each other. Now we ahve 2500 languages to choose from. We don't have a lingua franca to communicate to each other. Once upon a time all programmers understood `make`. Now we have `ant`, `grunt`, `make`, `rake`, `maven`, `jake`, `cake`, `bitcake`, `fabric`, `paper`, `shovel`, etc. A program in Erlang with 3 modules and with bitcake downloaded and compile 46000. WTF. Without Google and Stackoverflow programming would be impossible. ## Efficiency and clarity To make something clearer add a layer of abstraction. To make something more efficient remove a layer of abstraction. Focus on clarity, speeds doubles every year. ## Names * Names are imprecise * Name imply a namespace * The named thing might change * Deciding a name is difficult * Uniwue names are difficult to make ## What do the laws of physics say about computation ### Causality * A cause must always precede its effect * Information travels at or less than the speed of light * We do not know that something has happened until we get a message saying that the event has happened * We do not know how things are _now_ at a remote location only how they were the last time we got a message from them ### Simultanuity We have simultaneous observations about systems. ### Entropy always increases If you throw a lot of dices in the air, they are not going to appear with a 6 up. They will be random. The more dices, the more entropy we have. ### Speed of computation ### Quantum mechanics fun * Bremermann's limit * Margolous-Levitin theorem * The Bekenstein bound * The Landauer Limit * The Ultimate laptop * The Ultimate computer ### The Ultimate laptop The Ultimate laptop is a 1kg black hole... * 10^51 ops sec * Size 10^-27 meters * Storage capacity 10^16 bits * Lasts for 10^-21 seconds * Emits data through Hawkings radiation and quantum entanglement ### The Ultimate computer The ultimate computer is the entire universe * 10^123 ops since it was booted * Diameter 9.2x10^26 meters * Storage capacity 10^92 bits * Lasts for 10^10^1000 Reference: [Ultimate limits ultimate physical limits to computation - Seth Lloyd (MIT)](http://arxiv.org/abs/quant-ph/9908043) ## What can we do? The entropy reverser. Entropy increses the number of files All files -> The condenser -> Few files (break the 2nd law of thermodynamics) * Files mutate * Disk are huge * What name should a file have * Whic directory should I store the file in * What machine should I store the directory in * How will I find the file later * How can I replicate the file ## URIs are bad http://www.foo.se/a/b/c * DNS can be spoofed * The host can be unavailable * If the story is changed the reference is wrong * Can be cached (but for how long) * Content can be changed by a person-in-the-middle **Use hashes instead of names** hash://367816778a768b6c45a4465e79798987f * Address cannot be spoofed (there is no address) * No problem choosing a name (there is no name) * Can be cached (forever) * Content can be validated on arrival * No subject to person-in-the-middle attacks How do you get the hash? Chrod Kademlia. Ask the \"nearest\" machines to find stuff, which also have similar lists. ## How to make the condenser 1. Find all identical files 2. Merge all similar files. We need to find the most similar file to a given file. If is a new idea, probably I should keep it. If not, maybe we can merge or reuse. `A similar B if size(C(A)) is similar to size(C(A++B))` ## Summary * We've made a mess * We need to reverse entropy * Quantum mechanics sets limits to the ultimate speed of computation * We need math * Abolish names and places * Build the condenser * Make low-power computers, no net environmental damage --- References * [Programming Erlang](https://www.goodreads.com/book/show/38813666-programming-erlang) * [Concurrent programming in Erlang](https://www.goodreads.com/book/show/808815.Concurrent_Programming_ERLANG) ",
    "url": "/learning-notes/talks/the-mess-we-are-in/",
    "relUrl": "/talks/the-mess-we-are-in/"
  },"51": {
    "doc": "The World after Microservice Migration",
    "title": "The World after Microservice Migration",
    "content": "# [The World after Microservice Migration](https://www.youtube.com/watch?v=MxhcFPRkzlw) ## Motivation Car-sharing application integration and aggregation into the system. Not caring about the technical debt, right to bite you in the ass at the end. Providers as microservices. There is a gateway that either goes to providers or to an aggregator of internal services. Providers can be extracted from the monolith. They needed some kind of dynamic routing for this. ## Service mesh > A dedicated infrastructure layer for making service-to-service communication safe, fast and reliable. * Dynamic routing * Load-balancing * Retryable failures * Circuit-breaking * Distributed tracing and metrics We are already familiar with these concepts, the difference is that we want to extract all this behaviour from your application code. * Linkerd v1 ### Linkerd deployment models * All services talk to a single linkerd instance * Each service talk to a service linkerd sidecar. You can do as outgoing or incoming call. Then you can use Zipkin to analise and measure calls. POST http://free2move.com/providers/drive2move/login 1) Identification. Where to route can be identified by header Host, the path, or by method like POST 2) Binding the services that will match with the identification, starting from top to bottom 3) Resolution, after we get a Consul namer por example, we resolve the IPs Take into account that there is a little performance of having Linkerd intermediate between requests #### Linkerd 2.0 Linkerd is not precisely light. You shouldn't be deploying it with your application. The idea is to have a proxy sidecar (data plane) and the whole Linkerd logic plus configuration in Kubernetes or in another place (control plane). ### Alernatives to Linkerd * Istio, which looks like Linkerd 2.0 * Consul connect that now includes a service mesh with Istio ## Consumer-Driven Contract testing [Pact](https://docs.pact.io/) framework for doing contract testing, the specification of the contract. - How does a request/response format look - Upstream/downstream - The client defines the contract, and the provider the one that implements it Pact generates contract files that you can upload to the Pact broker, you can see a graph of your services. On the build pipeline, we can run the tests with the contracts against real services. You can combine it with Swagger too. For legacy applications you can use WireMock to check for recording incoming and outcoming requests. ",
    "url": "/learning-notes/talks/the-world-after-microservice-migration/",
    "relUrl": "/talks/the-world-after-microservice-migration/"
  },"52": {
    "doc": "What I Learned Doing 250 Interviews at Google",
    "title": "What I Learned Doing 250 Interviews at Google",
    "content": "# [What I Learned Doing 250 Interviews at Google](https://www.youtube.com/watch?v=r8RxkpUvxK0) ## General ### It's fun! Talking to people is fun. Meeting new people. Sharing expertise is also fun. You always learn stuff from interviews. ### Except when it isn't The time you spend on interviews is not that fun. It is stressful. You are meeting strangers. Random things can go sideways. People can be jerks. People refuse to write code. ### Noisy, inaccurate and arbitrary Writing code on a whiteboard is not natural. The interviewer may not have a good day. People may have different results depending on the interviewers, so it can be arbitrary. Missing someone who is good is better than hiring some who is bad. ### Data is a blessing and a curse People in Google are evaluated from 1 to 4. Same score cannot be compared with other people, same number may not mean the same thing. What was valuable for the committee was the written feedback. ### You can't put a number on stuff that matters A good example on this is the IQ number. ### You can't even ask about stuff that matters How would you test for collaboration? The best you can do is to ask for situations to check candidates creativity, initiative, etc. Interviews is more about criticism than it is about statistics. ### It's a team effort, and isolation hurts The more people involved, the better. If there is a single person deciding, quality may degrade quite fast. For that reason, the hiring committee in Google didn't participate in interviews, they wanted to decouple bias from teams. **Teams did not hire for their own positions because of that.** You get assigned people that passed the process. ### Be prepared! Give training classes. How interviews should be run, trying technical questions, practice dry-runs. ### But, it's like jazz Part of being prepared is to expect things may go south. ### Please make mistakes! Iteration is key. When people start interviewing it won't be great. You can have experienced interviewers guiding less experienced ones, so they can make mistakes. Don't be a dick, be cool. ### Every interview is a conversation (with goals) Is not ultimately a test to see if the candidate survives. You cannot stress the candidate out. It is a conversation. You are both humans. ### Primary goal: Happy candidate It overrides everything else. If you upset the candidate, you may be probably be doing something wrong. This was the primary goal in Google too, the problem is that just one arrogant jerk and the whole internet knows. It doesn't take much to gain notoriety. ### Other goal: Answer \"Would I love to work with this person?\" The candidate may not end up in your same team, or even in the same building. Maybe you can aim to hire people smarter than yourself, so you effectively raise the bar with each new hire. ### Good interviews are generous You can always learn or accomplish something, either as an interviewer or as a candidate. This came from interviewers being prepared and knowledgeable. ### Time is of the essence Time is super important. Take care on time management. ## Thoughts on technical interviews ### Good questions are like onions Good questions are like onions. There is a starting naive solution and dig on, deeper and deeper. You can ask about optimisations and the like. It's better if you are iterating on something. ### Strive for higher bandwidth Prefer video calls to phone calls, prefer in-person to video-calls. Use whiteboards. The more inputs the better. ### Artifice is inevitable Being a candidate is not the same as having a job. Being good at interviewing is a different skill than being a good engineer. Interviews are artificial. ### More signal; less noise There is no reason for mystery. You can give information to candidates to prepare for interviews. You should try your best to allow them to show their skills. ### This might be the best we can do Do not copy the model from other companies, like Google. ### It probably isn't It may not be the best model for interviews. Bootcamps may be a better way for recruiting. ### Have fun Try to have fun! ",
    "url": "/learning-notes/talks/what-i-learned-doing-250-interviews-at-google/",
    "relUrl": "/talks/what-i-learned-doing-250-interviews-at-google/"
  },"53": {
    "doc": "What I wish I had known before scaling Uber to 1000 services",
    "title": "What I wish I had known before scaling Uber to 1000 services",
    "content": "# [What I wish I had known before scaling Uber to 1000 services](https://www.youtube.com/watch?v=kb-m2fasdDY) ## Why micro services * Move and release independently * Own your uptime (on-call) * Use the \"best\" tool for the job ## What are the costs * Now you have a distributed system * Everything is an RPC * What if it breaks? ## Less obvious costs * Everything is a trade-offs, you are going to give something up * You can build around problems * Might trade complexity for politics * You get to keep your biases ## Tech stack * Pre-history: PHP (outsourced) * Dispatch: Node.JS, moving Go * Core Services: Python, moving to Go * Maps: Python and Java * Data: Python and Java * Metrics: Go ## Different languages costs * Hard to share code * Hard to move between teams * What I wish I knew: Fragments the culture ## RPC * HTTP/REST gets complicated * JSON needs a schema * RPCs are slower than PCs * What I wish I knew: Servers are not browsers ## How many repos * With one repo you can make cross-project changes easier. Not really usable * Multiple repos * 7000 repositories ## Operational * What happens when things break? Downstream dependencies; people issues, people fix your service * Can other teams release your service? * Understand a service in the larger context ## Performance * Depends on language tools. Tools are all different. Flamegraphs * If dashboards are not generated automatically, teams create their own. This could be solved as if you expose your service automatically, to generate the dashboards automatically and homogeneously across all teams * Doesn't matter until it does. You can place SLAs * Probably want at least simple perf requirements * What I wish I knew: \"good\" not required, but \"known\" is ## Fanout * Go for the slowest thing that you have in your chain * Overall latency >= latency of slowest. * Looking at p95, p99, p99.9 ## Tracing * Lots of ways to get this. Zipkin, Open-tracing, etc. * Best way to understand fanout * Probably want sampling * What I wish I knew: cross-lang context propagation ## Logging * Need consistent, structure logging * Multiple languages makes this hard * Logging floods can amplify problems * What I wish I knew: Accounting ## Load testing * Need to test against production * Without breaking metrics * Preferably all the time * What I wish I knew: All systems need to handle \"test\" traffic ## Failure testing * What I wish I knew: People won't like it ## Migrations * Old stuff still has to work, you cannot stop the world, people are making changes all the time * What happened to immutable? * What I wish I knew: mandates are bad. People will be more willing for change if you provide a better system rather than forcing them to change the current one ## Open source * Build/buy trade-off is hard * Commoditisation * What I wish I knew: This will make people sad ## Politics Services allow people to play politics. Politics happen when \"Company > Team > Self\" is out of order. ## Trade-offs Everything is a trade-off Try to make them intentionally ",
    "url": "/learning-notes/talks/what-i-wish-i-had-known-before-scaling-uber-to-1000-services/",
    "relUrl": "/talks/what-i-wish-i-had-known-before-scaling-uber-to-1000-services/"
  }
}
